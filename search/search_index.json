{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Pytorch extension for OpenML python","text":"<p>Pytorch extension for openml-python API. This library provides a simple way to run your Pytorch models on OpenML tasks. </p> <p>For a more native experience, PyTorch itself provides OpenML integrations for some tasks. You can find more information here.</p>"},{"location":"#installation-instructions","title":"Installation Instructions:","text":"<p>While this project does exist on pypi, while everything is being finalized, it is recommended to install the package directly from the repository. </p> <pre><code>pip install git+https://github.com/openml/openml-pytorch -U\n</code></pre> <p>PyPi link https://pypi.org/project/openml-pytorch/</p> <p>Set the API key for OpenML from the command line: <pre><code>openml configure apikey &lt;your API key&gt;\n</code></pre></p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#load-data-from-openml-and-train-a-model","title":"Load Data from OpenML and Train a Model","text":"<pre><code># Import libraries\nimport openml\nimport torch\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom typing import Any\nfrom tqdm import tqdm\n\nfrom openml_pytorch import GenericDataset\n\n# Get dataset by ID and split into train and test\ndataset = openml.datasets.get_dataset(20)\nX, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\nX = X.to_numpy(dtype=np.float32)  \ny = y.to_numpy(dtype=np.int64)    \nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n\n# Dataloaders\nds_train = GenericDataset(X_train, y_train)\nds_test = GenericDataset(X_test, y_test)\ndataloader_train = torch.utils.data.DataLoader(ds_train, batch_size=64, shuffle=True)\ndataloader_test = torch.utils.data.DataLoader(ds_test, batch_size=64, shuffle=False)\n\n# Model Definition\nclass TabularClassificationModel(torch.nn.Module):\n    def __init__(self, input_size, output_size):\n        super(TabularClassificationModel, self).__init__()\n        self.fc1 = torch.nn.Linear(input_size, 128)\n        self.fc2 = torch.nn.Linear(128, 64)\n        self.fc3 = torch.nn.Linear(64, output_size)\n        self.relu = torch.nn.ReLU()\n        self.softmax = torch.nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        x = self.softmax(x)\n        return x\n\n# Train the model. Feel free to replace this with your own training pipeline. \ntrainer = BasicTrainer(\n    model = TabularClassificationModel(X_train.shape[1], len(np.unique(y_train))),\n    loss_fn = torch.nn.CrossEntropyLoss(),\n    opt = torch.optim.Adam,\n    dataloader_train = dataloader_train,\n    dataloader_test = dataloader_test,\n    device= torch.device(\"mps\")\n)\ntrainer.fit(10)\n</code></pre>"},{"location":"#more-complex-image-classification-example","title":"More Complex Image Classification Example","text":"<p>Import openML libraries <pre><code># openml imports\nimport openml\nimport openml_pytorch as op\nfrom openml_pytorch.callbacks import TestCallback\nfrom openml_pytorch.metrics import accuracy\nfrom openml_pytorch.trainer import convert_to_rgb\n\n# pytorch imports\nfrom torch.utils.tensorboard.writer import SummaryWriter\nfrom torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda\nimport torchvision\n\n# other imports\nimport logging\nimport warnings\n\n# set up logging\nopenml.config.logger.setLevel(logging.DEBUG)\nop.config.logger.setLevel(logging.DEBUG)\nwarnings.simplefilter(action='ignore')\n## Data\n### Define image transformations\n\n\ntransform = Compose(\n    [\n        ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.\n        Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.\n        Resize((64, 64)),  # Resize the image.\n        ToTensor(),  # Convert the PIL Image back to a tensor.\n    ]\n)\n\n### Configure the Data Module and Choose a Task\n\"\"\"\n- Make sure the data is present in the `file_dir` directory, and the `filename_col` is correctly set along with this column correctly pointing to where your data is stored. \n\"\"\"\ndata_module = op.OpenMLDataModule(\n    type_of_data=\"image\",\n    file_dir=\"datasets\",\n    filename_col=\"image_path\",\n    target_mode=\"categorical\",\n    target_column=\"label\",\n    batch_size=64,\n    transform=transform,\n)\n\n# Download the OpenML task for tiniest imagenet\ntask = openml.tasks.get_task(363295)\n\n## Model\nmodel = torchvision.models.resnet18(num_classes=200)\n## Train your model on the data\n#- Note that by default, OpenML runs a 10 fold cross validation on the data. You cannot change this for now.\nimport torch\n\ntrainer = op.OpenMLTrainerModule(\n    experiment_name= \"Tiny ImageNet\",\n    data_module=data_module,\n    verbose=True,\n    epoch_count=2,\n    metrics= [accuracy],\n    # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.\n    callbacks=[\n        # TestCallback,\n    ],\n    opt = torch.optim.Adam,\n)\nop.config.trainer = trainer\nrun = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\n## View information about your run\n### Learning rate and loss plot\ntrainer.plot_loss()\ntrainer.plot_lr()\ntrainer.plot_all_metrics()\n### Class labels\ntrainer.model_classes\n## Model Vizualization\n#- Sometimes you may want to visualize the model. You can either use netron or tensorboard for this purpose.\n### Netron\ntrainer.export_to_netron()\n### Tensorboard\n\"\"\"\n- By default, openml will log the tensorboard logs in the `tensorboard_logs` directory. You can view the logs by running `tensorboard --logdir tensorboard_logs` in the terminal.\n\"\"\"\n## Publish your model to OpenML\n\"\"\"\n- This is Optional, but publishing your model to OpenML will allow you to track your experiments and compare them with others.\n- Make sure to set your apikey first.\n  - You can find your apikey on your OpenML account page.\n\"\"\"\ntrainer.plot_all_metrics()\nopenml.config.apikey = ''\nrun = op.add_experiment_info_to_run(run=run, trainer=trainer) \nrun.publish()\n</code></pre></p>"},{"location":"Integrations%20of%20OpenML%20in%20PyTorch/","title":"Integrations of OpenML in PyTorch","text":"<p>Along with this PyTorch API, OpenML is also integrated in PyTorch through the following modules.</p>"},{"location":"Integrations%20of%20OpenML%20in%20PyTorch/#reinforcement-learning","title":"Reinforcement Learning","text":"<ul> <li>The RL library TorchRL supports loading OpenML datasets as part of inbuilt modules.  </li> </ul>"},{"location":"Integrations%20of%20OpenML%20in%20PyTorch/#torchrl-openmlexperiencereplay","title":"TorchRL - OpenMLExperienceReplay","text":"<ul> <li>Experience replay is a technique used in reinforcement learning to improve the stability and performance of deep reinforcement learning algorithms by storing and reusing experience tuples.</li> <li>This module provides a direct interface to OpenML datasets to be used in experience replay buffers.</li> </ul> <pre><code>exp = OpenMLExperienceReplay(\"adult_onehot\", batch_size=2)\n# the following datasets are supported: \"adult_num\", \"adult_onehot\", \"mushroom_num\", \"mushroom_onehot\", \"covertype\", \"shuttle\" and \"magic\"\nprint(exp.sample())\n</code></pre>"},{"location":"Integrations%20of%20OpenML%20in%20PyTorch/#torchrl-openmlenv","title":"TorchRL -   OpenMLEnv","text":"<ul> <li>Bandits are a class of RL problems where the agent has to choose between multiple actions and receives a reward based on the action chosen.</li> <li>This module provides an environment interface to OpenML data to be used in bandits contexts.</li> <li>Given a dataset name (obtained from openml datasets), it returns a PyTorch environment that can be used in PyTorch training loops.</li> </ul> <pre><code>env = OpenMLEnv(\"adult_onehot\", batch_size=[2, 3])\n# the following datasets are supported: \"adult_num\", \"adult_onehot\", \"mushroom_num\", \"mushroom_onehot\", \"covertype\", \"shuttle\" and \"magic\"\nprint(env.reset())\n</code></pre>"},{"location":"Limitations%20of%20the%20API/","title":"Limitations","text":"<ul> <li>The way OpenML Pytorch works is by using the OpenMl python extension API. The latter relies on a method called <code>run_model_on_fold</code>, which requires any other extensions to pass their information through this function. In essence, this means it is not currently possible to just have a \"minimal\" extension, where you can have any training pipeline and it \"just works\" with OpenML. </li> <li>To counteract this, we have created two kinds of examples, one where you can ignore this limitation and just use data from OpenML along with your own pipelines example1 and the rest of them where you can use the API we provide to train your models and upload them to OpenML example2.</li> </ul>"},{"location":"Limitations%20of%20the%20API/#image-datasets","title":"Image datasets","text":"<ul> <li>OpenML so far does not focus on image datasets, and as such, they are stored as a header file (think of it as a table with file names and say categories in the case of classification). For some datasets, OpenML provides a way to download the image folders directly, but this is not always the case. This will eventually be replaced by something nicer, but for now, this is the way it is.</li> </ul>"},{"location":"Philosophy%20behind%20the%20API%20Design/","title":"Philosophy behind the API design","text":"<p>This API is designed to make it easier to use PyTorch with OpenML and has been heavily inspired by the current state of the art Deep Learning frameworks like FastAI and PyTorch Lightning. </p> <p>To make the library as modular as possible, callbacks are used throughout the training loop. This allows for easy customization of the training loop without having to modify the core code.</p>"},{"location":"Philosophy%20behind%20the%20API%20Design/#separation-of-concerns","title":"Separation of Concerns","text":"<p>Here, we focus on the data, model and training as separate blocks that can be strung together in a pipeline. This makes it easier to experiment with different models, data and training strategies.</p> <p>That being the case, the OpenMLDataModule and OpenMLTrainerModule are designed to handle the data and training respectively. This might seem a bit verbose at first, but it makes it easier to understand what is happening at each step of the process and allows for easier customization.</p>"},{"location":"API%20reference/Callbacks/","title":"Callbacks","text":"<p>Callbacks module contains classes and functions for handling callback functions during an event-driven process. This makes it easier to customize the behavior of the training loop and add additional functionality to the training process without modifying the core code.</p> <p>To use a callback, create a class that inherits from the Callback class and implement the necessary methods. Callbacks can be used to perform actions at different stages of the training process, such as at the beginning or end of an epoch, batch, or fitting process. Then pass the callback object to the Trainer.</p>"},{"location":"API%20reference/Callbacks/#callbacks--how-to-use","title":"How to Use:","text":"<pre><code>trainer = OpenMLTrainerModule(\n    data_module=data_module,\n    verbose = True,\n    epoch_count = 1,\n    callbacks=[ &lt;insert your callback class name here&gt; ],\n)\n</code></pre> <p>To add a custom parameter, for example to add a different metric to the AvgStatsCallBack. ```python trainer = OpenMLTrainerModule(     data_module=data_module,     verbose = True,     epoch_count = 1,     callbacks=[ AvgStatsCallBack([accuracy]) ], )</p>"},{"location":"API%20reference/Callbacks/#callbacks--useful-callbacks","title":"Useful Callbacks:","text":"<ul> <li>TestCallback: Use when you are testing out new code and want to iterate through the training loop quickly. Stops training after 2 iterations.</li> </ul>"},{"location":"API%20reference/Callbacks/#callbacks.AvgStats","title":"<code>AvgStats</code>","text":"<p>AvgStats class is used to track and accumulate average statistics (like loss and other metrics) during training and validation phases.</p> <p>Attributes:</p> Name Type Description <code>metrics</code> <code>list</code> <p>A list of metric functions to be tracked.</p> <code>in_train</code> <code>bool</code> <p>A flag to indicate if the statistics are for the training phase.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the AvgStats with metrics and in_train flag.</p> <code>reset</code> <p>Resets the accumulated statistics.</p> <code>all_stats</code> <p>Property that returns all accumulated statistics including loss and metrics.</p> <code>avg_stats</code> <p>Property that returns the average of the accumulated statistics.</p> <code>accumulate</code> <p>Accumulates the statistics using the data from the given run.</p> <code>__repr__</code> <p>Returns a string representation of the average statistics.</p> Source code in <code>openml_pytorch/callbacks/recording.py</code> <pre><code>class AvgStats:\n    \"\"\"\n    AvgStats class is used to track and accumulate average statistics (like loss and other metrics) during training and validation phases.\n\n    Attributes:\n        metrics (list): A list of metric functions to be tracked.\n        in_train (bool): A flag to indicate if the statistics are for the training phase.\n\n    Methods:\n        __init__(metrics, in_train):\n            Initializes the AvgStats with metrics and in_train flag.\n\n        reset():\n            Resets the accumulated statistics.\n\n        all_stats:\n            Property that returns all accumulated statistics including loss and metrics.\n\n        avg_stats:\n            Property that returns the average of the accumulated statistics.\n\n        accumulate(run):\n            Accumulates the statistics using the data from the given run.\n\n        __repr__():\n            Returns a string representation of the average statistics.\n    \"\"\"\n\n    def __init__(self, metrics, in_train):\n        self.metrics, self.in_train = listify(metrics), in_train\n\n    def reset(self):\n        self.tot_loss, self.count = 0.0, 0\n        self.tot_mets = [0.0] * len(self.metrics)\n\n    @property\n    def all_stats(self):\n        return [self.tot_loss.item()] + self.tot_mets\n\n    @property\n    def avg_stats(self):\n        return [o / self.count for o in self.all_stats]\n\n    def accumulate(self, run):\n        bn = run.xb.shape[0]\n        self.tot_loss += run.loss * bn\n        self.count += bn\n        for i, m in enumerate(self.metrics):\n            self.tot_mets[i] += m(run.pred, run.yb) * bn\n\n    def __repr__(self):\n        if not self.count:\n            return \"\"\n        phase = \"train\" if self.in_train else \"valid\"\n        try:\n            return f\"{phase} loss: {self.avg_stats[0]:.4f} | accuracy: {self.avg_stats[1]:.4f} | other metrics: {self.avg_stats[2:]}\"\n        except IndexError:\n            return f\"{phase} loss: {self.avg_stats[0]:.4f} | other metrics: {self.avg_stats[1:]}\"\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.AvgStatsCallback","title":"<code>AvgStatsCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>AvgStatsCallBack class is a custom callback used to track and print average statistics for training and validation phases during the training loop.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <p>A list of metric functions to evaluate during training and validation.</p> required <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the callback with given metrics and sets up AvgStats objects for both training and validation phases.</p> <code>begin_epoch</code> <p>Resets the statistics at the beginning of each epoch.</p> <code>after_loss</code> <p>Accumulates the metrics after computing the loss, differentiating between training and validation phases.</p> <code>after_epoch</code> <p>Prints the accumulated statistics for both training and validation phases after each epoch.</p> Source code in <code>openml_pytorch/callbacks/recording.py</code> <pre><code>class AvgStatsCallback(Callback):\n    \"\"\"\n    AvgStatsCallBack class is a custom callback used to track and print average statistics for training and validation phases during the training loop.\n\n    Arguments:\n        metrics: A list of metric functions to evaluate during training and validation.\n\n    Methods:\n        __init__: Initializes the callback with given metrics and sets up AvgStats objects for both training and validation phases.\n        begin_epoch: Resets the statistics at the beginning of each epoch.\n        after_loss: Accumulates the metrics after computing the loss, differentiating between training and validation phases.\n        after_epoch: Prints the accumulated statistics for both training and validation phases after each epoch.\n    \"\"\"\n\n    def __init__(self, metrics):\n        self.train_stats, self.valid_stats = (\n            AvgStats(metrics, True),\n            AvgStats(metrics, False),\n        )\n\n    def begin_epoch(self):\n        self.train_stats.reset()\n        self.valid_stats.reset()\n\n    def after_loss(self):\n        stats = self.train_stats if self.in_train else self.valid_stats\n        with torch.no_grad():\n            stats.accumulate(self.run)\n\n    def after_epoch(self):\n        current_epoch = self.current_epoch\n        print(f\"\\n{'='*40}\")\n        print(f\"Epoch {current_epoch}\")\n        print(f\"{'-'*40}\")\n        print(f\"Train: {self.train_stats}\")\n        print(f\"Valid: {self.valid_stats}\")\n        print(f\"{'='*40}\\n\")\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.Callback","title":"<code>Callback</code>","text":"<p>Callback class is a base class designed for handling different callback functions during an event-driven process. It provides functionality to set a runner, retrieve the class name in snake_case format, directly call callback methods, and delegate attribute access to the runner if the attribute does not exist in the Callback class.</p> <p>The _order is used to decide the order of Callbacks.</p> Source code in <code>openml_pytorch/callbacks/callback.py</code> <pre><code>class Callback:\n    \"\"\"\n\n    Callback class is a base class designed for handling different callback functions during\n    an event-driven process. It provides functionality to set a runner, retrieve the class\n    name in snake_case format, directly call callback methods, and delegate attribute access\n    to the runner if the attribute does not exist in the Callback class.\n\n    The _order is used to decide the order of Callbacks.\n\n    \"\"\"\n\n    _order = 0\n\n    def set_runner(self, run) -&gt; None:\n        self.run = run\n\n    @property\n    def name(self):\n        name = re.sub(r\"Callback$\", \"\", self.__class__.__name__)\n        return camel2snake(name or \"callback\")\n\n    def __call__(self, cb_name):\n        f = getattr(self, cb_name, None)\n        if f and f():\n            return True\n        return False\n\n    def __getattr__(self, k):\n        return getattr(self.run, k)\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.ParamScheduler","title":"<code>ParamScheduler</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Manages scheduling of parameter adjustments over the course of training.</p> Source code in <code>openml_pytorch/callbacks/annealing.py</code> <pre><code>class ParamScheduler(Callback):\n    \"\"\"\n    Manages scheduling of parameter adjustments over the course of training.\n    \"\"\"\n\n    _order = 1\n\n    def __init__(self, pname, sched_funcs):\n        self.pname, self.sched_funcs = pname, sched_funcs\n\n    def begin_fit(self):\n        \"\"\"\n        Prepare the scheduler at the start of the fitting process.\n        This method ensures that sched_funcs is a list with one function per parameter group.\n        \"\"\"\n        if not isinstance(self.sched_funcs, (list, tuple)):\n            self.sched_funcs = [self.sched_funcs] * len(self.opt.param_groups)\n\n    def set_param(self):\n        \"\"\"\n        Adjust the parameter value for each parameter group based on the scheduling function.\n        Ensures the number of scheduling functions matches the number of parameter groups.\n        \"\"\"\n        assert len(self.opt.param_groups) == len(self.sched_funcs)\n        for pg, f in zip(self.opt.param_groups, self.sched_funcs):\n            pg[self.pname] = f(self.n_epochs / self.epochs)\n\n    def begin_batch(self):\n        \"\"\"\n        Apply parameter adjustments at the beginning of each batch if in training mode.\n        \"\"\"\n        if self.in_train:\n            self.set_param()\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.ParamScheduler.begin_batch","title":"<code>begin_batch()</code>","text":"<p>Apply parameter adjustments at the beginning of each batch if in training mode.</p> Source code in <code>openml_pytorch/callbacks/annealing.py</code> <pre><code>def begin_batch(self):\n    \"\"\"\n    Apply parameter adjustments at the beginning of each batch if in training mode.\n    \"\"\"\n    if self.in_train:\n        self.set_param()\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.ParamScheduler.begin_fit","title":"<code>begin_fit()</code>","text":"<p>Prepare the scheduler at the start of the fitting process. This method ensures that sched_funcs is a list with one function per parameter group.</p> Source code in <code>openml_pytorch/callbacks/annealing.py</code> <pre><code>def begin_fit(self):\n    \"\"\"\n    Prepare the scheduler at the start of the fitting process.\n    This method ensures that sched_funcs is a list with one function per parameter group.\n    \"\"\"\n    if not isinstance(self.sched_funcs, (list, tuple)):\n        self.sched_funcs = [self.sched_funcs] * len(self.opt.param_groups)\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.ParamScheduler.set_param","title":"<code>set_param()</code>","text":"<p>Adjust the parameter value for each parameter group based on the scheduling function. Ensures the number of scheduling functions matches the number of parameter groups.</p> Source code in <code>openml_pytorch/callbacks/annealing.py</code> <pre><code>def set_param(self):\n    \"\"\"\n    Adjust the parameter value for each parameter group based on the scheduling function.\n    Ensures the number of scheduling functions matches the number of parameter groups.\n    \"\"\"\n    assert len(self.opt.param_groups) == len(self.sched_funcs)\n    for pg, f in zip(self.opt.param_groups, self.sched_funcs):\n        pg[self.pname] = f(self.n_epochs / self.epochs)\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.PutDataOnDeviceCallback","title":"<code>PutDataOnDeviceCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>PutDataOnDevice class is a custom callback used to move the input data and target labels to the device (CPU or GPU) before passing them to the model.</p> <p>Methods:</p> Name Description <code>begin_fit</code> <p>Moves the model to the device at the beginning of the fitting process.</p> <code>begin_batch</code> <p>Moves the input data and target labels to the device at the beginning of each batch.</p> Source code in <code>openml_pytorch/callbacks/device_callbacks.py</code> <pre><code>class PutDataOnDeviceCallback(Callback):\n    \"\"\"\n    PutDataOnDevice class is a custom callback used to move the input data and target labels to the device (CPU or GPU) before passing them to the model.\n\n    Methods:\n        begin_fit: Moves the model to the device at the beginning of the fitting process.\n        begin_batch: Moves the input data and target labels to the device at the beginning of each batch.\n    \"\"\"\n\n    def __init__(self, device):\n        self.device = device\n\n    def begin_fit(self):\n        self.model.to(self.device)\n\n    def begin_batch(self):\n        self.run.xb, self.run.yb = self.xb.to(self.device), self.yb.to(self.device)\n\n    def after_pred(self):\n        self.run.pred = self.run.pred.to(self.device)\n        self.run.yb = self.run.yb.to(self.device)\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.Recorder","title":"<code>Recorder</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Recorder is a callback class used to record learning rates, losses, and metrics during the training process.</p> Source code in <code>openml_pytorch/callbacks/recording.py</code> <pre><code>class Recorder(Callback):\n    \"\"\"\n    Recorder is a callback class used to record learning rates, losses, and metrics during the training process.\n    \"\"\"\n\n    def begin_fit(self):\n        \"\"\"\n        Initializes attributes necessary for the fitting process.\n        \"\"\"\n        self.lrs = [[] for _ in self.opt.param_groups]\n        self.losses = []\n        self.metrics = (\n            {metric.__name__: [] for metric in self.metrics}\n            if hasattr(self, \"metrics\")\n            else {}\n        )\n        self.epochs = []\n        self.current_epoch = 0\n\n    def begin_epoch(self):\n        \"\"\"\n        Handles operations at the beginning of each epoch.\n        \"\"\"\n        self.current_epoch += 1\n\n    def after_batch(self):\n        \"\"\"\n        Handles operations to execute after each training batch.\n        \"\"\"\n        if not self.in_train:\n            return\n\n        for pg, lr in zip(self.opt.param_groups, self.lrs):\n            lr.append(pg[\"lr\"])\n\n        self.losses.append(self.loss.detach().cpu())\n\n    def after_epoch(self):\n        \"\"\"\n        Records metrics at the end of each epoch.\n        \"\"\"\n        self.epochs.append(self.current_epoch)\n        # Record metrics from AvgStatsCallback if available\n        if hasattr(self, \"run\"):\n            for cb in self.run.cbs:\n                if isinstance(cb, AvgStatsCallback):\n                    for i, metric_fn in enumerate(cb.train_stats.metrics):\n                        metric_name = metric_fn.__name__\n                        if metric_name == \"\":\n                            metric_name = str(metric_fn)\n                        if metric_name not in self.metrics:\n                            self.metrics[metric_name] = []\n                        # Store both train and valid metrics\n                        self.metrics[metric_name].append(\n                            {\n                                \"train\": cb.train_stats.avg_stats[\n                                    i + 1\n                                ],  # +1 because avg_stats includes loss as first element\n                                \"valid\": cb.valid_stats.avg_stats[i + 1],\n                            }\n                        )\n\n    def plot_lr(self, pgid=-1, save_path=None):\n        \"\"\"\n        Plots the learning rate for a given parameter group.\n        \"\"\"\n        # check if empty\n        if not self.lrs[pgid]:\n            print(\"No learning rates recorded.\")\n            return\n        plot = plt.plot(self.lrs[pgid])\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Learning Rate\")\n        if save_path:\n            plt.savefig(save_path)\n        return plot\n\n    def plot_loss(self, skip_last=0, save_path=None):\n        \"\"\"\n        Plots the loss values.\n        \"\"\"\n        # check if empty\n        if not self.losses:\n            print(\"No losses recorded.\")\n            return\n        plot = plt.plot(self.losses[: len(self.losses) - skip_last])\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        if save_path:\n            plt.savefig(save_path)\n        return plot\n\n    def plot(self, skip_last=0, pgid=-1):\n        \"\"\"\n        Generates a plot of the loss values against the learning rates.\n        \"\"\"\n        losses = [o.item() for o in self.losses]\n        lrs = self.lrs[pgid]\n        n = len(losses) - skip_last\n        plt.xscale(\"log\")\n        plt.xlabel(\"Learning Rate\")\n        plt.ylabel(\"Loss\")\n        return plt.plot(lrs[:n], losses[:n])\n\n    def plot_metric(self, metric_name, skip_last=0, save_path=None):\n        \"\"\"\n        Plots a specific metric over epochs.\n\n        Args:\n            metric_name (str): Name of the metric to plot\n            skip_last (int): Number of last points to skip\n        \"\"\"\n        # check if empty\n        if not self.metrics:\n            print(\"No metrics recorded.\")\n            return\n        if metric_name not in self.metrics:\n            print(\n                f\"Metric '{metric_name}' not found. Available metrics: {list(self.metrics.keys())}\"\n            )\n            return\n\n        train_vals = [d[\"train\"] for d in self.metrics[metric_name]]\n        valid_vals = [d[\"valid\"] for d in self.metrics[metric_name]]\n\n        # convert to cpu numpy if necessary\n        train_vals = [\n            val.item() if isinstance(val, torch.Tensor) else val for val in train_vals\n        ]\n        valid_vals = [\n            val.item() if isinstance(val, torch.Tensor) else val for val in valid_vals\n        ]\n\n        plt.figure(figsize=(10, 6))\n        plt.plot(\n            self.epochs[:-skip_last] if skip_last &gt; 0 else self.epochs,\n            train_vals[:-skip_last] if skip_last &gt; 0 else train_vals,\n            label=f\"Train {metric_name}\",\n        )\n        plt.plot(\n            self.epochs[:-skip_last] if skip_last &gt; 0 else self.epochs,\n            valid_vals[:-skip_last] if skip_last &gt; 0 else valid_vals,\n            label=f\"Valid {metric_name}\",\n        )\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(metric_name)\n        plt.title(f\"{metric_name} vs. Epochs\")\n        plt.legend()\n        if save_path:\n            plt.savefig(save_path)\n        plt.show()\n        return plt\n\n    def plot_all_metrics(self, skip_last=0, save_path=None):\n        \"\"\"\n        Plots all available metrics in subplots.\n\n        Args:\n            skip_last (int): Number of last points to skip for all metrics\n        \"\"\"\n        # check if empty\n        if not self.metrics:\n            print(\"No metrics recorded.\")\n            return\n        if len(self.metrics) == 0:\n            print(\"No metrics recorded.\")\n            return\n        num_metrics = len(self.metrics)\n        fig, axes = plt.subplots(num_metrics, 1, figsize=(10, 6 * num_metrics))\n\n        # If there's only one metric, axes is not a list, so make sure we handle that.\n        if num_metrics == 1:\n            axes = [axes]\n\n        for i, (metric_name, metric_data) in enumerate(self.metrics.items()):\n            train_vals = [d[\"train\"] for d in metric_data]\n            valid_vals = [d[\"valid\"] for d in metric_data]\n\n            # convert to cpu numpy if necessary\n            train_vals = [\n                val.item() if isinstance(val, torch.Tensor) else val\n                for val in train_vals\n            ]\n            valid_vals = [\n                val.item() if isinstance(val, torch.Tensor) else val\n                for val in valid_vals\n            ]\n\n            # Plot the data\n            axes[i].plot(\n                self.epochs[:-skip_last] if skip_last &gt; 0 else self.epochs,\n                train_vals[:-skip_last] if skip_last &gt; 0 else train_vals,\n                label=f\"Train {metric_name}\",\n            )\n            axes[i].plot(\n                self.epochs[:-skip_last] if skip_last &gt; 0 else self.epochs,\n                valid_vals[:-skip_last] if skip_last &gt; 0 else valid_vals,\n                label=f\"Valid {metric_name}\",\n            )\n            axes[i].set_xlabel(\"Epochs\")\n            axes[i].set_ylabel(metric_name)\n            axes[i].set_title(f\"{metric_name} vs. Epochs\")\n            axes[i].legend()\n\n        # Adjust layout\n        plt.tight_layout()\n        if save_path:\n            plt.savefig(save_path)\n        plt.show()\n        return plt\n\n    def get_metrics_history(self):\n        \"\"\"\n        Returns a dictionary containing the history of all recorded metrics.\n\n        Returns:\n            dict: A dictionary with metric names as keys and lists of values as values\n        \"\"\"\n        return self.metrics\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.Recorder.after_batch","title":"<code>after_batch()</code>","text":"<p>Handles operations to execute after each training batch.</p> Source code in <code>openml_pytorch/callbacks/recording.py</code> <pre><code>def after_batch(self):\n    \"\"\"\n    Handles operations to execute after each training batch.\n    \"\"\"\n    if not self.in_train:\n        return\n\n    for pg, lr in zip(self.opt.param_groups, self.lrs):\n        lr.append(pg[\"lr\"])\n\n    self.losses.append(self.loss.detach().cpu())\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.Recorder.after_epoch","title":"<code>after_epoch()</code>","text":"<p>Records metrics at the end of each epoch.</p> Source code in <code>openml_pytorch/callbacks/recording.py</code> <pre><code>def after_epoch(self):\n    \"\"\"\n    Records metrics at the end of each epoch.\n    \"\"\"\n    self.epochs.append(self.current_epoch)\n    # Record metrics from AvgStatsCallback if available\n    if hasattr(self, \"run\"):\n        for cb in self.run.cbs:\n            if isinstance(cb, AvgStatsCallback):\n                for i, metric_fn in enumerate(cb.train_stats.metrics):\n                    metric_name = metric_fn.__name__\n                    if metric_name == \"\":\n                        metric_name = str(metric_fn)\n                    if metric_name not in self.metrics:\n                        self.metrics[metric_name] = []\n                    # Store both train and valid metrics\n                    self.metrics[metric_name].append(\n                        {\n                            \"train\": cb.train_stats.avg_stats[\n                                i + 1\n                            ],  # +1 because avg_stats includes loss as first element\n                            \"valid\": cb.valid_stats.avg_stats[i + 1],\n                        }\n                    )\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.Recorder.begin_epoch","title":"<code>begin_epoch()</code>","text":"<p>Handles operations at the beginning of each epoch.</p> Source code in <code>openml_pytorch/callbacks/recording.py</code> <pre><code>def begin_epoch(self):\n    \"\"\"\n    Handles operations at the beginning of each epoch.\n    \"\"\"\n    self.current_epoch += 1\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.Recorder.begin_fit","title":"<code>begin_fit()</code>","text":"<p>Initializes attributes necessary for the fitting process.</p> Source code in <code>openml_pytorch/callbacks/recording.py</code> <pre><code>def begin_fit(self):\n    \"\"\"\n    Initializes attributes necessary for the fitting process.\n    \"\"\"\n    self.lrs = [[] for _ in self.opt.param_groups]\n    self.losses = []\n    self.metrics = (\n        {metric.__name__: [] for metric in self.metrics}\n        if hasattr(self, \"metrics\")\n        else {}\n    )\n    self.epochs = []\n    self.current_epoch = 0\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.Recorder.get_metrics_history","title":"<code>get_metrics_history()</code>","text":"<p>Returns a dictionary containing the history of all recorded metrics.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with metric names as keys and lists of values as values</p> Source code in <code>openml_pytorch/callbacks/recording.py</code> <pre><code>def get_metrics_history(self):\n    \"\"\"\n    Returns a dictionary containing the history of all recorded metrics.\n\n    Returns:\n        dict: A dictionary with metric names as keys and lists of values as values\n    \"\"\"\n    return self.metrics\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.Recorder.plot","title":"<code>plot(skip_last=0, pgid=-1)</code>","text":"<p>Generates a plot of the loss values against the learning rates.</p> Source code in <code>openml_pytorch/callbacks/recording.py</code> <pre><code>def plot(self, skip_last=0, pgid=-1):\n    \"\"\"\n    Generates a plot of the loss values against the learning rates.\n    \"\"\"\n    losses = [o.item() for o in self.losses]\n    lrs = self.lrs[pgid]\n    n = len(losses) - skip_last\n    plt.xscale(\"log\")\n    plt.xlabel(\"Learning Rate\")\n    plt.ylabel(\"Loss\")\n    return plt.plot(lrs[:n], losses[:n])\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.Recorder.plot_all_metrics","title":"<code>plot_all_metrics(skip_last=0, save_path=None)</code>","text":"<p>Plots all available metrics in subplots.</p> <p>Parameters:</p> Name Type Description Default <code>skip_last</code> <code>int</code> <p>Number of last points to skip for all metrics</p> <code>0</code> Source code in <code>openml_pytorch/callbacks/recording.py</code> <pre><code>def plot_all_metrics(self, skip_last=0, save_path=None):\n    \"\"\"\n    Plots all available metrics in subplots.\n\n    Args:\n        skip_last (int): Number of last points to skip for all metrics\n    \"\"\"\n    # check if empty\n    if not self.metrics:\n        print(\"No metrics recorded.\")\n        return\n    if len(self.metrics) == 0:\n        print(\"No metrics recorded.\")\n        return\n    num_metrics = len(self.metrics)\n    fig, axes = plt.subplots(num_metrics, 1, figsize=(10, 6 * num_metrics))\n\n    # If there's only one metric, axes is not a list, so make sure we handle that.\n    if num_metrics == 1:\n        axes = [axes]\n\n    for i, (metric_name, metric_data) in enumerate(self.metrics.items()):\n        train_vals = [d[\"train\"] for d in metric_data]\n        valid_vals = [d[\"valid\"] for d in metric_data]\n\n        # convert to cpu numpy if necessary\n        train_vals = [\n            val.item() if isinstance(val, torch.Tensor) else val\n            for val in train_vals\n        ]\n        valid_vals = [\n            val.item() if isinstance(val, torch.Tensor) else val\n            for val in valid_vals\n        ]\n\n        # Plot the data\n        axes[i].plot(\n            self.epochs[:-skip_last] if skip_last &gt; 0 else self.epochs,\n            train_vals[:-skip_last] if skip_last &gt; 0 else train_vals,\n            label=f\"Train {metric_name}\",\n        )\n        axes[i].plot(\n            self.epochs[:-skip_last] if skip_last &gt; 0 else self.epochs,\n            valid_vals[:-skip_last] if skip_last &gt; 0 else valid_vals,\n            label=f\"Valid {metric_name}\",\n        )\n        axes[i].set_xlabel(\"Epochs\")\n        axes[i].set_ylabel(metric_name)\n        axes[i].set_title(f\"{metric_name} vs. Epochs\")\n        axes[i].legend()\n\n    # Adjust layout\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path)\n    plt.show()\n    return plt\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.Recorder.plot_loss","title":"<code>plot_loss(skip_last=0, save_path=None)</code>","text":"<p>Plots the loss values.</p> Source code in <code>openml_pytorch/callbacks/recording.py</code> <pre><code>def plot_loss(self, skip_last=0, save_path=None):\n    \"\"\"\n    Plots the loss values.\n    \"\"\"\n    # check if empty\n    if not self.losses:\n        print(\"No losses recorded.\")\n        return\n    plot = plt.plot(self.losses[: len(self.losses) - skip_last])\n    plt.xlabel(\"Iterations\")\n    plt.ylabel(\"Loss\")\n\n    if save_path:\n        plt.savefig(save_path)\n    return plot\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.Recorder.plot_lr","title":"<code>plot_lr(pgid=-1, save_path=None)</code>","text":"<p>Plots the learning rate for a given parameter group.</p> Source code in <code>openml_pytorch/callbacks/recording.py</code> <pre><code>def plot_lr(self, pgid=-1, save_path=None):\n    \"\"\"\n    Plots the learning rate for a given parameter group.\n    \"\"\"\n    # check if empty\n    if not self.lrs[pgid]:\n        print(\"No learning rates recorded.\")\n        return\n    plot = plt.plot(self.lrs[pgid])\n    plt.xlabel(\"Iterations\")\n    plt.ylabel(\"Learning Rate\")\n    if save_path:\n        plt.savefig(save_path)\n    return plot\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.Recorder.plot_metric","title":"<code>plot_metric(metric_name, skip_last=0, save_path=None)</code>","text":"<p>Plots a specific metric over epochs.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str</code> <p>Name of the metric to plot</p> required <code>skip_last</code> <code>int</code> <p>Number of last points to skip</p> <code>0</code> Source code in <code>openml_pytorch/callbacks/recording.py</code> <pre><code>def plot_metric(self, metric_name, skip_last=0, save_path=None):\n    \"\"\"\n    Plots a specific metric over epochs.\n\n    Args:\n        metric_name (str): Name of the metric to plot\n        skip_last (int): Number of last points to skip\n    \"\"\"\n    # check if empty\n    if not self.metrics:\n        print(\"No metrics recorded.\")\n        return\n    if metric_name not in self.metrics:\n        print(\n            f\"Metric '{metric_name}' not found. Available metrics: {list(self.metrics.keys())}\"\n        )\n        return\n\n    train_vals = [d[\"train\"] for d in self.metrics[metric_name]]\n    valid_vals = [d[\"valid\"] for d in self.metrics[metric_name]]\n\n    # convert to cpu numpy if necessary\n    train_vals = [\n        val.item() if isinstance(val, torch.Tensor) else val for val in train_vals\n    ]\n    valid_vals = [\n        val.item() if isinstance(val, torch.Tensor) else val for val in valid_vals\n    ]\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(\n        self.epochs[:-skip_last] if skip_last &gt; 0 else self.epochs,\n        train_vals[:-skip_last] if skip_last &gt; 0 else train_vals,\n        label=f\"Train {metric_name}\",\n    )\n    plt.plot(\n        self.epochs[:-skip_last] if skip_last &gt; 0 else self.epochs,\n        valid_vals[:-skip_last] if skip_last &gt; 0 else valid_vals,\n        label=f\"Valid {metric_name}\",\n    )\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(metric_name)\n    plt.title(f\"{metric_name} vs. Epochs\")\n    plt.legend()\n    if save_path:\n        plt.savefig(save_path)\n    plt.show()\n    return plt\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.TensorBoardCallback","title":"<code>TensorBoardCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Log specific things to TensorBoard. - Model</p> Source code in <code>openml_pytorch/callbacks/tensorboard.py</code> <pre><code>class TensorBoardCallback(Callback):\n    \"\"\"\n    Log specific things to TensorBoard.\n    - Model\n    \"\"\"\n\n    def __init__(self, writer):\n        self.writer = writer\n\n    def begin_batch(self):\n        if \"saved_graph\" not in self.__dict__ or not self.saved_graph:\n            self.writer.add_graph(self.model, self.xb)\n            self.saved_graph = True\n\n    def after_fit(self):\n        # check if tensorboard writer is available\n        try:\n            # add loss and learning rate  to tensorboard\n            self.writer.add_scalar(\"Loss\", self.run.loss, self.n_iter)\n            self.writer.add_scalar(\n                \"Learning rate\", self.run.opt.param_groups[0][\"lr\"], self.n_iter\n            )\n        except Exception as e:\n            print(f\"Error: {e}\")\n        self.writer.close()\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.TestCallback","title":"<code>TestCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>TestCallback class is a custom callback used to test the training loop by stopping the training process after 2 iterations. Useful for debugging and testing purposes, not intended for actual training.</p> Source code in <code>openml_pytorch/callbacks/training_callbacks.py</code> <pre><code>class TestCallback(Callback):\n    \"\"\"\n    TestCallback class is a custom callback used to test the training loop by stopping the training process after 2 iterations. Useful for debugging and testing purposes, not intended for actual training.\n    \"\"\"\n\n    def after_step(self):\n        if self.n_iter &gt;= 1:\n            raise CancelTrainException()\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.TrainEvalCallback","title":"<code>TrainEvalCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>TrainEvalCallback class is a custom callback used during the training and validation phases of a machine learning model to perform specific actions at the beginning and after certain events.</p> <p>Methods:</p> <p>begin_fit():     Initialize the number of epochs and iteration counts at the start     of the fitting process.</p> <p>after_batch():     Update the epoch and iteration counts after each batch during     training.</p> <p>begin_epoch():     Set the current epoch, switch the model to training mode, and     indicate that the model is in training.</p> <p>begin_validate():     Switch the model to evaluation mode and indicate that the model     is in validation.</p> Source code in <code>openml_pytorch/callbacks/training_callbacks.py</code> <pre><code>class TrainEvalCallback(Callback):\n    \"\"\"\n    TrainEvalCallback class is a custom callback used during the training\n    and validation phases of a machine learning model to perform specific\n    actions at the beginning and after certain events.\n\n    Methods:\n\n    begin_fit():\n        Initialize the number of epochs and iteration counts at the start\n        of the fitting process.\n\n    after_batch():\n        Update the epoch and iteration counts after each batch during\n        training.\n\n    begin_epoch():\n        Set the current epoch, switch the model to training mode, and\n        indicate that the model is in training.\n\n    begin_validate():\n        Switch the model to evaluation mode and indicate that the model\n        is in validation.\n    \"\"\"\n\n    def begin_fit(self):\n        self.run.n_epochs = 0\n        self.run.n_iter = 0\n\n    def after_batch(self):\n        if not self.in_train:\n            return\n        self.run.n_epochs += 1.0 / self.iters\n        self.run.n_iter += 1\n\n    def begin_epoch(self):\n        self.run.n_epochs = self.epoch\n        self.model.train()\n        self.run.in_train = True\n\n    def begin_validate(self):\n        self.model.eval()\n        self.run.in_train = False\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.annealer","title":"<code>annealer(f)</code>","text":"<p>A decorator function for creating a partially applied function with predefined start and end arguments. The inner function <code>_inner</code> captures the <code>start</code> and <code>end</code> parameters and returns a <code>partial</code> object that fixes these parameters for the decorated function <code>f</code>.</p> Source code in <code>openml_pytorch/callbacks/annealing.py</code> <pre><code>def annealer(f) -&gt; callable:\n    \"\"\"\n    A decorator function for creating a partially applied function with predefined start and end arguments.\n    The inner function `_inner` captures the `start` and `end` parameters and returns a `partial` object that fixes these parameters for the decorated function `f`.\n    \"\"\"\n\n    def _inner(start, end):\n        return partial(f, start, end)\n\n    return _inner\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.camel2snake","title":"<code>camel2snake(name)</code>","text":"<p>Convert <code>name</code> from camel case to snake case.</p> Source code in <code>openml_pytorch/callbacks/helper.py</code> <pre><code>def camel2snake(name: str) -&gt; str:\n    \"\"\"\n    Convert `name` from camel case to snake case.\n    \"\"\"\n    s1 = re.sub(_camel_re1, r\"\\1_\\2\", name)\n    return re.sub(_camel_re2, r\"\\1_\\2\", s1).lower()\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.combine_scheds","title":"<code>combine_scheds(pcts, scheds)</code>","text":"<p>Combine multiple scheduling functions.</p> Source code in <code>openml_pytorch/callbacks/annealing.py</code> <pre><code>def combine_scheds(pcts: Iterable[float], scheds: Iterable[callable]) -&gt; callable:\n    \"\"\"\n    Combine multiple scheduling functions.\n    \"\"\"\n    assert sum(pcts) == 1.0\n    pcts = torch.tensor([0] + listify(pcts))\n    assert torch.all(pcts &gt;= 0)\n    pcts = torch.cumsum(pcts, 0)\n\n    def _inner(pos):\n        idx = (pos &gt;= pcts).nonzero().max()\n        actual_pos = (pos - pcts[idx]) / (pcts[idx + 1] - pcts[idx])\n        return scheds[idx](actual_pos)\n\n    return _inner\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.listify","title":"<code>listify(o=None)</code>","text":"<p>Convert <code>o</code> to list. If <code>o</code> is None, return empty list.</p> Source code in <code>openml_pytorch/callbacks/helper.py</code> <pre><code>def listify(o=None) -&gt; list:\n    \"\"\"\n    Convert `o` to list. If `o` is None, return empty list.\n    \"\"\"\n    if o is None:\n        return []\n    if isinstance(o, list):\n        return o\n    if isinstance(o, str):\n        return [o]\n    if isinstance(o, Iterable):\n        return list(o)\n    return [o]\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.sched_cos","title":"<code>sched_cos(start, end, pos)</code>","text":"<p>A cosine schedule function.</p> Source code in <code>openml_pytorch/callbacks/annealing.py</code> <pre><code>@annealer\ndef sched_cos(start: float, end: float, pos: float) -&gt; float:\n    \"\"\"\n    A cosine schedule function.\n    \"\"\"\n    return start + (1 + math.cos(math.pi * (1 - pos))) * (end - start) / 2\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.sched_exp","title":"<code>sched_exp(start, end, pos)</code>","text":"<p>Exponential schedule function.</p> Source code in <code>openml_pytorch/callbacks/annealing.py</code> <pre><code>@annealer\ndef sched_exp(start: float, end: float, pos: float) -&gt; float:\n    \"\"\"\n    Exponential schedule function.\n    \"\"\"\n    if start == 0.0:\n        start = 0.00001\n    return start * (end / start) ** pos\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.sched_lin","title":"<code>sched_lin(start, end, pos)</code>","text":"<p>A linear schedule function.</p> Source code in <code>openml_pytorch/callbacks/annealing.py</code> <pre><code>@annealer\ndef sched_lin(start: float, end: float, pos: float) -&gt; float:\n    \"\"\"\n    A linear schedule function.\n    \"\"\"\n    return start + pos * (end - start)\n</code></pre>"},{"location":"API%20reference/Callbacks/#callbacks.sched_no","title":"<code>sched_no(start, end, pos)</code>","text":"<p>Disabled scheduling.</p> Source code in <code>openml_pytorch/callbacks/annealing.py</code> <pre><code>@annealer\ndef sched_no(start: float, end: float, pos: float) -&gt; float:\n    \"\"\"\n    Disabled scheduling.\n    \"\"\"\n    return start\n</code></pre>"},{"location":"API%20reference/Custom%20Datasets/","title":"Custom Datasets","text":"<p>This module contains the custom datasets for OpenML datasets.</p>"},{"location":"API%20reference/Custom%20Datasets/#custom_datasets.GenericDataset","title":"<code>GenericDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Generic dataset that takes X,y as input and returns them as tensors</p> Source code in <code>openml_pytorch/custom_datasets/generic_dataset.py</code> <pre><code>class GenericDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Generic dataset that takes X,y as input and returns them as tensors\"\"\"\n\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)  # Convert to tensors\n        self.y = torch.tensor(y, dtype=torch.long)  # Ensure labels are LongTensor\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n</code></pre>"},{"location":"API%20reference/Custom%20Datasets/#custom_datasets.OpenMLImageDataset","title":"<code>OpenMLImageDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Class representing an image dataset from OpenML for use in PyTorch.</p> <p>Methods:</p> <pre><code>__init__(self, X, y, image_size, image_dir, transform_x=None, transform_y=None)\n    Initializes the dataset with given data, image size, directory, and optional transformations.\n\n__getitem__(self, idx)\n    Retrieves an image and its corresponding label (if available) from the dataset at the specified index. Applies transformations if provided.\n\n__len__(self)\n    Returns the total number of images in the dataset.\n</code></pre> Source code in <code>openml_pytorch/custom_datasets/image_dataset.py</code> <pre><code>class OpenMLImageDataset(Dataset):\n    \"\"\"\n    Class representing an image dataset from OpenML for use in PyTorch.\n\n    Methods:\n\n        __init__(self, X, y, image_size, image_dir, transform_x=None, transform_y=None)\n            Initializes the dataset with given data, image size, directory, and optional transformations.\n\n        __getitem__(self, idx)\n            Retrieves an image and its corresponding label (if available) from the dataset at the specified index. Applies transformations if provided.\n\n        __len__(self)\n            Returns the total number of images in the dataset.\n    \"\"\"\n\n    def __init__(self, X, y, image_size, image_dir, transform_x=None, transform_y=None):\n        self.X = X\n        self.y = y\n        self.image_size = image_size\n        self.image_dir = image_dir\n        self.transform_x = transform_x\n        self.transform_y = transform_y\n\n    def __getitem__(self, idx):\n        img_name = str(os.path.join(self.image_dir, self.X.iloc[idx, 0]))\n        # hotfix for .DS_Store files\n        if \".DS_Store\" in img_name:\n            return self.__getitem__((idx + 1) % len(self))\n\n        # Open the image using PIL instead of read_image\n        try:\n            image = Image.open(img_name).convert(\"RGB\")  # Ensure it's in RGB mode\n        except Exception as e:\n            print(f\"Error opening image {img_name}: {e}\")\n            return self.__getitem__((idx + 1) % len(self))\n\n        # Resize using PIL-based transform\n        image = T.Resize((self.image_size, self.image_size))(image)\n        # Convert to tensor after all PIL transformations\n        image = T.ToTensor()(image)\n        # Apply additional transformations if provided\n        if self.transform_x is not None:\n            image = self.transform_x(image)\n\n        if self.y is not None:\n            label = self.y.iloc[idx]\n            if label is not None:\n                if self.transform_y is not None:\n                    label = self.transform_y(label)\n                return image, label\n        else:\n            return image\n\n    def __len__(self):\n        return len(self.X)\n</code></pre>"},{"location":"API%20reference/Custom%20Datasets/#custom_datasets.OpenMLTabularDataset","title":"<code>OpenMLTabularDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>OpenMLTabularDataset</p> <p>A custom dataset class to handle tabular data from OpenML (or any similar tabular dataset). It encodes categorical features and the target column using LabelEncoder from sklearn.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the dataset with the data and the target column.              Encodes the categorical features and target if provided.</p> <code>__getitem__</code> <p>Retrieves the input data and target value at the specified index.               Converts the data to tensors and returns them.</p> <code>__len__</code> <p>Returns the length of the dataset.</p> Source code in <code>openml_pytorch/custom_datasets/tabular_dataset.py</code> <pre><code>class OpenMLTabularDataset(Dataset):\n    \"\"\"\n    OpenMLTabularDataset\n\n    A custom dataset class to handle tabular data from OpenML (or any similar tabular dataset).\n    It encodes categorical features and the target column using LabelEncoder from sklearn.\n\n    Methods:\n        __init__(X, y) : Initializes the dataset with the data and the target column.\n                         Encodes the categorical features and target if provided.\n\n        __getitem__(idx): Retrieves the input data and target value at the specified index.\n                          Converts the data to tensors and returns them.\n\n        __len__(): Returns the length of the dataset.\n    \"\"\"\n\n    def __init__(self, X, y):\n        self.data = X\n        # self.target_col_name = target_col\n        for col in self.data.select_dtypes(include=[\"object\", \"category\"]):\n            # convert to float\n            self.data[col] = self.data[col].astype(\"category\").cat.codes\n        self.label_mapping = None\n\n        self.y = y\n\n    def __getitem__(self, idx):\n        # x is the input data, y is the target value from the target column\n        x = self.data.iloc[idx, :]\n        try:\n            x = torch.tensor(x.values.astype(\"float32\"))\n        except Exception as e:\n            print(f\"Error converting data to tensor: {e}\")\n            return self.__getitem__((idx + 1) % len(self))\n        if self.y is not None:\n            y = self.y[idx]\n            y = torch.tensor(y)\n            return x, y\n        else:\n            return x\n\n    def __len__(self):\n        return len(self.data)\n</code></pre>"},{"location":"API%20reference/Metrics/","title":"Metrics","text":"<p>This module provides utility functions for evaluating model performance and activation functions. It includes functions to compute the accuracy, top-k accuracy of model predictions, and the sigmoid function.</p>"},{"location":"API%20reference/Metrics/#metrics.accuracy","title":"<code>accuracy(out, yb)</code>","text":"<p>Computes the accuracy of model predictions.</p> <p>Parameters: out (Tensor): The output tensor from the model, containing predicted class scores. yb (Tensor): The ground truth labels tensor.</p> <p>Returns: Tensor: The mean accuracy of the predictions, computed as a float tensor.</p> Source code in <code>openml_pytorch/metrics.py</code> <pre><code>def accuracy(out, yb):\n    \"\"\"\n\n    Computes the accuracy of model predictions.\n\n    Parameters:\n    out (Tensor): The output tensor from the model, containing predicted class scores.\n    yb (Tensor): The ground truth labels tensor.\n\n    Returns:\n    Tensor: The mean accuracy of the predictions, computed as a float tensor.\n    \"\"\"\n    return (torch.argmax(out, dim=1) == yb.long()).float().mean()\n</code></pre>"},{"location":"API%20reference/Metrics/#metrics.accuracy_topk","title":"<code>accuracy_topk(out, yb, k=5)</code>","text":"<p>Computes the top-k accuracy of the given model outputs.</p> <p>Parameters:</p> Name Type Description Default <code>out</code> <code>Tensor</code> <p>The output predictions of the model, of shape (batch_size, num_classes).</p> required <code>yb</code> <code>Tensor</code> <p>The ground truth labels, of shape (batch_size,).</p> required <code>k</code> <code>int</code> <p>The number of top predictions to consider. Default is 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The top-k accuracy as a float value.</p> <p>The function calculates how often the true label is among the top-k predicted labels.</p> Source code in <code>openml_pytorch/metrics.py</code> <pre><code>def accuracy_topk(out, yb, k=5):\n    \"\"\"\n\n    Computes the top-k accuracy of the given model outputs.\n\n    Args:\n        out (torch.Tensor): The output predictions of the model, of shape (batch_size, num_classes).\n        yb (torch.Tensor): The ground truth labels, of shape (batch_size,).\n        k (int, optional): The number of top predictions to consider. Default is 5.\n\n    Returns:\n        float: The top-k accuracy as a float value.\n\n    The function calculates how often the true label is among the top-k predicted labels.\n    \"\"\"\n    return (torch.topk(out, k, dim=1)[1] == yb.long().unsqueeze(1)).float().mean()\n</code></pre>"},{"location":"API%20reference/Metrics/#metrics.f1_score","title":"<code>f1_score(out, yb)</code>","text":"<p>Computes the F1 score for the given model outputs and true labels.</p> <p>Parameters:</p> Name Type Description Default <code>out</code> <code>Tensor</code> <p>The output predictions of the model, of shape (batch_size, num_classes).</p> required <code>yb</code> <code>Tensor</code> <p>The ground truth labels, of shape (batch_size,).</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The F1 score as a float value.</p> Source code in <code>openml_pytorch/metrics.py</code> <pre><code>def f1_score(out, yb):\n    \"\"\"\n    Computes the F1 score for the given model outputs and true labels.\n\n    Args:\n        out (torch.Tensor): The output predictions of the model, of shape (batch_size, num_classes).\n        yb (torch.Tensor): The ground truth labels, of shape (batch_size,).\n\n    Returns:\n        float: The F1 score as a float value.\n    \"\"\"\n    pred = torch.argmax(out, dim=1)\n    tp = torch.sum((pred == 1) &amp; (yb == 1)).float()\n    fp = torch.sum((pred == 1) &amp; (yb == 0)).float()\n    fn = torch.sum((pred == 0) &amp; (yb == 1)).float()\n\n    precision = tp / (tp + fp + 1e-8)\n    recall = tp / (tp + fn + 1e-8)\n\n    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n\n    return f1\n</code></pre>"},{"location":"API%20reference/OpenML%20Connection/","title":"OpenML Connection","text":"<p>This module defines the Pytorch extension for OpenML-python.</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension","title":"<code>PytorchExtension</code>","text":"<p>               Bases: <code>Extension</code></p> <p>Connect Pytorch to OpenML-Python.</p> Source code in <code>openml_pytorch/extension.py</code> <pre><code>class PytorchExtension(Extension):\n    \"\"\"Connect Pytorch to OpenML-Python.\"\"\"\n\n    ################################################################################################\n    # General setup\n\n    @classmethod\n    def can_handle_flow(cls, flow: \"OpenMLFlow\") -&gt; bool:\n        \"\"\"Check whether a given describes a Pytorch estimator.\n\n        This is done by parsing the ``external_version`` field.\n\n        Parameters\n        ----------\n        flow : OpenMLFlow\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return cls._is_pytorch_flow(flow)\n\n    @classmethod\n    def can_handle_model(cls, model: Any) -&gt; bool:\n        \"\"\"Check whether a model is an instance of ``torch.nn.Module``.\n\n        Parameters\n        ----------\n        model : Any\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        from torch.nn import Module\n\n        return isinstance(model, Module) or isinstance(model, Callable)\n\n    ################################################################################################\n    # Method for dataloader\n\n    ################################################################################################\n    # Methods for flow serialization and de-serialization\n\n    def flow_to_model(\n        self, flow: \"OpenMLFlow\", initialize_with_defaults: bool = False\n    ) -&gt; Any:\n        \"\"\"Initializes a Pytorch model based on a flow.\n\n        Parameters\n        ----------\n        flow : mixed\n            the object to deserialize (can be flow object, or any serialized\n            parameter value that is accepted by)\n\n        initialize_with_defaults : bool, optional (default=False)\n            If this flag is set, the hyperparameter values of flows will be\n            ignored and a flow with its defaults is returned.\n\n        Returns\n        -------\n        mixed\n        \"\"\"\n        return self._deserialize_pytorch(\n            flow, initialize_with_defaults=initialize_with_defaults\n        )\n\n    def _deserialize_pytorch(\n        self,\n        o: Any,\n        components: Optional[Dict] = None,\n        initialize_with_defaults: bool = False,\n        recursion_depth: int = 0,\n    ) -&gt; Any:\n        \"\"\"Recursive function to deserialize a Pytorch flow.\n\n        Parameters and description remain the same as in your provided code.\n\n        \"\"\"\n        logging.info(\n            \"-%s flow_to_pytorch START o=%s, components=%s, \"\n            \"init_defaults=%s\"\n            % (\"-\" * recursion_depth, o, components, initialize_with_defaults)\n        )\n        depth_pp = recursion_depth + 1  # shortcut var, depth plus plus\n\n        if isinstance(o, str):\n            try:\n                o = json.loads(o)\n            except JSONDecodeError:\n                pass\n\n        if isinstance(o, dict):\n            if \"oml-python:serialized_object\" in o:\n                serialized_type = o[\"oml-python:serialized_object\"]\n                value = o[\"value\"]\n                if serialized_type == \"type\":\n                    rval = self._deserialize_type(value)\n                elif serialized_type == \"function\":\n                    rval = self._deserialize_function(value)\n                elif serialized_type == \"methoddescriptor\":\n                    rval = self._deserialize_methoddescriptor(value)\n                elif serialized_type == \"component_reference\":\n                    assert components is not None  # Necessary for mypy\n                    value = self._deserialize_pytorch(value, recursion_depth=depth_pp)\n                    step_name = value[\"step_name\"]\n                    key = value[\"key\"]\n                    if key not in components:\n                        key = str(key)\n                    component = self._deserialize_pytorch(\n                        components[key],\n                        initialize_with_defaults=initialize_with_defaults,\n                        recursion_depth=depth_pp,\n                    )\n                    del components[key]\n                    if step_name is None:\n                        rval = component\n                    elif \"argument_1\" not in value:\n                        rval = (step_name, component)\n                    else:\n                        rval = (step_name, component, value[\"argument_1\"])\n                else:\n                    raise ValueError(\"Cannot flow_to_pytorch %s\" % serialized_type)\n\n            else:\n                rval = OrderedDict(\n                    (\n                        self._deserialize_pytorch(\n                            o=key,\n                            components=components,\n                            initialize_with_defaults=initialize_with_defaults,\n                            recursion_depth=depth_pp,\n                        ),\n                        self._deserialize_pytorch(\n                            o=value,\n                            components=components,\n                            initialize_with_defaults=initialize_with_defaults,\n                            recursion_depth=depth_pp,\n                        ),\n                    )\n                    for key, value in sorted(o.items())\n                )\n        elif isinstance(o, (list, tuple)):\n            rval = [\n                self._deserialize_pytorch(\n                    o=element,\n                    components=components,\n                    initialize_with_defaults=initialize_with_defaults,\n                    recursion_depth=depth_pp,\n                )\n                for element in o\n            ]\n            if isinstance(o, tuple):\n                rval = tuple(rval)\n        elif isinstance(o, (bool, int, float, str)) or o is None:\n            rval = o\n        elif callable(o):  # Check if o is callable (like model_n)\n            # Deserialize the function (lambda) and return the result\n            # Note: You need to handle this part according to your application context.\n            rval = lambda x: self._deserialize_pytorch(\n                o(x),\n                components=components,\n                initialize_with_defaults=initialize_with_defaults,\n                recursion_depth=depth_pp,\n            )\n        elif isinstance(o, OpenMLFlow):\n            if not self._is_pytorch_flow(o):\n                raise ValueError(\"Only pytorch flows can be reinstantiated\")\n            rval = self._deserialize_model(\n                flow=o,\n                keep_defaults=initialize_with_defaults,\n                recursion_depth=recursion_depth,\n            )\n        else:\n            raise TypeError(o)\n        logging.info(\n            \"-%s flow_to_pytorch END   o=%s, rval=%s\" % (\"-\" * recursion_depth, o, rval)\n        )\n        return rval\n\n    def model_to_flow(\n        self, model: Any, custom_name: Optional[str] = None\n    ) -&gt; \"OpenMLFlow\":\n        \"\"\"Transform a Pytorch model to a flow for uploading it to OpenML.\n\n        Parameters\n        ----------\n        model : Any\n\n        Returns\n        -------\n        OpenMLFlow\n        \"\"\"\n        # Necessary to make pypy not complain about all the different possible return types\n        return self._serialize_pytorch(model, custom_name)\n\n    def _serialize_pytorch(\n        self,\n        o: Any,\n        parent_model: Optional[Any] = None,\n        custom_name: Optional[str] = None,\n    ) -&gt; Any:\n        rval = None  # type: Any\n\n        if self.is_estimator(o):\n            # Serialize the main model or a submodel\n            rval = self._serialize_model(o, custom_name)\n        elif isinstance(o, (list, tuple)):\n            rval = [self._serialize_pytorch(element, parent_model) for element in o]\n            if isinstance(o, tuple):\n                rval = tuple(rval)\n        elif isinstance(o, SIMPLE_TYPES) or o is None:\n            if isinstance(o, tuple(SIMPLE_NUMPY_TYPES)):\n                # Convert numpy types to python types\n                o = o.item()\n            # Base parameter values\n            rval = o\n        elif isinstance(o, dict):\n            if not isinstance(o, OrderedDict):\n                o = OrderedDict([(key, value) for key, value in sorted(o.items())])\n\n            rval = OrderedDict()\n            for key, value in o.items():\n                if not isinstance(key, str):\n                    raise TypeError(\n                        f\"Can only use string as keys, you passed type {type(key)} for value {str(key)}.\"\n                    )\n                key = self._serialize_pytorch(key, parent_model)\n                value = self._serialize_pytorch(value, parent_model)\n                rval[key] = value\n        elif isinstance(o, type):\n            rval = self._serialize_type(o)\n        elif inspect.isfunction(o):\n            rval = self._serialize_function(o)\n        elif inspect.ismethoddescriptor(o):\n            rval = self._serialize_methoddescriptor(o)\n        else:\n            # rval = str(o)\n            # Store saying its unsupported\n            rval = OrderedDict(\n                (\n                    (\"oml-python:serialized_object\", \"unsupported\"),\n                )\n            )\n            print(\n                \"While layers of this type are not officially supported yet, we will try to serialize them anyway.\"\n            )\n        return rval\n\n    def get_version_information(self) -&gt; List[str]:\n        \"\"\"List versions of libraries required by the flow.\n\n        Libraries listed are ``Python``, ``pytorch``, ``numpy`` and ``scipy``.\n\n        Returns\n        -------\n        List\n        \"\"\"\n\n        # This can possibly be done by a package such as pyxb, but I could not get\n        # it to work properly.\n        import numpy\n        import scipy\n\n        major, minor, micro, _, _ = sys.version_info\n        python_version = \"Python_{}.\".format(\n            \".\".join([str(major), str(minor), str(micro)])\n        )\n        pytorch_version = \"Torch_{}.\".format(torch.__version__)\n        numpy_version = \"NumPy_{}.\".format(numpy.__version__)\n        scipy_version = \"SciPy_{}.\".format(scipy.__version__)\n        pytorch_version_formatted = pytorch_version.replace(\"+\", \"_\")\n        return [python_version, pytorch_version_formatted, numpy_version, scipy_version]\n\n    def create_setup_string(self, model: Any) -&gt; str:\n        \"\"\"Create a string which can be used to reinstantiate the given model.\n\n        Parameters\n        ----------\n        model : Any\n\n        Returns\n        -------\n        str\n        \"\"\"\n        run_environment = \" \".join(self.get_version_information())\n        return run_environment + \" \" + str(model)\n\n    @classmethod\n    def _is_pytorch_flow(cls, flow: OpenMLFlow) -&gt; bool:\n        return (\n            flow.external_version.startswith(\"torch==\")\n            or \",torch==\" in flow.external_version\n        )\n\n    def _serialize_model(\n        self, model: Any, custom_name: Optional[str] = None\n    ) -&gt; OpenMLFlow:\n        \"\"\"Create an OpenMLFlow.\n\n        Calls `pytorch_to_flow` recursively to properly serialize the\n        parameters to strings and the components (other models) to OpenMLFlows.\n\n        Parameters\n        ----------\n        model : pytorch estimator or callable\n            If the model is a callable (e.g., a lambda or function wrapping the model),\n            we first apply it to obtain the actual model.\n\n        Returns\n        -------\n        OpenMLFlow\n        \"\"\"\n\n        # Now proceed with the serialization as usual\n        parameters, parameters_meta_info, subcomponents, subcomponents_explicit = (\n            self._extract_information_from_model(model)\n        )\n\n        # Check that a component does not occur multiple times in a flow as this\n        # is not supported by OpenML\n        self._check_multiple_occurence_of_component_in_flow(model, subcomponents)\n\n        import os\n        import zlib\n\n        # Generate a unique class name for the model\n        class_name = \"torch.nn\" + \".\" + model.__class__.__name__\n        class_name += \".\"\n        class_name += format(zlib.crc32(bytearray(os.urandom(32))), \"x\")\n        class_name += format(zlib.crc32(bytearray(os.urandom(32))), \"x\")\n\n        name = class_name\n\n        # Get the external versions of all sub-components\n        external_version = self._get_external_version_string(model, subcomponents)\n\n        dependencies = \"\\n\".join(\n            [\n                self._format_external_version(\n                    \"torch\",\n                    torch.__version__,\n                ),\n                \"numpy&gt;=1.6.1\",\n                \"scipy&gt;=0.9\",\n            ]\n        )\n\n        torch_version = self._format_external_version(\"torch\", torch.__version__)\n        torch_version_formatted = torch_version.replace(\"==\", \"_\")\n        torch_version_formatted = torch_version_formatted.replace(\"+\", \"_\")\n\n        flow = OpenMLFlow(\n            name=name,\n            class_name=class_name,\n            description=\"Automatically created pytorch flow.\",\n            model=model,\n            components=subcomponents,\n            parameters=parameters,\n            parameters_meta_info=parameters_meta_info,\n            external_version=external_version,\n            tags=[\"openml-python\", \"pytorch\", \"python\", torch_version_formatted],\n            language=\"English\",\n            dependencies=dependencies,\n            custom_name=custom_name,\n        )\n\n        return flow\n\n    def _get_external_version_string(\n        self,\n        model: Any,\n        sub_components: Dict[str, OpenMLFlow],\n    ) -&gt; str:\n        # Create external version string for a flow, given the model and the\n        # already parsed dictionary of sub_components. Retrieves the external\n        # version of all subcomponents, which themselves already contain all\n        # requirements for their subcomponents. The external version string is a\n        # sorted concatenation of all modules which are present in this run.\n        model_package_name = model.__module__.split(\".\")[0]\n        module = importlib.import_module(model_package_name)\n        model_package_version_number = \"module.__version__\"  # type: ignore\n        external_version = self._format_external_version(\n            model_package_name,\n            model_package_version_number,\n        )\n        openml_version = self._format_external_version(\"openml\", openml.__version__)\n        torch_version = self._format_external_version(\"torch\", torch.__version__)\n        external_versions = set()\n        external_versions.add(external_version)\n        external_versions.add(openml_version)\n        external_versions.add(torch_version)\n        for visitee in sub_components.values():\n            for external_version in visitee.external_version.split(\",\"):\n                external_versions.add(external_version)\n        return \",\".join(list(sorted(external_versions)))\n\n    def _check_multiple_occurence_of_component_in_flow(\n        self,\n        model: Any,\n        sub_components: Dict[str, OpenMLFlow],\n    ) -&gt; None:\n        to_visit_stack = []  # type: List[OpenMLFlow]\n        to_visit_stack.extend(sub_components.values())\n        known_sub_components = set()  # type: Set[str]\n        while len(to_visit_stack) &gt; 0:\n            visitee = to_visit_stack.pop()\n            if visitee.name in known_sub_components:\n                raise ValueError(\n                    \"Found a second occurence of component %s when \"\n                    \"trying to serialize %s.\" % (visitee.name, model)\n                )\n            else:\n                known_sub_components.add(visitee.name)\n                to_visit_stack.extend(visitee.components.values())\n\n    def _is_container_module(self, module: torch.nn.Module) -&gt; bool:\n        if isinstance(\n            module, (torch.nn.Sequential, torch.nn.ModuleDict, torch.nn.ModuleList)\n        ):\n            return True\n        if module in (\n            torch.nn.modules.container.Sequential,\n            torch.nn.modules.container.ModuleDict,\n            torch.nn.modules.container.ModuleList,\n        ):\n            return True\n        return False\n\n    def _get_module_hyperparameters(\n        self, module: torch.nn.Module, parameters: Dict[str, torch.nn.Parameter]\n    ) -&gt; Dict[str, Any]:\n        # Extract the signature of the module constructor\n        main_signature = inspect.signature(module.__init__)\n        params = dict()  # type: Dict[str, Any]\n\n        check_bases = False  # type: bool\n        for param_name, param in main_signature.parameters.items():\n            # Skip hyper-parameters which are actually parameters.\n            if param_name in parameters.keys():\n                continue\n\n            # Skip *args and **kwargs, and check the base classes instead.\n            if param.kind in (\n                inspect.Parameter.VAR_POSITIONAL,\n                inspect.Parameter.VAR_KEYWORD,\n            ):\n                check_bases = True\n                continue\n\n            # Extract the hyperparameter from the module.\n            if hasattr(module, param_name):\n                params[param_name] = getattr(module, param_name)\n\n        if check_bases:\n            for base in module.__class__.__bases__:\n                # Extract the signature  of the base constructor\n                base_signature = inspect.signature(base.__init__)\n\n                for param_name, param in base_signature.parameters.items():\n                    # Skip hyper-parameters which are actually parameters.\n                    if param_name in parameters.keys():\n                        continue\n\n                    # Skip *args and **kwargs since they are not relevant.\n                    if param.kind in (\n                        inspect.Parameter.VAR_POSITIONAL,\n                        inspect.Parameter.VAR_KEYWORD,\n                    ):\n                        continue\n\n                    # Extract the hyperparameter from the module.\n                    if hasattr(module, param_name):\n                        params[param_name] = getattr(module, param_name)\n\n        from .layers import Functional\n\n        if isinstance(module, Functional):\n            params[\"args\"] = getattr(module, \"args\")\n            params[\"kwargs\"] = getattr(module, \"kwargs\")\n\n        return params\n\n    def _get_module_descriptors(\n        self, model: torch.nn.Module, deep=True\n    ) -&gt; Dict[str, Any]:\n        # The named children (modules) of the given module.\n        named_children = list((k, v) for (k, v) in model.named_children())\n        # The parameters of the given module and its submodules.\n        model_parameters = dict((k, v) for (k, v) in model.named_parameters())\n\n        parameters = dict()  # type: Dict[str, Any]\n\n        if not self._is_container_module(model):\n            # For non-containers, we simply extract the hyperparameters.\n            parameters = self._get_module_hyperparameters(model, model_parameters)\n        else:\n            # Otherwise we serialize their children as lists of pairs in order\n            # to maintain the order of the sub modules.\n            parameters[\"children\"] = named_children\n\n        # If a deep description is required, append the children to the dictionary of\n        # returned parameters.\n        if deep:\n            named_children_dict = dict(named_children)\n            parameters = {**parameters, **named_children_dict}\n\n        return parameters\n\n    def _extract_information_from_model(\n        self,\n        model: Any,\n    ) -&gt; Tuple[\n        \"OrderedDict[str, Optional[str]]\",\n        \"OrderedDict[str, Optional[Dict]]\",\n        \"OrderedDict[str, OpenMLFlow]\",\n        Set,\n    ]:\n        # This function contains four \"global\" states and is quite long and\n        # complicated. If it gets to complicated to ensure it's correctness,\n        # it would be best to make it a class with the four \"global\" states being\n        # the class attributes and the if/elif/else in the for-loop calls to\n        # separate class methods\n\n        # stores all entities that should become subcomponents\n        sub_components = OrderedDict()  # type: OrderedDict[str, OpenMLFlow]\n        # stores the keys of all subcomponents that should become\n        sub_components_explicit = set()\n        parameters = OrderedDict()  # type: OrderedDict[str, Optional[str]]\n        parameters_meta_info = OrderedDict()  # type: OrderedDict[str, Optional[Dict]]\n\n        model_parameters = self._get_module_descriptors(model, deep=True)\n        for k, v in sorted(model_parameters.items(), key=lambda t: t[0]):\n            try:\n                rval = self._serialize_pytorch(v, model)\n            except TypeError:\n                rval = str(v)\n\n            def flatten_all(list_):\n                \"\"\"Flattens arbitrary depth lists of lists (e.g. [[1,2],[3,[1]]] -&gt; [1,2,3,1]).\"\"\"\n                for el in list_:\n                    if isinstance(el, (list, tuple)):\n                        yield from flatten_all(el)\n                    else:\n                        yield el\n\n            is_non_empty_list_of_lists_with_same_type = (\n                isinstance(rval, (list, tuple))\n                and len(rval) &gt; 0\n                and isinstance(rval[0], (list, tuple))\n                and all([isinstance(rval_i, type(rval[0])) for rval_i in rval])\n            )\n\n            # Check that all list elements are of simple types.\n            nested_list_of_simple_types = (\n                is_non_empty_list_of_lists_with_same_type\n                and all([isinstance(el, SIMPLE_TYPES) for el in flatten_all(rval)])\n            )\n\n            if (\n                is_non_empty_list_of_lists_with_same_type\n                and not nested_list_of_simple_types\n            ):\n                # If a list of lists is identified that include 'non-simple' types (e.g. objects),\n                # we assume they are steps in a pipeline, feature union, or base classifiers in\n                # a voting classifier.\n                parameter_value = list()  # type: List\n                reserved_keywords = set(\n                    self._get_module_descriptors(model, deep=False).keys()\n                )\n\n                for sub_component_tuple in rval:\n                    identifier = sub_component_tuple[0]\n                    sub_component = sub_component_tuple[1]\n                    sub_component_type = type(sub_component_tuple)\n                    if not 2 &lt;= len(sub_component_tuple) &lt;= 3:\n                        msg = \"Length of tuple does not match assumptions\"\n                        raise ValueError(msg)\n                    if not isinstance(sub_component, (OpenMLFlow, type(None))):\n                        msg = (\n                            \"Second item of tuple does not match assumptions. \"\n                            \"Expected OpenMLFlow, got %s\" % type(sub_component)\n                        )\n                        raise TypeError(msg)\n\n                    if identifier in reserved_keywords:\n                        parent_model = \"{}.{}\".format(\n                            model.__module__, model.__class__.__name__\n                        )\n                        msg = (\n                            \"Found element shadowing official \"\n                            \"parameter for %s: %s\" % (parent_model, identifier)\n                        )\n                        raise PyOpenMLError(msg)\n\n                    if sub_component is None:\n                        # In a FeatureUnion it is legal to have a None step\n\n                        pv = [identifier, None]\n                        if sub_component_type is tuple:\n                            parameter_value.append(tuple(pv))\n                        else:\n                            parameter_value.append(pv)\n\n                    else:\n                        # Add the component to the list of components, add a\n                        # component reference as a placeholder to the list of\n                        # parameters, which will be replaced by the real component\n                        # when deserializing the parameter\n                        sub_components_explicit.add(identifier)\n                        sub_components[identifier] = sub_component\n                        component_reference = (\n                            OrderedDict()\n                        )  # type: Dict[str, Union[str, Dict]]\n                        component_reference[\"oml-python:serialized_object\"] = (\n                            \"component_reference\"\n                        )\n                        cr_value = OrderedDict()  # type: Dict[str, Any]\n                        cr_value[\"key\"] = identifier\n                        cr_value[\"step_name\"] = identifier\n                        if len(sub_component_tuple) == 3:\n                            cr_value[\"argument_1\"] = sub_component_tuple[2]\n                        component_reference[\"value\"] = cr_value\n                        parameter_value.append(component_reference)\n\n                # Here (and in the elif and else branch below) are the only\n                # places where we encode a value as json to make sure that all\n                # parameter values still have the same type after\n                # deserialization\n\n                if isinstance(rval, tuple):\n                    parameter_json = json.dumps(tuple(parameter_value))\n                else:\n                    parameter_json = json.dumps(parameter_value)\n                parameters[k] = parameter_json\n\n            elif isinstance(rval, OpenMLFlow):\n\n                # A subcomponent, for example the layers in a sequential model\n                sub_components[k] = rval\n                sub_components_explicit.add(k)\n                component_reference = OrderedDict()\n                component_reference[\"oml-python:serialized_object\"] = (\n                    \"component_reference\"\n                )\n                cr_value = OrderedDict()\n                cr_value[\"key\"] = k\n                cr_value[\"step_name\"] = None\n                component_reference[\"value\"] = cr_value\n                cr = self._serialize_pytorch(component_reference, model)\n                parameters[k] = json.dumps(cr)\n\n            else:\n                # a regular hyperparameter\n                rval = json.dumps(rval)\n                parameters[k] = rval\n\n            parameters_meta_info[k] = OrderedDict(\n                ((\"description\", None), (\"data_type\", None))\n            )\n\n        return parameters, parameters_meta_info, sub_components, sub_components_explicit\n\n    def _get_fn_arguments_with_defaults(self, fn_name: Callable) -&gt; Tuple[Dict, Set]:\n        \"\"\"\n        Returns:\n            i) a dict with all parameter names that have a default value, and\n            ii) a set with all parameter names that do not have a default\n\n        Parameters\n        ----------\n        fn_name : callable\n            The function of which we want to obtain the defaults\n\n        Returns\n        -------\n        params_with_defaults: dict\n            a dict mapping parameter name to the default value\n        params_without_defaults: set\n            a set with all parameters that do not have a default value\n        \"\"\"\n        # parameters with defaults are optional, all others are required.\n        signature = inspect.getfullargspec(fn_name)\n        if signature.defaults:\n            optional_params = dict(\n                zip(reversed(signature.args), reversed(signature.defaults))\n            )\n        else:\n            optional_params = dict()\n        required_params = {arg for arg in signature.args if arg not in optional_params}\n        return optional_params, required_params\n\n    def _deserialize_model(\n        self,\n        flow: OpenMLFlow,\n        keep_defaults: bool,\n        recursion_depth: int,\n    ) -&gt; Any:\n        logging.info(\"-%s deserialize %s\" % (\"-\" * recursion_depth, flow.name))\n        model_name = flow.class_name\n        self._check_dependencies(flow.dependencies)\n\n        parameters = flow.parameters\n        components = flow.components\n        parameter_dict = OrderedDict()  # type: Dict[str, Any]\n\n        # Do a shallow copy of the components dictionary so we can remove the\n        # components from this copy once we added them into the pipeline. This\n        # allows us to not consider them any more when looping over the\n        # components, but keeping the dictionary of components untouched in the\n        # original components dictionary.\n        components_ = copy.copy(components)\n\n        for name in parameters:\n            value = parameters.get(name)\n            logging.info(\n                \"--%s flow_parameter=%s, value=%s\"\n                % (\"-\" * recursion_depth, name, value)\n            )\n\n            rval = self._deserialize_pytorch(\n                value,\n                components=components_,\n                initialize_with_defaults=keep_defaults,\n                recursion_depth=recursion_depth + 1,\n            )\n            parameter_dict[name] = rval\n\n        for name in components:\n            if name in parameter_dict:\n                continue\n            if name not in components_:\n                continue\n            value = components[name]\n            logging.info(\n                \"--%s flow_component=%s, value=%s\"\n                % (\"-\" * recursion_depth, name, value)\n            )\n            rval = self._deserialize_pytorch(\n                value,\n                recursion_depth=recursion_depth + 1,\n            )\n            parameter_dict[name] = rval\n\n        # Remove the unique identifier\n        model_name = model_name.rsplit(\".\", 1)[0]\n\n        module_name = model_name.rsplit(\".\", 1)\n        model_class = getattr(importlib.import_module(module_name[0]), module_name[1])\n\n        if keep_defaults:\n            # obtain all params with a default\n            param_defaults, _ = self._get_fn_arguments_with_defaults(\n                model_class.__init__\n            )\n\n            # delete the params that have a default from the dict,\n            # so they get initialized with their default value\n            # except [...]\n            for param in param_defaults:\n                # [...] the ones that also have a key in the components dict.\n                # As OpenML stores different flows for ensembles with different\n                # (base-)components, in OpenML terms, these are not considered\n                # hyperparameters but rather constants (i.e., changing them would\n                # result in a different flow)\n                if param not in components.keys() and param in parameter_dict:\n                    del parameter_dict[param]\n\n        if self._is_container_module(model_class):\n            children = parameter_dict[\"children\"]\n            children = list((str(k), v) for (k, v) in children)\n            children = OrderedDict(children)\n            return model_class(children)\n\n        from .layers import Functional\n\n        if model_class is Functional:\n            return model_class(\n                function=parameter_dict[\"function\"],\n                *parameter_dict[\"args\"],\n                **parameter_dict[\"kwargs\"],\n            )\n\n        return model_class(**parameter_dict)\n\n    def _check_dependencies(self, dependencies: str) -&gt; None:\n        if not dependencies:\n            return\n\n        dependencies_list = dependencies.split(\"\\n\")\n        for dependency_string in dependencies_list:\n            match = DEPENDENCIES_PATTERN.match(dependency_string)\n            if not match:\n                raise ValueError(\"Cannot parse dependency %s\" % dependency_string)\n\n            dependency_name = match.group(\"name\")\n            operation = match.group(\"operation\")\n            version = match.group(\"version\")\n\n            module = importlib.import_module(dependency_name)\n            required_version = LooseVersion(version)\n            installed_version = LooseVersion(module.__version__)  # type: ignore\n\n            if operation == \"==\":\n                check = required_version == installed_version\n            elif operation == \"&gt;\":\n                check = installed_version &gt; required_version\n            elif operation == \"&gt;=\":\n                check = (\n                    installed_version &gt; required_version\n                    or installed_version == required_version\n                )\n            else:\n                raise NotImplementedError(\"operation '%s' is not supported\" % operation)\n            if not check:\n                raise ValueError(\n                    \"Trying to deserialize a model with dependency \"\n                    \"%s not satisfied.\" % dependency_string\n                )\n\n    def _serialize_type(self, o: Any) -&gt; \"OrderedDict[str, str]\":\n        mapping = {\n            float: \"float\",\n            np.float32: \"np.float32\",\n            np.float64: \"np.float64\",\n            int: \"int\",\n            np.int32: \"np.int32\",\n            np.int64: \"np.int64\",\n        }\n        ret = OrderedDict()  # type: 'OrderedDict[str, str]'\n        ret[\"oml-python:serialized_object\"] = \"type\"\n        ret[\"value\"] = mapping[o]\n        return ret\n\n    def _deserialize_type(self, o: str) -&gt; Any:\n        mapping = {\n            \"float\": float,\n            \"np.float32\": np.float32,\n            \"np.float64\": np.float64,\n            \"int\": int,\n            \"np.int32\": np.int32,\n            \"np.int64\": np.int64,\n        }\n        return mapping[o]\n\n    def _serialize_function(self, o: Callable) -&gt; \"OrderedDict[str, str]\":\n        name = o.__module__ + \".\" + o.__name__\n        ret = OrderedDict()  # type: 'OrderedDict[str, str]'\n        ret[\"oml-python:serialized_object\"] = \"function\"\n        ret[\"value\"] = name\n        return ret\n\n    def _deserialize_function(self, name: str) -&gt; Callable:\n        module_name = name.rsplit(\".\", 1)\n        function_handle = getattr(\n            importlib.import_module(module_name[0]), module_name[1]\n        )\n        return function_handle\n\n    def _serialize_methoddescriptor(self, o: Any) -&gt; \"OrderedDict[str, str]\":\n        name = (\n            o.__objclass__.__module__ + \".\" + o.__objclass__.__name__ + \".\" + o.__name__\n        )\n        ret = OrderedDict()  # type: 'OrderedDict[str, str]'\n        ret[\"oml-python:serialized_object\"] = \"methoddescriptor\"\n        ret[\"value\"] = name\n        return ret\n\n    def _deserialize_methoddescriptor(self, name: str) -&gt; Any:\n        module_name = name.rsplit(\".\", 2)\n        object_handle = getattr(importlib.import_module(module_name[0]), module_name[1])\n        function_handle = getattr(object_handle, module_name[2])\n        return function_handle\n\n    def _format_external_version(\n        self,\n        model_package_name: str,\n        model_package_version_number: str,\n    ) -&gt; str:\n        return \"%s==%s\" % (model_package_name, model_package_version_number)\n\n    @staticmethod\n    def _get_parameter_values_recursive(\n        param_grid: Union[Dict, List[Dict]], parameter_name: str\n    ) -&gt; List[Any]:\n        \"\"\"\n        Returns a list of values for a given hyperparameter, encountered\n        recursively throughout the flow. (e.g., n_jobs can be defined\n        for various flows)\n\n        Parameters\n        ----------\n        param_grid: Union[Dict, List[Dict]]\n            Dict mapping from hyperparameter list to value, to a list of\n            such dicts\n\n        parameter_name: str\n            The hyperparameter that needs to be inspected\n\n        Returns\n        -------\n        List\n            A list of all values of hyperparameters with this name\n        \"\"\"\n        if isinstance(param_grid, dict):\n            result = list()\n            for param, value in param_grid.items():\n                if param.split(\"__\")[-1] == parameter_name:\n                    result.append(value)\n            return result\n        elif isinstance(param_grid, list):\n            result = list()\n            for sub_grid in param_grid:\n                result.extend(\n                    PytorchExtension._get_parameter_values_recursive(\n                        sub_grid, parameter_name\n                    )\n                )\n            return result\n        else:\n            raise ValueError(\"Param_grid should either be a dict or list of dicts\")\n\n    ################################################################################################\n    # Methods for performing runs with extension modules\n\n    def is_estimator(self, model: Any) -&gt; bool:\n        \"\"\"Check whether the given model is a pytorch estimator.\n\n        This function is only required for backwards compatibility and will be removed in the\n        near future.\n\n        Parameters\n        ----------\n        model : Any\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return isinstance(model, torch.nn.Module)\n\n    def seed_model(self, model: Any, seed: Optional[int] = None) -&gt; Any:\n        \"\"\"Set the random state of all the unseeded components of a model and return the seeded\n        model.\n\n        Required so that all seed information can be uploaded to OpenML for reproducible results.\n\n        Models that are already seeded will maintain the seed. In this case,\n        only integer seeds are allowed (An exception is raised when a RandomState was used as\n        seed).\n\n        Parameters\n        ----------\n        model : pytorch model\n            The model to be seeded\n        seed : int\n            The seed to initialize the RandomState with. Unseeded subcomponents\n            will be seeded with a random number from the RandomState.\n\n        Returns\n        -------\n        Any\n        \"\"\"\n\n        return model\n\n    def _run_model_on_fold(\n        self,\n        model: Any,\n        task: \"OpenMLTask\",\n        X_train: Union[np.ndarray, scipy.sparse.spmatrix, pd.DataFrame],\n        rep_no: int,\n        fold_no: int,\n        y_train: Optional[np.ndarray] = None,\n        X_test: Optional[Union[np.ndarray, scipy.sparse.spmatrix, pd.DataFrame]] = None,\n    ) -&gt; Tuple[\n        np.ndarray,\n        np.ndarray,\n        \"OrderedDict[str, float]\",\n        Optional[OpenMLRunTrace],\n        Optional[Any],\n    ]:\n        \"\"\"Run a model on a repeat,fold,subsample triplet of the task and return prediction\n        information.\n\n        Furthermore, it will measure run time measures in case multi-core behaviour allows this.\n        * exact user cpu time will be measured if the number of cores is set (recursive throughout\n        the model) exactly to 1\n        * wall clock time will be measured if the number of cores is set (recursive throughout the\n        model) to any given number (but not when it is set to -1)\n\n        Returns the data that is necessary to construct the OpenML Run object. Is used by\n        run_task_get_arff_content. Do not use this function unless you know what you are doing.\n\n        Parameters\n        ----------\n        model : Any\n            The UNTRAINED model to run. The model instance will be copied and not altered.\n        task : OpenMLTask\n            The task to run the model on.\n        X_train : array-like\n            Training data for the given repetition and fold.\n        rep_no : int\n            The repeat of the experiment (0-based; in case of 1 time CV, always 0)\n        fold_no : int\n            The fold nr of the experiment (0-based; in case of holdout, always 0)\n        y_train : Optional[np.ndarray] (default=None)\n            Target attributes for supervised tasks. In case of classification, these are integer\n            indices to the potential classes specified by dataset.\n        X_test : Optional, array-like (default=None)\n            Test attributes to test for generalization in supervised tasks.\n\n        Returns\n        -------\n        predictions : np.ndarray\n            Model predictions.\n        probabilities :  Optional, np.ndarray\n            Predicted probabilities (only applicable for supervised classification tasks).\n        user_defined_measures : OrderedDict[str, float]\n            User defined measures that were generated on this fold\n        trace : Optional, OpenMLRunTrace\n            Hyperparameter optimization trace (only applicable for supervised tasks with\n            hyperparameter optimization).\n        additional_information: Optional, Any\n            Additional information provided by the extension to be converted into additional files.\n        \"\"\"\n\n        try:\n            trainer: OpenMLTrainerModule = config.trainer\n            trainer.logger = config.logger\n        except AttributeError:\n            raise ValueError(\n                \"Trainer not set to config. Please use openml_pytorch.config.trainer = trainer to set the trainer.\"\n            )\n        print(f\"Cross validation {fold_no} for task {task.task_id}\")\n        return trainer.run_model_on_fold(\n            model, task, X_train, rep_no, fold_no, y_train, X_test\n        )\n\n    def compile_additional_information(\n        self, task: \"OpenMLTask\", additional_information: List[Tuple[int, int, Any]]\n    ) -&gt; Dict[str, Tuple[str, str]]:\n        \"\"\"Compiles additional information provided by the extension during the runs into a final\n        set of files.\n\n        Parameters\n        ----------\n        task : OpenMLTask\n            The task the model was run on.\n        additional_information: List[Tuple[int, int, Any]]\n            A list of (fold, repetition, additional information) tuples obtained during training.\n\n        Returns\n        -------\n        files : Dict[str, Tuple[str, str]]\n            A dictionary of files with their file name and contents.\n        \"\"\"\n        return dict()\n\n    def obtain_parameter_values(\n        self,\n        flow: \"OpenMLFlow\",\n        model: Any = None,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Extracts all parameter settings required for the flow from the model.\n\n        If no explicit model is provided, the parameters will be extracted from `flow.model`\n        instead.\n\n        Parameters\n        ----------\n        flow : OpenMLFlow\n            OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)\n\n        model: Any, optional (default=None)\n            The model from which to obtain the parameter values. Must match the flow signature.\n            If None, use the model specified in ``OpenMLFlow.model``.\n\n        Returns\n        -------\n        list\n            A list of dicts, where each dict has the following entries:\n            - ``oml:name`` : str: The OpenML parameter name\n            - ``oml:value`` : mixed: A representation of the parameter value\n            - ``oml:component`` : int: flow id to which the parameter belongs\n        \"\"\"\n        openml.flows.functions._check_flow_for_server_id(flow)\n\n        def get_flow_dict(_flow):\n            flow_map = {_flow.name: _flow.flow_id}\n            for subflow in _flow.components:\n                flow_map.update(get_flow_dict(_flow.components[subflow]))\n            return flow_map\n\n        def extract_parameters(\n            _flow, _flow_dict, component_model, _main_call=False, main_id=None\n        ):\n            def is_subcomponent_specification(values):\n                # checks whether the current value can be a specification of\n                # subcomponents, as for example the value for steps parameter\n                # (in Pipeline) or transformers parameter (in\n                # ColumnTransformer). These are always lists/tuples of lists/\n                # tuples, size bigger than 2 and an OpenMLFlow item involved.\n                if not isinstance(values, (tuple, list)):\n                    return False\n                for item in values:\n                    if not isinstance(item, (tuple, list)):\n                        return False\n                    if len(item) &lt; 2:\n                        return False\n                    if not isinstance(item[1], openml.flows.OpenMLFlow):\n                        return False\n                return True\n\n            # _flow is openml flow object, _param dict maps from flow name to flow\n            # id for the main call, the param dict can be overridden (useful for\n            # unit tests / sentinels) this way, for flows without subflows we do\n            # not have to rely on _flow_dict\n            exp_parameters = set(_flow.parameters)\n            exp_components = set(_flow.components)\n            model_parameters = set(\n                [\n                    mp\n                    for mp in self._get_module_descriptors(component_model)\n                    if \"__\" not in mp\n                ]\n            )\n            if len((exp_parameters | exp_components) ^ model_parameters) != 0:\n                flow_params = sorted(exp_parameters | exp_components)\n                model_params = sorted(model_parameters)\n                raise ValueError(\n                    \"Parameters of the model do not match the \"\n                    \"parameters expected by the \"\n                    \"flow:\\nexpected flow parameters: \"\n                    \"%s\\nmodel parameters: %s\" % (flow_params, model_params)\n                )\n\n            _params = []\n            for _param_name in _flow.parameters:\n                _current = OrderedDict()\n                _current[\"oml:name\"] = _param_name\n\n                current_param_values = self.model_to_flow(\n                    self._get_module_descriptors(component_model)[_param_name]\n                )\n\n                # Try to filter out components (a.k.a. subflows) which are\n                # handled further down in the code (by recursively calling\n                # this function)!\n                if isinstance(current_param_values, openml.flows.OpenMLFlow):\n                    continue\n\n                if is_subcomponent_specification(current_param_values):\n                    # complex parameter value, with subcomponents\n                    parsed_values = list()\n                    for subcomponent in current_param_values:\n                        if len(subcomponent) &lt; 2 or len(subcomponent) &gt; 3:\n                            raise ValueError(\n                                \"Component reference should be \" \"size {2,3}. \"\n                            )\n\n                        subcomponent_identifier = subcomponent[0]\n                        subcomponent_flow = subcomponent[1]\n                        if not isinstance(subcomponent_identifier, str):\n                            raise TypeError(\n                                \"Subcomponent identifier should be \" \"string\"\n                            )\n                        if not isinstance(subcomponent_flow, openml.flows.OpenMLFlow):\n                            raise TypeError(\"Subcomponent flow should be string\")\n\n                        current = {\n                            \"oml-python:serialized_object\": \"component_reference\",\n                            \"value\": {\n                                \"key\": subcomponent_identifier,\n                                \"step_name\": subcomponent_identifier,\n                            },\n                        }\n                        if len(subcomponent) == 3:\n                            if not isinstance(subcomponent[2], list):\n                                raise TypeError(\n                                    \"Subcomponent argument should be\" \"list\"\n                                )\n                            current[\"value\"][\"argument_1\"] = subcomponent[2]\n                        parsed_values.append(current)\n                    parsed_values = json.dumps(parsed_values)\n                else:\n                    # vanilla parameter value\n                    parsed_values = json.dumps(current_param_values)\n\n                _current[\"oml:value\"] = parsed_values\n                if _main_call:\n                    _current[\"oml:component\"] = main_id\n                else:\n                    _current[\"oml:component\"] = _flow_dict[_flow.name]\n                _params.append(_current)\n\n            for _identifier in _flow.components:\n                subcomponent_model = self._get_module_descriptors(component_model)[\n                    _identifier\n                ]\n                _params.extend(\n                    extract_parameters(\n                        _flow.components[_identifier], _flow_dict, subcomponent_model\n                    )\n                )\n            return _params\n\n        flow_dict = get_flow_dict(flow)\n        model = model if model is not None else flow.model\n        parameters = extract_parameters(flow, flow_dict, model, True, flow.flow_id)\n\n        return parameters\n\n    def _openml_param_name_to_pytorch(\n        self,\n        openml_parameter: openml.setups.OpenMLParameter,\n        flow: OpenMLFlow,\n    ) -&gt; str:\n        \"\"\"\n        Converts the name of an OpenMLParameter into the pytorch name, given a flow.\n\n        Parameters\n        ----------\n        openml_parameter: OpenMLParameter\n            The parameter under consideration\n\n        flow: OpenMLFlow\n            The flow that provides context.\n\n        Returns\n        -------\n        pytorch_parameter_name: str\n            The name the parameter will have once used in pytorch\n        \"\"\"\n        if not isinstance(openml_parameter, openml.setups.OpenMLParameter):\n            raise ValueError(\n                \"openml_parameter should be an instance of OpenMLParameter\"\n            )\n        if not isinstance(flow, OpenMLFlow):\n            raise ValueError(\"flow should be an instance of OpenMLFlow\")\n\n        flow_structure = flow.get_structure(\"name\")\n        if openml_parameter.flow_name not in flow_structure:\n            raise ValueError(\n                \"Obtained OpenMLParameter and OpenMLFlow do not correspond. \"\n            )\n        name = openml_parameter.flow_name  # for PEP8\n        return \"__\".join(flow_structure[name] + [openml_parameter.parameter_name])\n\n    ################################################################################################\n    # Methods for hyperparameter optimization\n\n    def instantiate_model_from_hpo_class(\n        self,\n        model: Any,\n        trace_iteration: OpenMLTraceIteration,\n    ) -&gt; Any:\n        \"\"\"Instantiate a ``base_estimator`` which can be searched over by the hyperparameter\n        optimization model (UNUSED)\n\n        Parameters\n        ----------\n        model : Any\n            A hyperparameter optimization model which defines the model to be instantiated.\n        trace_iteration : OpenMLTraceIteration\n            Describing the hyperparameter settings to instantiate.\n\n        Returns\n        -------\n        Any\n        \"\"\"\n\n        return model\n\n    def check_if_model_fitted(self, model: Any) -&gt; bool:\n        \"\"\"Returns True/False denoting if the model has already been fitted/trained\n        Parameters\n        ----------\n        model : Any\n        Returns\n        -------\n        bool\n        \"\"\"\n</code></pre>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.can_handle_flow","title":"<code>can_handle_flow(flow)</code>  <code>classmethod</code>","text":"<p>Check whether a given describes a Pytorch estimator.</p> <p>This is done by parsing the <code>external_version</code> field.</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.can_handle_flow--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.can_handle_flow--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml_pytorch/extension.py</code> <pre><code>@classmethod\ndef can_handle_flow(cls, flow: \"OpenMLFlow\") -&gt; bool:\n    \"\"\"Check whether a given describes a Pytorch estimator.\n\n    This is done by parsing the ``external_version`` field.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    return cls._is_pytorch_flow(flow)\n</code></pre>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.can_handle_model","title":"<code>can_handle_model(model)</code>  <code>classmethod</code>","text":"<p>Check whether a model is an instance of <code>torch.nn.Module</code>.</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.can_handle_model--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.can_handle_model--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml_pytorch/extension.py</code> <pre><code>@classmethod\ndef can_handle_model(cls, model: Any) -&gt; bool:\n    \"\"\"Check whether a model is an instance of ``torch.nn.Module``.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    from torch.nn import Module\n\n    return isinstance(model, Module) or isinstance(model, Callable)\n</code></pre>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.check_if_model_fitted","title":"<code>check_if_model_fitted(model)</code>","text":"<p>Returns True/False denoting if the model has already been fitted/trained Parameters</p> <p>model : Any Returns</p> <p>bool</p> Source code in <code>openml_pytorch/extension.py</code> <pre><code>def check_if_model_fitted(self, model: Any) -&gt; bool:\n    \"\"\"Returns True/False denoting if the model has already been fitted/trained\n    Parameters\n    ----------\n    model : Any\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.compile_additional_information","title":"<code>compile_additional_information(task, additional_information)</code>","text":"<p>Compiles additional information provided by the extension during the runs into a final set of files.</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.compile_additional_information--parameters","title":"Parameters","text":"<p>task : OpenMLTask     The task the model was run on. additional_information: List[Tuple[int, int, Any]]     A list of (fold, repetition, additional information) tuples obtained during training.</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.compile_additional_information--returns","title":"Returns","text":"<p>files : Dict[str, Tuple[str, str]]     A dictionary of files with their file name and contents.</p> Source code in <code>openml_pytorch/extension.py</code> <pre><code>def compile_additional_information(\n    self, task: \"OpenMLTask\", additional_information: List[Tuple[int, int, Any]]\n) -&gt; Dict[str, Tuple[str, str]]:\n    \"\"\"Compiles additional information provided by the extension during the runs into a final\n    set of files.\n\n    Parameters\n    ----------\n    task : OpenMLTask\n        The task the model was run on.\n    additional_information: List[Tuple[int, int, Any]]\n        A list of (fold, repetition, additional information) tuples obtained during training.\n\n    Returns\n    -------\n    files : Dict[str, Tuple[str, str]]\n        A dictionary of files with their file name and contents.\n    \"\"\"\n    return dict()\n</code></pre>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.create_setup_string","title":"<code>create_setup_string(model)</code>","text":"<p>Create a string which can be used to reinstantiate the given model.</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.create_setup_string--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.create_setup_string--returns","title":"Returns","text":"<p>str</p> Source code in <code>openml_pytorch/extension.py</code> <pre><code>def create_setup_string(self, model: Any) -&gt; str:\n    \"\"\"Create a string which can be used to reinstantiate the given model.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    str\n    \"\"\"\n    run_environment = \" \".join(self.get_version_information())\n    return run_environment + \" \" + str(model)\n</code></pre>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.flow_to_model","title":"<code>flow_to_model(flow, initialize_with_defaults=False)</code>","text":"<p>Initializes a Pytorch model based on a flow.</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.flow_to_model--parameters","title":"Parameters","text":"<p>flow : mixed     the object to deserialize (can be flow object, or any serialized     parameter value that is accepted by)</p> bool, optional (default=False) <p>If this flag is set, the hyperparameter values of flows will be ignored and a flow with its defaults is returned.</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.flow_to_model--returns","title":"Returns","text":"<p>mixed</p> Source code in <code>openml_pytorch/extension.py</code> <pre><code>def flow_to_model(\n    self, flow: \"OpenMLFlow\", initialize_with_defaults: bool = False\n) -&gt; Any:\n    \"\"\"Initializes a Pytorch model based on a flow.\n\n    Parameters\n    ----------\n    flow : mixed\n        the object to deserialize (can be flow object, or any serialized\n        parameter value that is accepted by)\n\n    initialize_with_defaults : bool, optional (default=False)\n        If this flag is set, the hyperparameter values of flows will be\n        ignored and a flow with its defaults is returned.\n\n    Returns\n    -------\n    mixed\n    \"\"\"\n    return self._deserialize_pytorch(\n        flow, initialize_with_defaults=initialize_with_defaults\n    )\n</code></pre>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.get_version_information","title":"<code>get_version_information()</code>","text":"<p>List versions of libraries required by the flow.</p> <p>Libraries listed are <code>Python</code>, <code>pytorch</code>, <code>numpy</code> and <code>scipy</code>.</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.get_version_information--returns","title":"Returns","text":"<p>List</p> Source code in <code>openml_pytorch/extension.py</code> <pre><code>def get_version_information(self) -&gt; List[str]:\n    \"\"\"List versions of libraries required by the flow.\n\n    Libraries listed are ``Python``, ``pytorch``, ``numpy`` and ``scipy``.\n\n    Returns\n    -------\n    List\n    \"\"\"\n\n    # This can possibly be done by a package such as pyxb, but I could not get\n    # it to work properly.\n    import numpy\n    import scipy\n\n    major, minor, micro, _, _ = sys.version_info\n    python_version = \"Python_{}.\".format(\n        \".\".join([str(major), str(minor), str(micro)])\n    )\n    pytorch_version = \"Torch_{}.\".format(torch.__version__)\n    numpy_version = \"NumPy_{}.\".format(numpy.__version__)\n    scipy_version = \"SciPy_{}.\".format(scipy.__version__)\n    pytorch_version_formatted = pytorch_version.replace(\"+\", \"_\")\n    return [python_version, pytorch_version_formatted, numpy_version, scipy_version]\n</code></pre>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.instantiate_model_from_hpo_class","title":"<code>instantiate_model_from_hpo_class(model, trace_iteration)</code>","text":"<p>Instantiate a <code>base_estimator</code> which can be searched over by the hyperparameter optimization model (UNUSED)</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.instantiate_model_from_hpo_class--parameters","title":"Parameters","text":"<p>model : Any     A hyperparameter optimization model which defines the model to be instantiated. trace_iteration : OpenMLTraceIteration     Describing the hyperparameter settings to instantiate.</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.instantiate_model_from_hpo_class--returns","title":"Returns","text":"<p>Any</p> Source code in <code>openml_pytorch/extension.py</code> <pre><code>def instantiate_model_from_hpo_class(\n    self,\n    model: Any,\n    trace_iteration: OpenMLTraceIteration,\n) -&gt; Any:\n    \"\"\"Instantiate a ``base_estimator`` which can be searched over by the hyperparameter\n    optimization model (UNUSED)\n\n    Parameters\n    ----------\n    model : Any\n        A hyperparameter optimization model which defines the model to be instantiated.\n    trace_iteration : OpenMLTraceIteration\n        Describing the hyperparameter settings to instantiate.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n\n    return model\n</code></pre>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.is_estimator","title":"<code>is_estimator(model)</code>","text":"<p>Check whether the given model is a pytorch estimator.</p> <p>This function is only required for backwards compatibility and will be removed in the near future.</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.is_estimator--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.is_estimator--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml_pytorch/extension.py</code> <pre><code>def is_estimator(self, model: Any) -&gt; bool:\n    \"\"\"Check whether the given model is a pytorch estimator.\n\n    This function is only required for backwards compatibility and will be removed in the\n    near future.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    return isinstance(model, torch.nn.Module)\n</code></pre>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.model_to_flow","title":"<code>model_to_flow(model, custom_name=None)</code>","text":"<p>Transform a Pytorch model to a flow for uploading it to OpenML.</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.model_to_flow--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.model_to_flow--returns","title":"Returns","text":"<p>OpenMLFlow</p> Source code in <code>openml_pytorch/extension.py</code> <pre><code>def model_to_flow(\n    self, model: Any, custom_name: Optional[str] = None\n) -&gt; \"OpenMLFlow\":\n    \"\"\"Transform a Pytorch model to a flow for uploading it to OpenML.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    OpenMLFlow\n    \"\"\"\n    # Necessary to make pypy not complain about all the different possible return types\n    return self._serialize_pytorch(model, custom_name)\n</code></pre>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.obtain_parameter_values","title":"<code>obtain_parameter_values(flow, model=None)</code>","text":"<p>Extracts all parameter settings required for the flow from the model.</p> <p>If no explicit model is provided, the parameters will be extracted from <code>flow.model</code> instead.</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.obtain_parameter_values--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow     OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)</p> Any, optional (default=None) <p>The model from which to obtain the parameter values. Must match the flow signature. If None, use the model specified in <code>OpenMLFlow.model</code>.</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.obtain_parameter_values--returns","title":"Returns","text":"<p>list     A list of dicts, where each dict has the following entries:     - <code>oml:name</code> : str: The OpenML parameter name     - <code>oml:value</code> : mixed: A representation of the parameter value     - <code>oml:component</code> : int: flow id to which the parameter belongs</p> Source code in <code>openml_pytorch/extension.py</code> <pre><code>def obtain_parameter_values(\n    self,\n    flow: \"OpenMLFlow\",\n    model: Any = None,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Extracts all parameter settings required for the flow from the model.\n\n    If no explicit model is provided, the parameters will be extracted from `flow.model`\n    instead.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n        OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)\n\n    model: Any, optional (default=None)\n        The model from which to obtain the parameter values. Must match the flow signature.\n        If None, use the model specified in ``OpenMLFlow.model``.\n\n    Returns\n    -------\n    list\n        A list of dicts, where each dict has the following entries:\n        - ``oml:name`` : str: The OpenML parameter name\n        - ``oml:value`` : mixed: A representation of the parameter value\n        - ``oml:component`` : int: flow id to which the parameter belongs\n    \"\"\"\n    openml.flows.functions._check_flow_for_server_id(flow)\n\n    def get_flow_dict(_flow):\n        flow_map = {_flow.name: _flow.flow_id}\n        for subflow in _flow.components:\n            flow_map.update(get_flow_dict(_flow.components[subflow]))\n        return flow_map\n\n    def extract_parameters(\n        _flow, _flow_dict, component_model, _main_call=False, main_id=None\n    ):\n        def is_subcomponent_specification(values):\n            # checks whether the current value can be a specification of\n            # subcomponents, as for example the value for steps parameter\n            # (in Pipeline) or transformers parameter (in\n            # ColumnTransformer). These are always lists/tuples of lists/\n            # tuples, size bigger than 2 and an OpenMLFlow item involved.\n            if not isinstance(values, (tuple, list)):\n                return False\n            for item in values:\n                if not isinstance(item, (tuple, list)):\n                    return False\n                if len(item) &lt; 2:\n                    return False\n                if not isinstance(item[1], openml.flows.OpenMLFlow):\n                    return False\n            return True\n\n        # _flow is openml flow object, _param dict maps from flow name to flow\n        # id for the main call, the param dict can be overridden (useful for\n        # unit tests / sentinels) this way, for flows without subflows we do\n        # not have to rely on _flow_dict\n        exp_parameters = set(_flow.parameters)\n        exp_components = set(_flow.components)\n        model_parameters = set(\n            [\n                mp\n                for mp in self._get_module_descriptors(component_model)\n                if \"__\" not in mp\n            ]\n        )\n        if len((exp_parameters | exp_components) ^ model_parameters) != 0:\n            flow_params = sorted(exp_parameters | exp_components)\n            model_params = sorted(model_parameters)\n            raise ValueError(\n                \"Parameters of the model do not match the \"\n                \"parameters expected by the \"\n                \"flow:\\nexpected flow parameters: \"\n                \"%s\\nmodel parameters: %s\" % (flow_params, model_params)\n            )\n\n        _params = []\n        for _param_name in _flow.parameters:\n            _current = OrderedDict()\n            _current[\"oml:name\"] = _param_name\n\n            current_param_values = self.model_to_flow(\n                self._get_module_descriptors(component_model)[_param_name]\n            )\n\n            # Try to filter out components (a.k.a. subflows) which are\n            # handled further down in the code (by recursively calling\n            # this function)!\n            if isinstance(current_param_values, openml.flows.OpenMLFlow):\n                continue\n\n            if is_subcomponent_specification(current_param_values):\n                # complex parameter value, with subcomponents\n                parsed_values = list()\n                for subcomponent in current_param_values:\n                    if len(subcomponent) &lt; 2 or len(subcomponent) &gt; 3:\n                        raise ValueError(\n                            \"Component reference should be \" \"size {2,3}. \"\n                        )\n\n                    subcomponent_identifier = subcomponent[0]\n                    subcomponent_flow = subcomponent[1]\n                    if not isinstance(subcomponent_identifier, str):\n                        raise TypeError(\n                            \"Subcomponent identifier should be \" \"string\"\n                        )\n                    if not isinstance(subcomponent_flow, openml.flows.OpenMLFlow):\n                        raise TypeError(\"Subcomponent flow should be string\")\n\n                    current = {\n                        \"oml-python:serialized_object\": \"component_reference\",\n                        \"value\": {\n                            \"key\": subcomponent_identifier,\n                            \"step_name\": subcomponent_identifier,\n                        },\n                    }\n                    if len(subcomponent) == 3:\n                        if not isinstance(subcomponent[2], list):\n                            raise TypeError(\n                                \"Subcomponent argument should be\" \"list\"\n                            )\n                        current[\"value\"][\"argument_1\"] = subcomponent[2]\n                    parsed_values.append(current)\n                parsed_values = json.dumps(parsed_values)\n            else:\n                # vanilla parameter value\n                parsed_values = json.dumps(current_param_values)\n\n            _current[\"oml:value\"] = parsed_values\n            if _main_call:\n                _current[\"oml:component\"] = main_id\n            else:\n                _current[\"oml:component\"] = _flow_dict[_flow.name]\n            _params.append(_current)\n\n        for _identifier in _flow.components:\n            subcomponent_model = self._get_module_descriptors(component_model)[\n                _identifier\n            ]\n            _params.extend(\n                extract_parameters(\n                    _flow.components[_identifier], _flow_dict, subcomponent_model\n                )\n            )\n        return _params\n\n    flow_dict = get_flow_dict(flow)\n    model = model if model is not None else flow.model\n    parameters = extract_parameters(flow, flow_dict, model, True, flow.flow_id)\n\n    return parameters\n</code></pre>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.seed_model","title":"<code>seed_model(model, seed=None)</code>","text":"<p>Set the random state of all the unseeded components of a model and return the seeded model.</p> <p>Required so that all seed information can be uploaded to OpenML for reproducible results.</p> <p>Models that are already seeded will maintain the seed. In this case, only integer seeds are allowed (An exception is raised when a RandomState was used as seed).</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.seed_model--parameters","title":"Parameters","text":"<p>model : pytorch model     The model to be seeded seed : int     The seed to initialize the RandomState with. Unseeded subcomponents     will be seeded with a random number from the RandomState.</p>"},{"location":"API%20reference/OpenML%20Connection/#extension.PytorchExtension.seed_model--returns","title":"Returns","text":"<p>Any</p> Source code in <code>openml_pytorch/extension.py</code> <pre><code>def seed_model(self, model: Any, seed: Optional[int] = None) -&gt; Any:\n    \"\"\"Set the random state of all the unseeded components of a model and return the seeded\n    model.\n\n    Required so that all seed information can be uploaded to OpenML for reproducible results.\n\n    Models that are already seeded will maintain the seed. In this case,\n    only integer seeds are allowed (An exception is raised when a RandomState was used as\n    seed).\n\n    Parameters\n    ----------\n    model : pytorch model\n        The model to be seeded\n    seed : int\n        The seed to initialize the RandomState with. Unseeded subcomponents\n        will be seeded with a random number from the RandomState.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n\n    return model\n</code></pre>"},{"location":"API%20reference/Trainer/","title":"Trainer","text":"<p>This module provides classes and methods to facilitate the configuration, data handling, training, and evaluation of machine learning models using PyTorch and OpenML datasets. The functionalities include: - Generation of default configurations for models. - Handling of image and tabular data. - Training and evaluating machine learning models. - Exporting trained models to ONNX format. - Managing data transformations and loaders.</p> <p>This module provides classes and methods to facilitate the configuration, data handling, training, and evaluation of machine learning models using PyTorch and OpenML datasets. The functionalities include: - Generation of default configurations for models. - Handling of image and tabular data. - Training and evaluating machine learning models. - Exporting trained models to ONNX format. - Managing data transformations and loaders.</p>"},{"location":"API%20reference/Trainer/#trainer.BaseDataHandler","title":"<code>BaseDataHandler</code>","text":"<p>BaseDataHandler class is an abstract base class for data handling operations.</p> Source code in <code>openml_pytorch/trainer.py</code> <pre><code>class BaseDataHandler:\n    \"\"\"\n    BaseDataHandler class is an abstract base class for data handling operations.\n    \"\"\"\n\n    def prepare_data(\n        self, X_train, y_train, X_val, y_val, data_config: SimpleNamespace\n    ):\n        raise NotImplementedError\n\n    def prepare_test_data(self, X_test, data_config: SimpleNamespace):\n        raise NotImplementedError\n</code></pre>"},{"location":"API%20reference/Trainer/#trainer.BasicTrainer","title":"<code>BasicTrainer</code>","text":"<p>BasicTrainer class provides a simple training loop for PyTorch models.You pass in the model, loss function, optimizer, data loaders, and device. The fit method trains the model for the specified number of epochs.</p> Source code in <code>openml_pytorch/trainer.py</code> <pre><code>class BasicTrainer:\n    \"\"\"\n    BasicTrainer class provides a simple training loop for PyTorch models.You pass in the model, loss function, optimizer, data loaders, and device. The fit method trains the model for the specified number of epochs.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Any,\n        loss_fn: Any,\n        opt: Any,\n        dataloader_train: torch.utils.data.DataLoader,\n        dataloader_test: torch.utils.data.DataLoader,\n        device: torch.device,\n    ):\n        self.device = device\n        self.model = model.to(self.device)\n        self.loss_fn = loss_fn\n        self.opt = opt(self.model.parameters())\n        self.dataloader_train = dataloader_train\n        self.dataloader_test = dataloader_test\n        self.losses = {\"train\": [], \"test\": []}\n\n    def train_step(self, x, y):\n        self.model.train()\n        self.opt.zero_grad()\n        yhat = self.model(x)\n        loss = self.loss_fn(yhat, y)\n        loss.backward()\n        self.opt.step()\n        return loss.item()\n\n    def test_step(self, x, y):\n        self.model.eval()\n        with torch.no_grad():\n            yhat = self.model(x)\n            loss = self.loss_fn(yhat, y)\n        return loss.item()\n\n    def fit(self, epochs):\n        if self.dataloader_train is None:\n            raise ValueError(\"dataloader_train is not set\")\n        if self.dataloader_test is None:\n            raise ValueError(\"dataloader_test is not set\")\n        bar = tqdm(range(epochs), desc=\"Epochs\")\n        for epoch in bar:\n            # train\n            for x, y in self.dataloader_train:\n                x, y = x.to(self.device), y.to(self.device)\n                loss = self.train_step(x, y)\n                self.losses[\"train\"].append(loss)\n            # test\n            test_loss = 0\n            for x, y in self.dataloader_test:\n                x, y = x.to(self.device), y.to(self.device)\n                test_loss += self.test_step(x, y)\n                self.losses[\"test\"].append(test_loss)\n            bar.set_postfix(\n                {\"Train loss\": loss, \"Test loss\": test_loss, \"Epoch\": epoch + 1}\n            )\n</code></pre>"},{"location":"API%20reference/Trainer/#trainer.DataContainer","title":"<code>DataContainer</code>","text":"class DataContainer <p>A class to contain the training, validation, and test data loaders. This just makes it easier to access them when required.</p> <p>Attributes: train_dl: DataLoader object for the training data. valid_dl: DataLoader object for the validation data. test_dl: Optional DataLoader object for the test data.</p> Source code in <code>openml_pytorch/trainer.py</code> <pre><code>class DataContainer:\n    \"\"\"\n    class DataContainer:\n        A class to contain the training, validation, and test data loaders. This just makes it easier to access them when required.\n\n        Attributes:\n        train_dl: DataLoader object for the training data.\n        valid_dl: DataLoader object for the validation data.\n        test_dl: Optional DataLoader object for the test data.\n    \"\"\"\n\n    def __init__(\n        self,\n        train_dl: DataLoader,\n        valid_dl: torch.utils.data.DataLoader,\n        test_dl: torch.utils.data.DataLoader = None,\n    ):  # type: ignore\n        self.train_dl, self.valid_dl = train_dl, valid_dl\n        self.test_dl = test_dl\n\n    @property\n    def train_ds(self):\n        return self.train_dl.dataset\n\n    @property\n    def valid_ds(self):\n        return self.valid_dl.dataset\n\n    @property\n    def test_ds(self):\n        return self.test_dl.dataset\n</code></pre>"},{"location":"API%20reference/Trainer/#trainer.DefaultConfigGenerator","title":"<code>DefaultConfigGenerator</code>","text":"<p>DefaultConfigGenerator class provides various methods to generate default configurations.</p> Source code in <code>openml_pytorch/trainer.py</code> <pre><code>class DefaultConfigGenerator:\n    \"\"\"\n    DefaultConfigGenerator class provides various methods to generate default configurations.\n    \"\"\"\n\n    @staticmethod\n    def _default_loss_fn_gen(task: OpenMLTask) -&gt; torch.nn.Module:\n        \"\"\"\n        _default_loss_fn_gen returns a loss fn based on the task type - regressions use\n        torch.nn.SmoothL1Loss while classifications use torch.nn.CrossEntropyLoss\n        \"\"\"\n        if isinstance(task, OpenMLRegressionTask):\n            return torch.nn.SmoothL1Loss()\n        elif isinstance(task, OpenMLClassificationTask):\n            return torch.nn.CrossEntropyLoss()\n        else:\n            raise ValueError(task)\n\n    @staticmethod\n    def _default_predict(output: torch.Tensor, task: OpenMLTask) -&gt; torch.Tensor:\n        \"\"\"\n        Converts model outputs to predicted labels.\n        For classification: uses argmax.\n        For regression: flattens output.\n        \"\"\"\n        if isinstance(task, OpenMLClassificationTask):\n            return torch.argmax(output, dim=-1)\n        elif isinstance(task, OpenMLRegressionTask):\n            return output.view(-1)\n        else:\n            raise ValueError(f\"Unsupported task type: {type(task)}\")\n\n    @staticmethod\n    def _default_predict_proba(output: torch.Tensor, task: OpenMLTask) -&gt; torch.Tensor:\n        \"\"\"\n        Converts model outputs to probabilities using softmax.\n        \"\"\"\n        if not isinstance(task, OpenMLClassificationTask):\n            raise ValueError(\"predict_proba is only valid for classification tasks\")\n\n        return torch.nn.functional.softmax(output, dim=-1)\n\n    @staticmethod\n    def _default_sanitize(tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Replaces NaNs with 1e-6 in-place if possible.\n        \"\"\"\n        nan_mask = torch.isnan(tensor)\n        if nan_mask.any():\n            tensor = tensor.clone()  # clone only if modification is needed\n            tensor[nan_mask] = 1e-6\n        return tensor\n\n    @staticmethod\n    def _default_retype_labels(tensor: torch.Tensor, task: OpenMLTask) -&gt; torch.Tensor:\n        \"\"\"\n        _default_retype_labels changes the type of the tensor to long for classification tasks and to float for regression tasks\n        \"\"\"\n        if isinstance(task, OpenMLClassificationTask):\n            return tensor.long()\n        elif isinstance(task, OpenMLRegressionTask):\n            return tensor.float()\n        else:\n            raise ValueError(task)\n\n    def get_device(\n        self,\n    ):\n        \"\"\"\n        Checks if a GPU is available and returns the device to be used for training (cuda, mps or cpu)\n        \"\"\"\n        if torch.cuda.is_available():\n            device = torch.device(\"cuda\")\n        elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n            device = torch.device(\"mps\")\n        else:\n            device = torch.device(\"cpu\")\n\n        return device\n\n    def default_image_transform(self):\n        return Compose(\n            [\n                ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.\n                Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.\n                Resize((128, 128)),  # Resize the image.\n                ToTensor(),  # Convert the PIL Image back to a tensor.\n            ]\n        )\n\n    def default_image_transform_test(self):\n        return Compose(\n            [\n                ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.\n                Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.\n                Resize((128, 128)),  # Resize the image.\n                ToTensor(),  # Convert the PIL Image back to a tensor.\n            ]\n        )\n\n    def return_model_config(self):\n        \"\"\"\n        Returns a configuration object for the model\n        \"\"\"\n\n        return SimpleNamespace(\n            device=self.get_device(),\n            loss_fn=self._default_loss_fn_gen,\n            # predict turns the outputs of the model into actual predictions\n            predict=self._default_predict,  # type: Callable[[torch.Tensor, OpenMLTask], torch.Tensor]\n            # predict_proba turns the outputs of the model into probabilities for each class\n            predict_proba=self._default_predict_proba,  # type: Callable[[torch.Tensor], torch.Tensor]\n            # epoch_count represents the number of epochs the model should be trained for\n            epoch_count=3,  # type: int,\n            verbose=True,\n            scheduler=None,\n            opt_kwargs={},\n            scheduler_kwargs={},\n        )\n\n    def return_data_config(self):\n        \"\"\"\n        Returns a configuration object for the data\n        \"\"\"\n        return SimpleNamespace(\n            type_of_data=\"image\",\n            perform_validation=False,\n            # progress_callback is called when a training step is finished, in order to report the current progress\n            # sanitize sanitizes the input data in order to ensure that models can be trained safely\n            sanitize=self._default_sanitize,  # type: Callable[[torch.Tensor], torch.Tensor]\n            # retype_labels changes the types of the labels in order to ensure type compatibility\n            retype_labels=(\n                self._default_retype_labels\n            ),  # type: Callable[[torch.Tensor, OpenMLTask], torch.Tensor]\n            # image_size is the size of the images that are fed into the model\n            image_size=128,\n            # batch_size represents the processing batch size for training\n            batch_size=64,  # type: int\n            data_augmentation=None,\n            validation_split=0.1,\n            transform=self.default_image_transform(),\n            transform_test=self.default_image_transform_test(),\n        )\n</code></pre>"},{"location":"API%20reference/Trainer/#trainer.DefaultConfigGenerator.get_device","title":"<code>get_device()</code>","text":"<p>Checks if a GPU is available and returns the device to be used for training (cuda, mps or cpu)</p> Source code in <code>openml_pytorch/trainer.py</code> <pre><code>def get_device(\n    self,\n):\n    \"\"\"\n    Checks if a GPU is available and returns the device to be used for training (cuda, mps or cpu)\n    \"\"\"\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n\n    return device\n</code></pre>"},{"location":"API%20reference/Trainer/#trainer.DefaultConfigGenerator.return_data_config","title":"<code>return_data_config()</code>","text":"<p>Returns a configuration object for the data</p> Source code in <code>openml_pytorch/trainer.py</code> <pre><code>def return_data_config(self):\n    \"\"\"\n    Returns a configuration object for the data\n    \"\"\"\n    return SimpleNamespace(\n        type_of_data=\"image\",\n        perform_validation=False,\n        # progress_callback is called when a training step is finished, in order to report the current progress\n        # sanitize sanitizes the input data in order to ensure that models can be trained safely\n        sanitize=self._default_sanitize,  # type: Callable[[torch.Tensor], torch.Tensor]\n        # retype_labels changes the types of the labels in order to ensure type compatibility\n        retype_labels=(\n            self._default_retype_labels\n        ),  # type: Callable[[torch.Tensor, OpenMLTask], torch.Tensor]\n        # image_size is the size of the images that are fed into the model\n        image_size=128,\n        # batch_size represents the processing batch size for training\n        batch_size=64,  # type: int\n        data_augmentation=None,\n        validation_split=0.1,\n        transform=self.default_image_transform(),\n        transform_test=self.default_image_transform_test(),\n    )\n</code></pre>"},{"location":"API%20reference/Trainer/#trainer.DefaultConfigGenerator.return_model_config","title":"<code>return_model_config()</code>","text":"<p>Returns a configuration object for the model</p> Source code in <code>openml_pytorch/trainer.py</code> <pre><code>def return_model_config(self):\n    \"\"\"\n    Returns a configuration object for the model\n    \"\"\"\n\n    return SimpleNamespace(\n        device=self.get_device(),\n        loss_fn=self._default_loss_fn_gen,\n        # predict turns the outputs of the model into actual predictions\n        predict=self._default_predict,  # type: Callable[[torch.Tensor, OpenMLTask], torch.Tensor]\n        # predict_proba turns the outputs of the model into probabilities for each class\n        predict_proba=self._default_predict_proba,  # type: Callable[[torch.Tensor], torch.Tensor]\n        # epoch_count represents the number of epochs the model should be trained for\n        epoch_count=3,  # type: int,\n        verbose=True,\n        scheduler=None,\n        opt_kwargs={},\n        scheduler_kwargs={},\n    )\n</code></pre>"},{"location":"API%20reference/Trainer/#trainer.Learner","title":"<code>Learner</code>","text":"<p>A class to store the model, optimizer, loss_fn, and data loaders for training and evaluation.</p> Source code in <code>openml_pytorch/trainer.py</code> <pre><code>class Learner:\n    \"\"\"\n    A class to store the model, optimizer, loss_fn, and data loaders for training and evaluation.\n    \"\"\"\n\n    def __init__(\n        self, model, opt, loss_fn, scheduler, data, model_classes, device=torch.device(\"cpu\")\n    ):\n        (\n            self.model,\n            self.opt,\n            self.loss_fn,\n            self.scheduler,\n            self.data,\n            self.model_classes,\n            self.device,\n        ) = (model, opt, loss_fn, scheduler, data, model_classes, device)\n</code></pre>"},{"location":"API%20reference/Trainer/#trainer.OpenMLDataModule","title":"<code>OpenMLDataModule</code>","text":"Source code in <code>openml_pytorch/trainer.py</code> <pre><code>class OpenMLDataModule:\n    def __init__(\n        self,\n        type_of_data=\"image\",\n        filename_col=\"Filename\",\n        file_dir=\"images\",\n        target_mode=\"categorical\",\n        transform=None,\n        transform_test=None,\n        target_column=\"encoded_labels\",\n        num_workers=0,\n        batch_size = 64,\n        **kwargs,\n    ):\n        self.config_gen = DefaultConfigGenerator()\n        self.data_config = self.config_gen.return_data_config()\n        self.data_config.type_of_data = type_of_data\n        self.data_config.filename_col = filename_col\n        self.data_config.file_dir = file_dir\n        self.data_config.target_mode = target_mode\n        self.data_config.target_column = target_column\n        self.data_config.batch_size = batch_size\n        self.handler: BaseDataHandler | None = data_handlers.get(type_of_data)\n        self.num_workers = num_workers\n\n        if transform is not None:\n            self.data_config.transform = transform\n        if transform_test is not None:\n            self.data_config.transform_test = transform_test\n\n        if not self.handler:\n            raise ValueError(f\"Data type {type_of_data} not supported.\")\n\n    def get_data(\n        self,\n        X_train: pd.DataFrame,\n        y_train: Optional[pd.Series],\n        X_test: pd.DataFrame,\n        task,\n    ):\n        # Split the training data\n        X_train_train, X_val, y_train_train, y_val = self.split_training_data(\n            X_train, y_train\n        )\n\n        y_train_train, y_val, model_classes = self.encode_labels(y_train_train, y_val)\n\n        # Use handler to prepare datasets\n        train_loader, val_loader = self.prepare_datasets_for_training_and_validation(\n            X_train_train, X_val, y_train_train, y_val\n        )\n\n        # Prepare test data\n        test_loader = self.process_test_data(X_test)\n\n        return DataContainer(train_loader, val_loader, test_loader), model_classes\n\n    def process_test_data(self, X_test):\n        test = self.handler.prepare_test_data(X_test, self.data_config)\n        test_loader = DataLoader(\n            test, batch_size=self.data_config.batch_size, shuffle=False, num_workers = self.num_workers \n        )\n\n        return test_loader\n\n    def prepare_datasets_for_training_and_validation(\n        self, X_train_train, X_val, y_train_train, y_val\n    ):\n        if self.handler is None:\n            raise ValueError(\n                f\"Data type {self.data_config.type_of_data} not supported.\"\n            )\n\n        train, val = self.handler.prepare_data(\n            X_train_train, y_train_train, X_val, y_val, self.data_config\n        )\n\n        train_loader = DataLoader(\n            train, batch_size=self.data_config.batch_size, shuffle=True, num_workers=self.num_workers\n        )\n        val_loader = DataLoader(\n            val, batch_size=self.data_config.batch_size, shuffle=False, num_workers=self.num_workers\n        )\n\n        return train_loader, val_loader\n\n    def encode_labels(self, y_train_train, y_val):\n        \"\"\"\n        Encode the labels for categorical data\n        \"\"\"\n        if self.data_config.target_mode == \"categorical\":\n            label_encoder = preprocessing.LabelEncoder().fit(y_train_train)\n            y_train_train = pd.Series(label_encoder.transform(y_train_train))\n            y_val = pd.Series(label_encoder.transform(y_val))\n            # Determine model classes\n            model_classes = (\n                label_encoder.classes_\n                if self.data_config.target_mode == \"categorical\"\n                else None\n            )\n\n        return y_train_train, y_val, model_classes\n\n    def split_training_data(self, X_train, y_train):\n        if type(y_train) != pd.Series:\n            y_train = pd.Series(y_train)\n\n        X_train_train, X_val, y_train_train, y_val = train_test_split(\n            X_train,\n            y_train,\n            test_size=self.data_config.validation_split,\n            shuffle=True,\n            stratify=y_train,\n            random_state=0,\n        )\n\n        return X_train_train, X_val, y_train_train, y_val\n</code></pre>"},{"location":"API%20reference/Trainer/#trainer.OpenMLDataModule.encode_labels","title":"<code>encode_labels(y_train_train, y_val)</code>","text":"<p>Encode the labels for categorical data</p> Source code in <code>openml_pytorch/trainer.py</code> <pre><code>def encode_labels(self, y_train_train, y_val):\n    \"\"\"\n    Encode the labels for categorical data\n    \"\"\"\n    if self.data_config.target_mode == \"categorical\":\n        label_encoder = preprocessing.LabelEncoder().fit(y_train_train)\n        y_train_train = pd.Series(label_encoder.transform(y_train_train))\n        y_val = pd.Series(label_encoder.transform(y_val))\n        # Determine model classes\n        model_classes = (\n            label_encoder.classes_\n            if self.data_config.target_mode == \"categorical\"\n            else None\n        )\n\n    return y_train_train, y_val, model_classes\n</code></pre>"},{"location":"API%20reference/Trainer/#trainer.OpenMLImageHandler","title":"<code>OpenMLImageHandler</code>","text":"<p>               Bases: <code>BaseDataHandler</code></p> <p>OpenMLImageHandler is a class that extends BaseDataHandler to handle image data from OpenML datasets.</p> Source code in <code>openml_pytorch/trainer.py</code> <pre><code>class OpenMLImageHandler(BaseDataHandler):\n    \"\"\"\n    OpenMLImageHandler is a class that extends BaseDataHandler to handle image data from OpenML datasets.\n    \"\"\"\n\n    def prepare_data(self, X_train, y_train, X_val, y_val, data_config=None):\n        train = OpenMLImageDataset(\n            image_dir=data_config.file_dir,\n            X=X_train,\n            y=y_train,\n            transform_x=data_config.transform,\n            image_size=data_config.image_size,\n        )\n        val = OpenMLImageDataset(\n            image_dir=data_config.file_dir,\n            X=X_val,\n            y=y_val,\n            transform_x=data_config.transform_test,\n            image_size=data_config.image_size,\n        )\n        return train, val\n\n    def prepare_test_data(self, X_test, data_config=None):\n        test = OpenMLImageDataset(\n            image_dir=data_config.file_dir,\n            X=X_test,\n            y=None,\n            transform_x=data_config.transform_test,\n            image_size=data_config.image_size,\n        )\n        return test\n</code></pre>"},{"location":"API%20reference/Trainer/#trainer.OpenMLTabularHandler","title":"<code>OpenMLTabularHandler</code>","text":"<p>               Bases: <code>BaseDataHandler</code></p> <p>OpenMLTabularHandler is a class that extends BaseDataHandler to handle tabular data from OpenML datasets.</p> Source code in <code>openml_pytorch/trainer.py</code> <pre><code>class OpenMLTabularHandler(BaseDataHandler):\n    \"\"\"\n    OpenMLTabularHandler is a class that extends BaseDataHandler to handle tabular data from OpenML datasets.\n    \"\"\"\n\n    def prepare_data(self, X_train, y_train, X_val, y_val, data_config=None):\n        train = OpenMLTabularDataset(X=X_train, y=y_train)\n        val = OpenMLTabularDataset(X=X_val, y=y_val)\n        return train, val\n\n    def prepare_test_data(self, X_test, data_config=None):\n        test = OpenMLTabularDataset(X=X_test, y=None)\n        return test\n</code></pre>"},{"location":"API%20reference/Trainer/#trainer.OpenMLTrainerModule","title":"<code>OpenMLTrainerModule</code>","text":"Source code in <code>openml_pytorch/trainer.py</code> <pre><code>class OpenMLTrainerModule:\n    def _default_progress_callback(\n        self, fold: int, rep: int, epoch: int, step: int, loss: float, accuracy: float\n    ):\n        # todo : move this into callback\n        \"\"\"\n                _default_progress_callback reports the current fold, rep, epoch, step and loss for every\n        training iteration to the default logger\n        \"\"\"\n        self.logger.info(\n            \"[%d, %d, %d, %d] loss: %.4f, accuracy: %.4f\"\n            % (fold, rep, epoch, step, loss, accuracy)\n        )\n\n    def __init__(\n        self,\n        experiment_name: str,\n        data_module: OpenMLDataModule,\n        opt: Callable = torch.optim.AdamW,\n        opt_kwargs: dict = {\"lr\": 3e-4, \"weight_decay\": 1e-4},\n        loss_fn: Callable = torch.nn.CrossEntropyLoss,\n        loss_fn_kwargs: dict = {},\n        callbacks: List[Callback] = [],\n        use_tensorboard: bool = True,\n        metrics: List[Callable] = [],\n        scheduler=torch.optim.lr_scheduler.CosineAnnealingLR,\n        scheduler_kwargs: dict = {\"T_max\": 50, \"eta_min\": 1e-6},\n        **kwargs,\n    ):\n        self.experiment_name = experiment_name\n        self.config_gen = DefaultConfigGenerator()\n        self.model_config = self.config_gen.return_model_config()\n        self.data_module = data_module\n        self.callbacks = callbacks\n        self.metrics = metrics\n\n        self.config = SimpleNamespace(\n            **{**self.model_config.__dict__, **self.data_module.data_config.__dict__}\n        )\n        # update the config with the user defined values\n        self.config.__dict__.update(kwargs)\n        self.config.opt = opt\n        self.config.opt_kwargs = opt_kwargs\n        self.config.loss_fn_kwargs = loss_fn_kwargs\n        if loss_fn is not None:\n            self.loss_fn = loss_fn(**self.config.loss_fn_kwargs)\n        self.config.progress_callback = self._default_progress_callback\n        self.logger: logging.Logger = logging.getLogger(__name__)\n\n        self.user_defined_measures = OrderedDict()\n        # Tensorboard support\n        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.tensorboard_writer = None\n\n        if use_tensorboard:\n            self.tensorboard_writer = SummaryWriter(\n                comment=experiment_name,\n                log_dir=f\"tensorboard_logs/{experiment_name}/{timestamp}\",\n            )\n\n        self.loss = 0\n        self.training_state = True\n\n        self.phases = [0.2, 0.8]\n        self.config.scheduler = scheduler\n        self.config.scheduler_kwargs = scheduler_kwargs\n\n        # Add default callbacks\n        self.cbfs = [\n            Recorder,\n            partial(AvgStatsCallback, self.metrics),\n            partial(PutDataOnDeviceCallback, self.config.device),\n        ]\n        if self.tensorboard_writer is not None:\n            self.cbfs.append(partial(TensorBoardCallback, self.tensorboard_writer))\n\n        self.add_callbacks()\n\n    def export_to_onnx(self, model_copy):\n        \"\"\"\n        Converts the model to ONNX format. Uses a hack for now (global variable) to get the sample input.\n        \"\"\"\n        global sample_input\n        f = io.BytesIO()\n        torch.onnx.export(model_copy, sample_input, f)\n        onnx_model = onnx.load_model_from_string(f.getvalue())\n        onnx_ = onnx_model.SerializeToString()\n        global last_models\n        last_models = onnx_\n        return onnx_\n\n    def export_to_netron(self, onnx_file_name: str = f\"model.onnx\"):\n        \"\"\"\n        Exports the model to ONNX format and serves it using netron.\n        \"\"\"\n        if self.onnx_model is None:\n            try:\n                self.onnx_model = self.export_to_onnx(self.model)\n            except Exception as e:\n                raise ValueError(\"Model is not defined\")\n            raise ValueError(\"Model is not defined\")\n\n        # write the onnx model to a file\n        with open(onnx_file_name, \"wb\") as f:\n            f.write(self.onnx_model)\n            print(f\"Writing onnx model to {onnx_file_name}. Delete if neeeded\")\n\n        # serve with netro\n        netron.start(onnx_file_name)\n\n    def run_model_on_fold(\n        self,\n        model: torch.nn.Module,\n        task: OpenMLTask,\n        X_train: pd.DataFrame,\n        rep_no: int,\n        fold_no: int,\n        y_train: Optional[pd.Series],\n        X_test: pd.DataFrame,\n    ):\n        # if task has no class labels, we assign the class labels to be the unique values in the training set\n        if task.class_labels is None:\n            task.class_labels = y_train.unique()\n\n        # Add the user defined callbacks\n\n        self.model = copy.deepcopy(model)\n\n        try:\n            data, model_classes = self.run_training(task, X_train, y_train, X_test)\n\n        except AttributeError as e:\n            # typically happens when training a regressor8 on classification task\n            raise PyOpenMLError(str(e))\n\n        # In supervised learning this returns the predictions for Y\n        pred_y, proba_y = self.run_evaluation(task, data, model_classes)\n\n        # Convert predictions to class labels\n        if task.class_labels is not None:\n            pred_y = [task.class_labels[i] for i in pred_y]\n\n        # Convert model to onnx\n        onnx_ = self.export_to_onnx(self.model)\n\n        # Hack to store the last model for ONNX conversion\n        global last_models\n        # last_models = onnx_\n        self.onnx_model = onnx_\n\n        return pred_y, proba_y, self.user_defined_measures, None\n\n    def check_config(self):\n        raise NotImplementedError\n\n    def _prediction_to_probabilities(self, y: np.ndarray, classes: List[Any]) -&gt; np.ndarray:\n        \"\"\"\n        Converts predicted class indices into one-hot probability vectors matching OpenML class indices.\n        \"\"\"\n        if not isinstance(classes, list):\n            raise ValueError(\"Please convert model classes to list prior to calling this function.\")\n\n        n_samples = len(y)\n        n_classes = len(classes)\n\n        # Ensure y is an integer array\n        y = np.asarray(y, dtype=np.int64)\n\n        result = np.zeros((n_samples, n_classes), dtype=np.float32)\n        result[np.arange(n_samples), y] = 1.0  # vectorized one-hot assignment\n        return result\n\n    def run_evaluation(self, task, data, model_classes):\n        if not isinstance(task, OpenMLSupervisedTask):\n            raise ValueError(task)\n\n        self.model.eval()\n        with torch.no_grad():\n\n            # Run inference once\n            logits = self.pred_test(task, self.model, data.test_dl, lambda x, _: x)\n\n            # Get predictions\n            pred_y = self.config.predict(torch.from_numpy(logits), task).numpy()\n\n            proba_y = None\n            if isinstance(task, OpenMLClassificationTask):\n                try:\n                    proba_y = self.config.predict_proba(torch.from_numpy(logits), task).numpy()\n                except AttributeError:\n                    if task.class_labels is None:\n                        raise ValueError(\"The task has no class labels\")\n                    proba_y = self._prediction_to_probabilities(pred_y, list(task.class_labels))\n\n                # Adjust shape if needed\n                if task.class_labels is not None and proba_y.shape[1] != len(task.class_labels):\n                    self.logger.warning(\"Mismatch in predicted probabilities and number of class labels.\")\n                    proba_y_new = np.zeros((proba_y.shape[0], len(task.class_labels)))\n                    for idx, model_class in enumerate(model_classes):\n                        if model_class &lt; proba_y_new.shape[1]:\n                            proba_y_new[:, model_class] = proba_y[:, idx]\n                    proba_y = proba_y_new\n\n                    if proba_y.shape[1] != len(task.class_labels):\n                        message = f\"Estimator only predicted for {proba_y.shape[1]}/{len(task.class_labels)} classes!\"\n                        warnings.warn(message)\n                        self.logger.warning(message)\n\n            elif isinstance(task, OpenMLRegressionTask):\n                proba_y = None\n            else:\n                raise TypeError(type(task))\n\n        print(\"Evaluation done\")\n        return pred_y, proba_y\n\n    def run_training(self, task, X_train, y_train, X_test):\n        if isinstance(task, OpenMLSupervisedTask) or isinstance(\n            task, OpenMLClassificationTask\n        ):\n            self.opt = self.config.opt(\n                self.model.parameters(), **self.config.opt_kwargs\n            )\n            self.scheduler = self.config.scheduler(\n                optimizer=self.opt, **self.config.scheduler_kwargs\n            )\n            if self.loss_fn is None:\n                self.loss_fn = self.config.loss_fn(task)\n            self.device = self.config.device\n\n            if self.config.device != torch.device(\"cpu\"):\n                self.loss_fn = self.loss_fn.to(self.config.device)\n\n            self.data, self.model_classes = self.data_module.get_data(\n                X_train, y_train, X_test, task\n            )\n            self.learn = Learner(\n                model=self.model,\n                opt=self.opt,\n                loss_fn=self.loss_fn,\n                scheduler=self.scheduler,\n                data=self.data,\n                model_classes=self.model_classes,\n            )\n            self.learn.device = self.device\n            self.learn.model.to(self.device)\n            gc.collect()\n\n            self.runner = ModelRunner(cb_funcs=self.cbfs)\n\n            # some additional default callbacks\n            self.stats = self.runner.cbs[1]\n            self.plot_loss = self.stats.plot_loss\n            self.plot_lr = self.stats.plot_lr\n            self.plot_metric = self.stats.plot_metric\n            self.plot_all_metrics = self.stats.plot_all_metrics\n\n            self.learn.model.train()\n            self.runner.fit(epochs=self.config.epoch_count, learn=self.learn)\n            self.learn.model.eval()\n\n            self.lrs = self.runner.cbs[1].lrs\n\n            print(\"Loss\", self.runner.loss)\n        else:\n            raise Exception(\"OpenML Task type not supported\")\n        return self.data, self.model_classes\n\n    def add_callbacks(self):\n        \"\"\"\n        Adds the user-defined callbacks to the list of callbacks\n        \"\"\"\n        if self.callbacks is not None and len(self.callbacks) &gt; 0:\n            for callback in self.callbacks:\n                if callback not in self.cbfs:\n                    self.cbfs.append(callback)\n                else:\n                    # replace the callback with the new one in the same position\n                    self.cbfs[self.cbfs.index(callback)] = callback\n\n    def pred_test(self, task, model, test_loader, predict_func):\n        model.eval()\n        device = self.config.device\n        sanitize = self.config.sanitize\n\n        all_outputs = []\n\n        with torch.no_grad():\n            for batch in test_loader:\n                inputs = sanitize(batch).to(device)\n                outputs = model(inputs)\n                outputs = predict_func(outputs, task)\n                all_outputs.append(outputs.detach().cpu())  # keep as tensor\n\n        return torch.cat(all_outputs, dim=0).numpy()  # only convert once\n</code></pre>"},{"location":"API%20reference/Trainer/#trainer.OpenMLTrainerModule.add_callbacks","title":"<code>add_callbacks()</code>","text":"<p>Adds the user-defined callbacks to the list of callbacks</p> Source code in <code>openml_pytorch/trainer.py</code> <pre><code>def add_callbacks(self):\n    \"\"\"\n    Adds the user-defined callbacks to the list of callbacks\n    \"\"\"\n    if self.callbacks is not None and len(self.callbacks) &gt; 0:\n        for callback in self.callbacks:\n            if callback not in self.cbfs:\n                self.cbfs.append(callback)\n            else:\n                # replace the callback with the new one in the same position\n                self.cbfs[self.cbfs.index(callback)] = callback\n</code></pre>"},{"location":"API%20reference/Trainer/#trainer.OpenMLTrainerModule.export_to_netron","title":"<code>export_to_netron(onnx_file_name=f'model.onnx')</code>","text":"<p>Exports the model to ONNX format and serves it using netron.</p> Source code in <code>openml_pytorch/trainer.py</code> <pre><code>def export_to_netron(self, onnx_file_name: str = f\"model.onnx\"):\n    \"\"\"\n    Exports the model to ONNX format and serves it using netron.\n    \"\"\"\n    if self.onnx_model is None:\n        try:\n            self.onnx_model = self.export_to_onnx(self.model)\n        except Exception as e:\n            raise ValueError(\"Model is not defined\")\n        raise ValueError(\"Model is not defined\")\n\n    # write the onnx model to a file\n    with open(onnx_file_name, \"wb\") as f:\n        f.write(self.onnx_model)\n        print(f\"Writing onnx model to {onnx_file_name}. Delete if neeeded\")\n\n    # serve with netro\n    netron.start(onnx_file_name)\n</code></pre>"},{"location":"API%20reference/Trainer/#trainer.OpenMLTrainerModule.export_to_onnx","title":"<code>export_to_onnx(model_copy)</code>","text":"<p>Converts the model to ONNX format. Uses a hack for now (global variable) to get the sample input.</p> Source code in <code>openml_pytorch/trainer.py</code> <pre><code>def export_to_onnx(self, model_copy):\n    \"\"\"\n    Converts the model to ONNX format. Uses a hack for now (global variable) to get the sample input.\n    \"\"\"\n    global sample_input\n    f = io.BytesIO()\n    torch.onnx.export(model_copy, sample_input, f)\n    onnx_model = onnx.load_model_from_string(f.getvalue())\n    onnx_ = onnx_model.SerializeToString()\n    global last_models\n    last_models = onnx_\n    return onnx_\n</code></pre>"},{"location":"API%20reference/Trainer/#trainer.convert_to_rgb","title":"<code>convert_to_rgb(image)</code>","text":"<p>Converts an image to RGB mode if it is not already in that mode.</p> <p>Parameters: image (PIL.Image): The image to be converted.</p> <p>Returns: PIL.Image: The converted image in RGB mode.</p> Source code in <code>openml_pytorch/trainer.py</code> <pre><code>def convert_to_rgb(image):\n    \"\"\"\n    Converts an image to RGB mode if it is not already in that mode.\n\n    Parameters:\n    image (PIL.Image): The image to be converted.\n\n    Returns:\n    PIL.Image: The converted image in RGB mode.\n    \"\"\"\n    if image.mode != \"RGB\":\n        return image.convert(\"RGB\")\n    return image\n</code></pre>"},{"location":"Examples/","title":"Examples","text":"<ul> <li>This folder contains examples of how to use the <code>openml-pytorch</code> extension for different types of data. </li> <li>Please refer to limitations before using the extension.</li> <li>Just want data from OpenML? Check out get data from OpenML</li> <li>Want a quick start? Check out image classification example</li> </ul>"},{"location":"Examples/Create%20Dataset%20and%20Task/","title":"Create dataset and task - tiniest imagenet","text":"In\u00a0[13]: Copied! <pre>import openml\n\nimport pandas as pd\n\nimport openml\nfrom openml.datasets.functions import create_dataset\nimport os\nimport requests\nimport zipfile\nimport glob\n</pre> import openml  import pandas as pd  import openml from openml.datasets.functions import create_dataset import os import requests import zipfile import glob In\u00a0[14]: Copied! <pre>def create_tiny_imagenet():\n    dir_name = \"datasets\"\n    os.makedirs(dir_name, exist_ok=True)\n\n    # download the dataset\n    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n    r = requests.get(url, stream=True)\n\n    if not os.path.exists(f\"{dir_name}/tiny-imagenet-200.zip\"):\n        with open(f\"{dir_name}/tiny-imagenet-200.zip\", \"wb\") as f:\n            f.write(r.content)\n\n        with zipfile.ZipFile(f\"{dir_name}/tiny-imagenet-200.zip\", 'r') as zip_ref:\n            zip_ref.extractall(f\"{dir_name}/\")\n    ## recusively find all the images\n    image_paths = glob.glob(f\"{dir_name}/tiny-imagenet-200/train/*/*/*.JPEG\")\n    ## remove the first part of the path\n    image_paths = [path.split(\"/\", 1)[-1] for path in image_paths]\n    ## create a dataframe with the image path and the label\n    label_func = lambda x: x.split(\"/\")[2]\n    df = pd.DataFrame(image_paths, columns=[\"image_path\"])\n    df[\"label\"] = df[\"image_path\"].apply(label_func)\n    ## encode the labels as integers\n    # df[\"Class_encoded\"] = pd.factorize(df[\"label\"])[0]\n\n    ## encode types\n    df[\"image_path\"] = df[\"image_path\"].astype(\"string\")\n    df[\"label\"] = df[\"label\"].astype(\"category\")\n\n\n    name = \"tiny-imagenet-200\"\n    attribute_names = df.columns\n    description = \"Tiny ImageNet contains 100000 images of 200 classes (500 for each class) downsized to 64 x 64 colored images. Each class has 500 training images, 50 validation images, and 50 test images. The dataset here just contains links to the images and the labels. The dataset can be downloaded from the official website ![here](http://cs231n.stanford.edu/tiny-imagenet-200.zip). /n Link to the paper - [Tiny ImageNet Classification with CNN](https://cs231n.stanford.edu/reports/2017/pdfs/930.pdf)\"\n    paper_url = \"https://cs231n.stanford.edu/reports/2017/pdfs/930.pdf\"\n    citation = (\"Wu, J., Zhang, Q., &amp; Xu, G. (2017). Tiny imagenet challenge. Technical report.\")\n\n    tinyim = create_dataset(\n        name = name,\n        description = description,\n        creator= \"Jiayu Wu, Qixiang Zhang, Guoxi Xu\",\n        contributor = \"Jiayu Wu, Qixiang Zhang, Guoxi Xu\",\n        collection_date = \"2017\",\n        language= \"English\",\n        licence=\"DbCL v1.0\",\n        default_target_attribute=\"label\",\n        attributes=\"auto\",\n        data=df,\n        citation=citation,\n        ignore_attribute=None\n    )\n    openml.config.apikey = ''\n    tinyim.publish()\n    print(f\"URL for dataset: {tinyim.openml_url}\")\n</pre> def create_tiny_imagenet():     dir_name = \"datasets\"     os.makedirs(dir_name, exist_ok=True)      # download the dataset     url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"     r = requests.get(url, stream=True)      if not os.path.exists(f\"{dir_name}/tiny-imagenet-200.zip\"):         with open(f\"{dir_name}/tiny-imagenet-200.zip\", \"wb\") as f:             f.write(r.content)          with zipfile.ZipFile(f\"{dir_name}/tiny-imagenet-200.zip\", 'r') as zip_ref:             zip_ref.extractall(f\"{dir_name}/\")     ## recusively find all the images     image_paths = glob.glob(f\"{dir_name}/tiny-imagenet-200/train/*/*/*.JPEG\")     ## remove the first part of the path     image_paths = [path.split(\"/\", 1)[-1] for path in image_paths]     ## create a dataframe with the image path and the label     label_func = lambda x: x.split(\"/\")[2]     df = pd.DataFrame(image_paths, columns=[\"image_path\"])     df[\"label\"] = df[\"image_path\"].apply(label_func)     ## encode the labels as integers     # df[\"Class_encoded\"] = pd.factorize(df[\"label\"])[0]      ## encode types     df[\"image_path\"] = df[\"image_path\"].astype(\"string\")     df[\"label\"] = df[\"label\"].astype(\"category\")       name = \"tiny-imagenet-200\"     attribute_names = df.columns     description = \"Tiny ImageNet contains 100000 images of 200 classes (500 for each class) downsized to 64 x 64 colored images. Each class has 500 training images, 50 validation images, and 50 test images. The dataset here just contains links to the images and the labels. The dataset can be downloaded from the official website ![here](http://cs231n.stanford.edu/tiny-imagenet-200.zip). /n Link to the paper - [Tiny ImageNet Classification with CNN](https://cs231n.stanford.edu/reports/2017/pdfs/930.pdf)\"     paper_url = \"https://cs231n.stanford.edu/reports/2017/pdfs/930.pdf\"     citation = (\"Wu, J., Zhang, Q., &amp; Xu, G. (2017). Tiny imagenet challenge. Technical report.\")      tinyim = create_dataset(         name = name,         description = description,         creator= \"Jiayu Wu, Qixiang Zhang, Guoxi Xu\",         contributor = \"Jiayu Wu, Qixiang Zhang, Guoxi Xu\",         collection_date = \"2017\",         language= \"English\",         licence=\"DbCL v1.0\",         default_target_attribute=\"label\",         attributes=\"auto\",         data=df,         citation=citation,         ignore_attribute=None     )     openml.config.apikey = ''     tinyim.publish()     print(f\"URL for dataset: {tinyim.openml_url}\")  In\u00a0[\u00a0]: Copied! <pre>create_tiny_imagenet()\n# https://www.openml.org/d/46577\n</pre> create_tiny_imagenet() # https://www.openml.org/d/46577 In\u00a0[16]: Copied! <pre>def create_tiniest_imagenet():\n    dir_name = \"datasets\"\n    os.makedirs(dir_name, exist_ok=True)\n\n    # download the dataset\n    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n    r = requests.get(url, stream=True)\n\n    if not os.path.exists(f\"{dir_name}/tiny-imagenet-200.zip\"):\n        with open(f\"{dir_name}/tiny-imagenet-200.zip\", \"wb\") as f:\n            f.write(r.content)\n\n        with zipfile.ZipFile(f\"{dir_name}/tiny-imagenet-200.zip\", 'r') as zip_ref:\n            zip_ref.extractall(f\"{dir_name}/\")\n    ## recusively find all the images\n    image_paths = glob.glob(f\"{dir_name}/tiny-imagenet-200/train/*/*/*.JPEG\")\n    ## remove the first part of the path\n    image_paths = [path.split(\"/\", 1)[-1] for path in image_paths]\n    image_paths[-1]\n    ## create a dataframe with the image path and the label\n    label_func = lambda x: x.split(\"/\")[2]\n    df = pd.DataFrame(image_paths, columns=[\"image_path\"])\n    df[\"label\"] = df[\"image_path\"].apply(label_func)\n    ## encode types\n    df[\"image_path\"] = df[\"image_path\"].astype(\"string\")\n    df[\"label\"] = df[\"label\"].astype(\"category\")\n\n    # keep only first 20 images for each label\n    df = df.groupby(\"label\").head(20)\n\n\n    name = \"tiniest-imagenet-200\"\n    attribute_names = df.columns\n    description = \"Tiny ImageNet contains 100000 images of 200 classes (500 for each class) downsized to 64 x 64 colored images. !!! This dataset only links to 20 images per class (instead of the usual 500) and is ONLY for quickly testing a framework. !!! Each class has 500 training images, 50 validation images, and 50 test images. The dataset here just contains links to the images and the labels. The dataset can be downloaded from the official website ![here](http://cs231n.stanford.edu/tiny-imagenet-200.zip). /n Link to the paper - [Tiny ImageNet Classification with CNN](https://cs231n.stanford.edu/reports/2017/pdfs/930.pdf)\"\n    paper_url = \"https://cs231n.stanford.edu/reports/2017/pdfs/930.pdf\"\n    citation = (\"Wu, J., Zhang, Q., &amp; Xu, G. (2017). Tiny imagenet challenge. Technical report.\")\n\n    tinyim = create_dataset(\n        name = name,\n        description = description,\n        creator= \"Jiayu Wu, Qixiang Zhang, Guoxi Xu\",\n        contributor = \"Jiayu Wu, Qixiang Zhang, Guoxi Xu\",\n        collection_date = \"2017\",\n        language= \"English\",\n        licence=\"DbCL v1.0\",\n        default_target_attribute=\"label\",\n        attributes=\"auto\",\n        data=df,\n        citation=citation,\n        ignore_attribute=None\n    )\n    openml.config.apikey = ''\n    tinyim.publish()\n    print(f\"URL for dataset: {tinyim.openml_url}\")\n</pre> def create_tiniest_imagenet():     dir_name = \"datasets\"     os.makedirs(dir_name, exist_ok=True)      # download the dataset     url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"     r = requests.get(url, stream=True)      if not os.path.exists(f\"{dir_name}/tiny-imagenet-200.zip\"):         with open(f\"{dir_name}/tiny-imagenet-200.zip\", \"wb\") as f:             f.write(r.content)          with zipfile.ZipFile(f\"{dir_name}/tiny-imagenet-200.zip\", 'r') as zip_ref:             zip_ref.extractall(f\"{dir_name}/\")     ## recusively find all the images     image_paths = glob.glob(f\"{dir_name}/tiny-imagenet-200/train/*/*/*.JPEG\")     ## remove the first part of the path     image_paths = [path.split(\"/\", 1)[-1] for path in image_paths]     image_paths[-1]     ## create a dataframe with the image path and the label     label_func = lambda x: x.split(\"/\")[2]     df = pd.DataFrame(image_paths, columns=[\"image_path\"])     df[\"label\"] = df[\"image_path\"].apply(label_func)     ## encode types     df[\"image_path\"] = df[\"image_path\"].astype(\"string\")     df[\"label\"] = df[\"label\"].astype(\"category\")      # keep only first 20 images for each label     df = df.groupby(\"label\").head(20)       name = \"tiniest-imagenet-200\"     attribute_names = df.columns     description = \"Tiny ImageNet contains 100000 images of 200 classes (500 for each class) downsized to 64 x 64 colored images. !!! This dataset only links to 20 images per class (instead of the usual 500) and is ONLY for quickly testing a framework. !!! Each class has 500 training images, 50 validation images, and 50 test images. The dataset here just contains links to the images and the labels. The dataset can be downloaded from the official website ![here](http://cs231n.stanford.edu/tiny-imagenet-200.zip). /n Link to the paper - [Tiny ImageNet Classification with CNN](https://cs231n.stanford.edu/reports/2017/pdfs/930.pdf)\"     paper_url = \"https://cs231n.stanford.edu/reports/2017/pdfs/930.pdf\"     citation = (\"Wu, J., Zhang, Q., &amp; Xu, G. (2017). Tiny imagenet challenge. Technical report.\")      tinyim = create_dataset(         name = name,         description = description,         creator= \"Jiayu Wu, Qixiang Zhang, Guoxi Xu\",         contributor = \"Jiayu Wu, Qixiang Zhang, Guoxi Xu\",         collection_date = \"2017\",         language= \"English\",         licence=\"DbCL v1.0\",         default_target_attribute=\"label\",         attributes=\"auto\",         data=df,         citation=citation,         ignore_attribute=None     )     openml.config.apikey = ''     tinyim.publish()     print(f\"URL for dataset: {tinyim.openml_url}\")  In\u00a0[\u00a0]: Copied! <pre>create_tiniest_imagenet()\n# https://www.openml.org/d/46578\n</pre> create_tiniest_imagenet() # https://www.openml.org/d/46578 In\u00a0[20]: Copied! <pre>def create_task():\n    # Define task parameters\n    task_type = openml.tasks.TaskType.SUPERVISED_CLASSIFICATION\n    dataset_id = 46578 # Obtained from the dataset creation step\n    evaluation_measure = 'predictive_accuracy'\n    target_name = 'label'\n    class_labels = list(pd.read_csv(\"datasets/tiniest_imagenet.csv\")[\"label\"].unique())\n    cost_matrix = None\n\n    # Create the task\n    new_task = openml.tasks.create_task(\n        task_type=task_type,\n        dataset_id=dataset_id, \n        estimation_procedure_id = 1,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        class_labels=class_labels,\n        cost_matrix=cost_matrix\n    )\n    openml.config.apikey = ''\n    new_task.publish()\n    print(f\"URL for task: {new_task.openml_url}\")\n</pre> def create_task():     # Define task parameters     task_type = openml.tasks.TaskType.SUPERVISED_CLASSIFICATION     dataset_id = 46578 # Obtained from the dataset creation step     evaluation_measure = 'predictive_accuracy'     target_name = 'label'     class_labels = list(pd.read_csv(\"datasets/tiniest_imagenet.csv\")[\"label\"].unique())     cost_matrix = None      # Create the task     new_task = openml.tasks.create_task(         task_type=task_type,         dataset_id=dataset_id,          estimation_procedure_id = 1,         evaluation_measure=evaluation_measure,         target_name=target_name,         class_labels=class_labels,         cost_matrix=cost_matrix     )     openml.config.apikey = ''     new_task.publish()     print(f\"URL for task: {new_task.openml_url}\") In\u00a0[\u00a0]: Copied! <pre>create_task()\n# https://www.openml.org/t/363295\n</pre> create_task() # https://www.openml.org/t/363295"},{"location":"Examples/Create%20Dataset%20and%20Task/#create-dataset-and-task-tiniest-imagenet","title":"Create dataset and task - tiniest imagenet\u00b6","text":"<ul> <li>An example of how to create a custom dataset and task using the OpenML API and upload it to the OpenML server.</li> <li>Note that you must have an API key from the OpenML website to upload datasets and tasks.</li> </ul>"},{"location":"Examples/Create%20Dataset%20and%20Task/#create-dataset-on-openml","title":"Create dataset on OpenML\u00b6","text":"<ul> <li>Instead of making our own, we obtain a subset of the ImageNet dataset from Stanford. This dataset has 200 classes.</li> </ul>"},{"location":"Examples/Create%20Dataset%20and%20Task/#another-even-tinier-dataset","title":"Another, even tinier dataset\u00b6","text":"<ul> <li>We subset the previous dataset to 20 images per class.</li> </ul>"},{"location":"Examples/Create%20Dataset%20and%20Task/#create-task-on-openml","title":"Create task on OpenML\u00b6","text":"<ul> <li>Now to actually use the OpenML Pytorch API, we need to have a task associated with the dataset. This is how we create it.</li> </ul>"},{"location":"Examples/Getting%20data%20from%20OpenML/","title":"Get data and create dataloaders","text":"In\u00a0[\u00a0]: Copied! <pre># import libraries\nimport openml\nimport torch\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom typing import Any\nfrom tqdm import tqdm\n\nimport openml_pytorch as op\n</pre> # import libraries import openml import torch import numpy as np from sklearn.model_selection import train_test_split from typing import Any from tqdm import tqdm  import openml_pytorch as op In\u00a0[\u00a0]: Copied! <pre># Get dataset by ID\ndataset = openml.datasets.get_dataset(20)\n\n# Get the X, y data\nX, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\nX = X.to_numpy(dtype=np.float32)  # Ensure X is a NumPy array of float32\ny = y.to_numpy(dtype=np.int64)    # Ensure y is a NumPy array of int64 (for classification)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n\n# Dataloaders\nds_train = op.GenericDataset(X_train, y_train)\nds_test = op.GenericDataset(X_test, y_test)\ndataloader_train = torch.utils.data.DataLoader(ds_train, batch_size=64, shuffle=True)\ndataloader_test = torch.utils.data.DataLoader(ds_test, batch_size=64, shuffle=False)\n</pre>  # Get dataset by ID dataset = openml.datasets.get_dataset(20)  # Get the X, y data X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute) X = X.to_numpy(dtype=np.float32)  # Ensure X is a NumPy array of float32 y = y.to_numpy(dtype=np.int64)    # Ensure y is a NumPy array of int64 (for classification)  # Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)  # Dataloaders ds_train = op.GenericDataset(X_train, y_train) ds_test = op.GenericDataset(X_test, y_test) dataloader_train = torch.utils.data.DataLoader(ds_train, batch_size=64, shuffle=True) dataloader_test = torch.utils.data.DataLoader(ds_test, batch_size=64, shuffle=False)  In\u00a0[\u00a0]: Copied! <pre># Model Definition\nclass TabularClassificationModel(torch.nn.Module):\n    def __init__(self, input_size, output_size):\n        super(TabularClassificationModel, self).__init__()\n        self.fc1 = torch.nn.Linear(input_size, 128)\n        self.fc2 = torch.nn.Linear(128, 64)\n        self.fc3 = torch.nn.Linear(64, output_size)\n        self.relu = torch.nn.ReLU()\n        self.softmax = torch.nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        x = self.softmax(x)\n        return x\n</pre> # Model Definition class TabularClassificationModel(torch.nn.Module):     def __init__(self, input_size, output_size):         super(TabularClassificationModel, self).__init__()         self.fc1 = torch.nn.Linear(input_size, 128)         self.fc2 = torch.nn.Linear(128, 64)         self.fc3 = torch.nn.Linear(64, output_size)         self.relu = torch.nn.ReLU()         self.softmax = torch.nn.Softmax(dim=1)      def forward(self, x):         x = self.fc1(x)         x = self.relu(x)         x = self.fc2(x)         x = self.relu(x)         x = self.fc3(x)         x = self.softmax(x)         return x  In\u00a0[\u00a0]: Copied! <pre># Train the model\ntrainer = op.BasicTrainer(\n    model = TabularClassificationModel(X_train.shape[1], len(np.unique(y_train))),\n    loss_fn = torch.nn.CrossEntropyLoss(),\n    opt = torch.optim.Adam,\n    dataloader_train = dataloader_train,\n    dataloader_test = dataloader_test,\n    device= torch.device(\"mps\")\n)\ntrainer.fit(10)\n</pre> # Train the model trainer = op.BasicTrainer(     model = TabularClassificationModel(X_train.shape[1], len(np.unique(y_train))),     loss_fn = torch.nn.CrossEntropyLoss(),     opt = torch.optim.Adam,     dataloader_train = dataloader_train,     dataloader_test = dataloader_test,     device= torch.device(\"mps\") ) trainer.fit(10) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Examples/Getting%20data%20from%20OpenML/#getting-data-from-openml-and-then-using-your-own-training-pipeline","title":"Getting Data from OpenML ... and then using your own training pipeline\u00b6","text":"<ul> <li>Just want the data and don't want to deal with anything else?</li> <li>Have some complicated idea you want to try? Don't want to be limited by this API? No problem!</li> <li>You can use your own training pipeline and still use data from OpenML ... but, you cannot upload your results back to OpenML this way as of now. ):</li> </ul>"},{"location":"Examples/Getting%20data%20from%20OpenML/#get-data-and-create-dataloaders","title":"Get data and create dataloaders\u00b6","text":"<ul> <li>!!!! This is the ONLY required step. Everything else is completely up to you.</li> <li>You might be wondering what the GenericDataset is. It is just a simple dataset class</li> </ul> <pre>import torch\nclass GenericDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Generic dataset that takes X,y as input and returns them as tensors\"\"\"\n\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)  # Convert to tensors\n        self.y = torch.tensor(y, dtype=torch.long)  # Ensure labels are LongTensor\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n</pre>"},{"location":"Examples/Getting%20data%20from%20OpenML/#choose-your-model","title":"Choose your model\u00b6","text":""},{"location":"Examples/Getting%20data%20from%20OpenML/#define-your-own-training-pipeline","title":"Define your own training pipeline\u00b6","text":""},{"location":"Examples/Upload%20results%20to%20OpenML/","title":"Upload data to OpenML","text":"In\u00a0[\u00a0]: Copied! <pre># openml imports\nimport openml\nimport openml_pytorch as op\nfrom openml_pytorch.callbacks import TestCallback\nfrom openml_pytorch.metrics import accuracy\nfrom openml_pytorch.trainer import convert_to_rgb\n\n# pytorch imports\nfrom torch.utils.tensorboard.writer import SummaryWriter\nfrom torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda\nimport torchvision\n\n# other imports\nimport logging\nimport warnings\n\n# set up logging\nopenml.config.logger.setLevel(logging.DEBUG)\nop.config.logger.setLevel(logging.DEBUG)\nwarnings.simplefilter(action='ignore')\n</pre> # openml imports import openml import openml_pytorch as op from openml_pytorch.callbacks import TestCallback from openml_pytorch.metrics import accuracy from openml_pytorch.trainer import convert_to_rgb  # pytorch imports from torch.utils.tensorboard.writer import SummaryWriter from torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda import torchvision  # other imports import logging import warnings  # set up logging openml.config.logger.setLevel(logging.DEBUG) op.config.logger.setLevel(logging.DEBUG) warnings.simplefilter(action='ignore') In\u00a0[\u00a0]: Copied! <pre>transform = Compose(\n    [\n        ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.\n        Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.\n        Resize((64, 64)),  # Resize the image.\n        ToTensor(),  # Convert the PIL Image back to a tensor.\n    ]\n)\n</pre>  transform = Compose(     [         ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.         Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.         Resize((64, 64)),  # Resize the image.         ToTensor(),  # Convert the PIL Image back to a tensor.     ] )  In\u00a0[\u00a0]: Copied! <pre>data_module = op.OpenMLDataModule(\n    type_of_data=\"image\",\n    file_dir=\"datasets\",\n    filename_col=\"image_path\",\n    target_mode=\"categorical\",\n    target_column=\"label\",\n    batch_size=64,\n    transform=transform,\n)\n\n# Download the OpenML task for tiniest imagenet\ntask = openml.tasks.get_task(363295)\n</pre> data_module = op.OpenMLDataModule(     type_of_data=\"image\",     file_dir=\"datasets\",     filename_col=\"image_path\",     target_mode=\"categorical\",     target_column=\"label\",     batch_size=64,     transform=transform, )  # Download the OpenML task for tiniest imagenet task = openml.tasks.get_task(363295)  In\u00a0[\u00a0]: Copied! <pre>model = torchvision.models.resnet18(num_classes=200)\n</pre> model = torchvision.models.resnet18(num_classes=200) In\u00a0[\u00a0]: Copied! <pre>import torch\n\ntrainer = op.OpenMLTrainerModule(\n    experiment_name= \"Tiny ImageNet\",\n    data_module=data_module,\n    verbose=True,\n    epoch_count=2,\n    metrics= [accuracy],\n    # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.\n    callbacks=[\n        # TestCallback,\n    ],\n    opt = torch.optim.Adam,\n)\nop.config.trainer = trainer\nrun = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\n</pre> import torch  trainer = op.OpenMLTrainerModule(     experiment_name= \"Tiny ImageNet\",     data_module=data_module,     verbose=True,     epoch_count=2,     metrics= [accuracy],     # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.     callbacks=[         # TestCallback,     ],     opt = torch.optim.Adam, ) op.config.trainer = trainer run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False) In\u00a0[\u00a0]: Copied! <pre>openml.config.apikey = ''\nrun = op.add_experiment_info_to_run(run=run, trainer=trainer, upload_model=True)\nrun.publish()\n</pre> openml.config.apikey = '' run = op.add_experiment_info_to_run(run=run, trainer=trainer, upload_model=True) run.publish()"},{"location":"Examples/Upload%20results%20to%20OpenML/#upload-data-to-openml","title":"Upload data to OpenML\u00b6","text":"<ul> <li>Results, models and other artifacts can be uploaded to OpenML.</li> </ul>"},{"location":"Examples/Upload%20results%20to%20OpenML/#data","title":"Data\u00b6","text":""},{"location":"Examples/Upload%20results%20to%20OpenML/#define-image-transformations","title":"Define image transformations\u00b6","text":""},{"location":"Examples/Upload%20results%20to%20OpenML/#configure-the-data-module-and-choose-a-task","title":"Configure the Data Module and Choose a Task\u00b6","text":"<ul> <li>Make sure the data is present in the <code>file_dir</code> directory, and the <code>filename_col</code> is correctly set along with this column correctly pointing to where your data is stored.</li> </ul>"},{"location":"Examples/Upload%20results%20to%20OpenML/#model","title":"Model\u00b6","text":""},{"location":"Examples/Upload%20results%20to%20OpenML/#train-your-model-on-the-data","title":"Train your model on the data\u00b6","text":"<ul> <li>Note that by default, OpenML runs a 10 fold cross validation on the data. You cannot change this for now.</li> </ul>"},{"location":"Examples/Upload%20results%20to%20OpenML/#publish-your-model-to-openml","title":"Publish your model to OpenML\u00b6","text":"<ul> <li>This is Optional, but publishing your model to OpenML will allow you to track your experiments and compare them with others.</li> <li>Make sure to set your apikey first.<ul> <li>You can find your apikey on your OpenML account page.</li> </ul> </li> <li>If upload_model is False, only the results and an ONNX model will be uploaded. If it is true, model weights will be uploaded as well</li> </ul>"},{"location":"Examples/Custom%20Training%20Pipelines/Class%20Weighting/","title":"Class Weighting","text":"In\u00a0[\u00a0]: Copied! <pre># openml imports\nimport openml\nimport openml_pytorch as op\nfrom openml_pytorch.callbacks import TestCallback\nfrom openml_pytorch.metrics import accuracy\nfrom openml_pytorch.trainer import convert_to_rgb\n\n# pytorch imports\nfrom torch.utils.tensorboard.writer import SummaryWriter\nfrom torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda\nimport torchvision\n\n# other imports\nimport logging\nimport warnings\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport torch\n\n# set up logging\nopenml.config.logger.setLevel(logging.DEBUG)\nop.config.logger.setLevel(logging.DEBUG)\nwarnings.simplefilter(action='ignore')\n</pre> # openml imports import openml import openml_pytorch as op from openml_pytorch.callbacks import TestCallback from openml_pytorch.metrics import accuracy from openml_pytorch.trainer import convert_to_rgb  # pytorch imports from torch.utils.tensorboard.writer import SummaryWriter from torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda import torchvision  # other imports import logging import warnings from sklearn.utils import compute_class_weight from sklearn.model_selection import train_test_split import numpy as np import torch  # set up logging openml.config.logger.setLevel(logging.DEBUG) op.config.logger.setLevel(logging.DEBUG) warnings.simplefilter(action='ignore') In\u00a0[\u00a0]: Copied! <pre>transform = Compose(\n    [\n        ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.\n        Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.\n        Resize((64, 64)),  # Resize the image.\n        ToTensor(),  # Convert the PIL Image back to a tensor.\n    ]\n)\n</pre> transform = Compose(     [         ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.         Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.         Resize((64, 64)),  # Resize the image.         ToTensor(),  # Convert the PIL Image back to a tensor.     ] )  In\u00a0[\u00a0]: Copied! <pre># Get the y_train and y_test data, keep random_state=0 for reproducibility and stratify\ndataset = openml.datasets.get_dataset(46578)\nX, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n_, _, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y)\n</pre> # Get the y_train and y_test data, keep random_state=0 for reproducibility and stratify dataset = openml.datasets.get_dataset(46578) X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute) _, _, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y) In\u00a0[\u00a0]: Copied! <pre># set device\ndevice = torch.device(\"mps\" if torch.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n</pre> # set device device = torch.device(\"mps\" if torch.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\") In\u00a0[\u00a0]: Copied! <pre># compute class weights\nweights = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\nclass_weights = torch.tensor(weights, dtype=torch.float).to(device=device)\n</pre> # compute class weights weights = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train) class_weights = torch.tensor(weights, dtype=torch.float).to(device=device)  In\u00a0[\u00a0]: Copied! <pre>data_module = op.OpenMLDataModule(\n    type_of_data=\"image\",\n    file_dir=\"datasets\",\n    filename_col=\"image_path\",\n    target_mode=\"categorical\",\n    target_column=\"label\",\n    batch_size=64,\n    transform=transform,\n    loss_fn = torch.nn.CrossEntropyLoss,\n    loss_fn_kwargs={\n        \"weight\": class_weights,\n        \"label_smoothing\":0.1,\n    },\n)\n\n# Download the OpenML task for tiniest imagenet\ntask = openml.tasks.get_task(363295)\n</pre> data_module = op.OpenMLDataModule(     type_of_data=\"image\",     file_dir=\"datasets\",     filename_col=\"image_path\",     target_mode=\"categorical\",     target_column=\"label\",     batch_size=64,     transform=transform,     loss_fn = torch.nn.CrossEntropyLoss,     loss_fn_kwargs={         \"weight\": class_weights,         \"label_smoothing\":0.1,     }, )  # Download the OpenML task for tiniest imagenet task = openml.tasks.get_task(363295)  In\u00a0[\u00a0]: Copied! <pre>model = torchvision.models.resnet18(num_classes=200)\n</pre> model = torchvision.models.resnet18(num_classes=200) In\u00a0[\u00a0]: Copied! <pre>import torch\n\ntrainer = op.OpenMLTrainerModule(\n    experiment_name= \"Tiny ImageNet\",\n    data_module=data_module,\n    verbose=True,\n    epoch_count=1,\n    metrics= [accuracy],\n    # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.\n    callbacks=[\n        # TestCallback,\n    ],\n)\nop.config.trainer = trainer\nrun = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\n</pre> import torch  trainer = op.OpenMLTrainerModule(     experiment_name= \"Tiny ImageNet\",     data_module=data_module,     verbose=True,     epoch_count=1,     metrics= [accuracy],     # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.     callbacks=[         # TestCallback,     ], ) op.config.trainer = trainer run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False) In\u00a0[\u00a0]: Copied! <pre>openml.config.apikey = ''\nrun = op.add_experiment_info_to_run(run=run, trainer=trainer) \nrun.publish()\n</pre> openml.config.apikey = '' run = op.add_experiment_info_to_run(run=run, trainer=trainer)  run.publish()"},{"location":"Examples/Custom%20Training%20Pipelines/Class%20Weighting/#class-weighting","title":"Class Weighting\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Class%20Weighting/#data","title":"Data\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Class%20Weighting/#define-image-transformations","title":"Define image transformations\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Class%20Weighting/#configure-the-data-module-and-choose-a-task","title":"Configure the Data Module and Choose a Task\u00b6","text":"<ul> <li>Make sure the data is present in the <code>file_dir</code> directory, and the <code>filename_col</code> is correctly set along with this column correctly pointing to where your data is stored.</li> </ul>"},{"location":"Examples/Custom%20Training%20Pipelines/Class%20Weighting/#model","title":"Model\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Class%20Weighting/#train-your-model-on-the-data","title":"Train your model on the data\u00b6","text":"<ul> <li>Note that by default, OpenML runs a 10 fold cross validation on the data. You cannot change this for now.</li> </ul>"},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Learning%20Rate%20Scheduler/","title":"Use a LR Scheduler","text":"In\u00a0[\u00a0]: Copied! <pre># openml imports\nimport openml\nimport openml_pytorch as op\nfrom openml_pytorch.callbacks import TestCallback\nfrom openml_pytorch.metrics import accuracy\nfrom openml_pytorch.trainer import convert_to_rgb\n\n# pytorch imports\nfrom torch.utils.tensorboard.writer import SummaryWriter\nfrom torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda\nimport torchvision\n\n# other imports\nimport logging\nimport warnings\n\n# set up logging\nopenml.config.logger.setLevel(logging.DEBUG)\nop.config.logger.setLevel(logging.DEBUG)\nwarnings.simplefilter(action='ignore')\n</pre> # openml imports import openml import openml_pytorch as op from openml_pytorch.callbacks import TestCallback from openml_pytorch.metrics import accuracy from openml_pytorch.trainer import convert_to_rgb  # pytorch imports from torch.utils.tensorboard.writer import SummaryWriter from torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda import torchvision  # other imports import logging import warnings  # set up logging openml.config.logger.setLevel(logging.DEBUG) op.config.logger.setLevel(logging.DEBUG) warnings.simplefilter(action='ignore') In\u00a0[\u00a0]: Copied! <pre>transform = Compose(\n    [\n        ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.\n        Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.\n        Resize((64, 64)),  # Resize the image.\n        ToTensor(),  # Convert the PIL Image back to a tensor.\n    ]\n)\n</pre> transform = Compose(     [         ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.         Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.         Resize((64, 64)),  # Resize the image.         ToTensor(),  # Convert the PIL Image back to a tensor.     ] )  In\u00a0[\u00a0]: Copied! <pre>data_module = op.OpenMLDataModule(\n    type_of_data=\"image\",\n    file_dir=\"datasets\",\n    filename_col=\"image_path\",\n    target_mode=\"categorical\",\n    target_column=\"label\",\n    batch_size=64,\n    transform=transform,\n)\n\n# Download the OpenML task for tiniest imagenet\ntask = openml.tasks.get_task(363295)\n</pre> data_module = op.OpenMLDataModule(     type_of_data=\"image\",     file_dir=\"datasets\",     filename_col=\"image_path\",     target_mode=\"categorical\",     target_column=\"label\",     batch_size=64,     transform=transform, )  # Download the OpenML task for tiniest imagenet task = openml.tasks.get_task(363295)  In\u00a0[\u00a0]: Copied! <pre>model = torchvision.models.resnet18(num_classes=200)\n</pre> model = torchvision.models.resnet18(num_classes=200) In\u00a0[\u00a0]: Copied! <pre>import torch\n\ntrainer = op.OpenMLTrainerModule(\n    experiment_name= \"Tiny ImageNet\",\n    data_module=data_module,\n    verbose=True,\n    epoch_count=1,\n    metrics= [op.metrics.accuracy],\n    # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.\n    callbacks=[\n        # TestCallback,\n    ],\n    torch_lr_scheduler = torch.optim.lr_scheduler.StepLR, # The learning rate scheduler to use. Note that this is the class itself, not an instance of it. \n    lr_scheduler_kwargs = {\"step_size\": 1, \"gamma\": 0.1},\n    opt = torch.optim.AdamW,\n    opt_kwargs= {\"lr\": 1e-3, \"weight_decay\": 1e-4},\n)\nop.config.trainer = trainer\nrun = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\n</pre> import torch  trainer = op.OpenMLTrainerModule(     experiment_name= \"Tiny ImageNet\",     data_module=data_module,     verbose=True,     epoch_count=1,     metrics= [op.metrics.accuracy],     # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.     callbacks=[         # TestCallback,     ],     torch_lr_scheduler = torch.optim.lr_scheduler.StepLR, # The learning rate scheduler to use. Note that this is the class itself, not an instance of it.      lr_scheduler_kwargs = {\"step_size\": 1, \"gamma\": 0.1},     opt = torch.optim.AdamW,     opt_kwargs= {\"lr\": 1e-3, \"weight_decay\": 1e-4}, ) op.config.trainer = trainer run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False) In\u00a0[\u00a0]: Copied! <pre>openml.config.apikey = ''\nrun = op.add_experiment_info_to_run(run=run, trainer=trainer) \nrun.publish()\n</pre> openml.config.apikey = '' run = op.add_experiment_info_to_run(run=run, trainer=trainer)  run.publish()"},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Learning%20Rate%20Scheduler/#use-a-lr-scheduler","title":"Use a LR Scheduler\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Learning%20Rate%20Scheduler/#data","title":"Data\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Learning%20Rate%20Scheduler/#define-image-transformations","title":"Define image transformations\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Learning%20Rate%20Scheduler/#configure-the-data-module-and-choose-a-task","title":"Configure the Data Module and Choose a Task\u00b6","text":"<ul> <li>Make sure the data is present in the <code>file_dir</code> directory, and the <code>filename_col</code> is correctly set along with this column correctly pointing to where your data is stored.</li> </ul>"},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Learning%20Rate%20Scheduler/#model","title":"Model\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Learning%20Rate%20Scheduler/#train-your-model-on-the-data","title":"Train your model on the data\u00b6","text":"<ul> <li>Note that by default, OpenML runs a 10 fold cross validation on the data. You cannot change this for now.</li> <li>The default Learning rate scheduler is CosineAnnealingLR with {\"T_max\": 50, \"eta_min\": 1e-6}, you can change this to whatever you want</li> </ul>"},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Loss%20Function/","title":"Use a Custom Loss Function","text":"In\u00a0[\u00a0]: Copied! <pre># openml imports\nimport openml\nimport openml_pytorch as op\nfrom openml_pytorch.callbacks import TestCallback\nfrom openml_pytorch.metrics import accuracy\nfrom openml_pytorch.trainer import convert_to_rgb\n\n# pytorch imports\nfrom torch.utils.tensorboard.writer import SummaryWriter\nfrom torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda\nimport torchvision\n\n# other imports\nimport logging\nimport warnings\n\n# set up logging\nopenml.config.logger.setLevel(logging.DEBUG)\nop.config.logger.setLevel(logging.DEBUG)\nwarnings.simplefilter(action='ignore')\n</pre> # openml imports import openml import openml_pytorch as op from openml_pytorch.callbacks import TestCallback from openml_pytorch.metrics import accuracy from openml_pytorch.trainer import convert_to_rgb  # pytorch imports from torch.utils.tensorboard.writer import SummaryWriter from torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda import torchvision  # other imports import logging import warnings  # set up logging openml.config.logger.setLevel(logging.DEBUG) op.config.logger.setLevel(logging.DEBUG) warnings.simplefilter(action='ignore') In\u00a0[\u00a0]: Copied! <pre>transform = Compose(\n    [\n        ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.\n        Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.\n        Resize((64, 64)),  # Resize the image.\n        ToTensor(),  # Convert the PIL Image back to a tensor.\n    ]\n)\n</pre> transform = Compose(     [         ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.         Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.         Resize((64, 64)),  # Resize the image.         ToTensor(),  # Convert the PIL Image back to a tensor.     ] )  In\u00a0[\u00a0]: Copied! <pre>data_module = op.OpenMLDataModule(\n    type_of_data=\"image\",\n    file_dir=\"datasets\",\n    filename_col=\"image_path\",\n    target_mode=\"categorical\",\n    target_column=\"label\",\n    batch_size=64,\n    transform=transform,\n)\n\n# Download the OpenML task for tiniest imagenet\ntask = openml.tasks.get_task(363295)\n</pre> data_module = op.OpenMLDataModule(     type_of_data=\"image\",     file_dir=\"datasets\",     filename_col=\"image_path\",     target_mode=\"categorical\",     target_column=\"label\",     batch_size=64,     transform=transform, )  # Download the OpenML task for tiniest imagenet task = openml.tasks.get_task(363295)  In\u00a0[\u00a0]: Copied! <pre>model = torchvision.models.resnet18(num_classes=200)\n</pre> model = torchvision.models.resnet18(num_classes=200) In\u00a0[\u00a0]: Copied! <pre>import torch\n\ntrainer = op.OpenMLTrainerModule(\n    experiment_name= \"Tiny ImageNet\",\n    data_module=data_module,\n    verbose=True,\n    epoch_count=1,\n    metrics= [accuracy],\n    # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.\n    callbacks=[\n        # TestCallback,\n    ],\n    loss_fn= torch.nn.CrossEntropyLoss , #NOTE! There is the class, do not call it like torch.nn.CrossEntropyLoss()\n)\nop.config.trainer = trainer\nrun = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\n</pre> import torch  trainer = op.OpenMLTrainerModule(     experiment_name= \"Tiny ImageNet\",     data_module=data_module,     verbose=True,     epoch_count=1,     metrics= [accuracy],     # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.     callbacks=[         # TestCallback,     ],     loss_fn= torch.nn.CrossEntropyLoss , #NOTE! There is the class, do not call it like torch.nn.CrossEntropyLoss() ) op.config.trainer = trainer run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False) In\u00a0[\u00a0]: Copied! <pre>openml.config.apikey = ''\nrun = op.add_experiment_info_to_run(run=run, trainer=trainer) \nrun.publish()\n</pre> openml.config.apikey = '' run = op.add_experiment_info_to_run(run=run, trainer=trainer)  run.publish()"},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Loss%20Function/#use-a-custom-loss-function","title":"Use a Custom Loss Function\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Loss%20Function/#data","title":"Data\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Loss%20Function/#define-image-transformations","title":"Define image transformations\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Loss%20Function/#configure-the-data-module-and-choose-a-task","title":"Configure the Data Module and Choose a Task\u00b6","text":"<ul> <li>Make sure the data is present in the <code>file_dir</code> directory, and the <code>filename_col</code> is correctly set along with this column correctly pointing to where your data is stored.</li> </ul>"},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Loss%20Function/#model","title":"Model\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Loss%20Function/#train-your-model-on-the-data","title":"Train your model on the data\u00b6","text":"<ul> <li>Note that by default, OpenML runs a 10 fold cross validation on the data. You cannot change this for now.</li> <li>The default Loss function is <code>CrossEntropyLoss</code> which is used for classification tasks. You can change this to any other loss function that is available in PyTorch. Note that sometimes you might get errors if the loss function is not compatible with the task you are trying to solve.<ul> <li>If you cant solve it yourself, feel free to create a Github issue</li> </ul> </li> </ul>"},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Optimizer/","title":"Use a Custom Optimizer","text":"In\u00a0[\u00a0]: Copied! <pre># openml imports\nimport openml\nimport openml_pytorch as op\nfrom openml_pytorch.callbacks import TestCallback\nfrom openml_pytorch.metrics import accuracy\nfrom openml_pytorch.trainer import convert_to_rgb\n\n# pytorch imports\nfrom torch.utils.tensorboard.writer import SummaryWriter\nfrom torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda\nimport torchvision\n\n# other imports\nimport logging\nimport warnings\n\n# set up logging\nopenml.config.logger.setLevel(logging.DEBUG)\nop.config.logger.setLevel(logging.DEBUG)\nwarnings.simplefilter(action='ignore')\n</pre> # openml imports import openml import openml_pytorch as op from openml_pytorch.callbacks import TestCallback from openml_pytorch.metrics import accuracy from openml_pytorch.trainer import convert_to_rgb  # pytorch imports from torch.utils.tensorboard.writer import SummaryWriter from torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda import torchvision  # other imports import logging import warnings  # set up logging openml.config.logger.setLevel(logging.DEBUG) op.config.logger.setLevel(logging.DEBUG) warnings.simplefilter(action='ignore') In\u00a0[2]: Copied! <pre>transform = Compose(\n    [\n        ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.\n        Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.\n        Resize((64, 64)),  # Resize the image.\n        ToTensor(),  # Convert the PIL Image back to a tensor.\n    ]\n)\n</pre> transform = Compose(     [         ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.         Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.         Resize((64, 64)),  # Resize the image.         ToTensor(),  # Convert the PIL Image back to a tensor.     ] )  In\u00a0[3]: Copied! <pre>data_module = op.OpenMLDataModule(\n    type_of_data=\"image\",\n    file_dir=\"datasets\",\n    filename_col=\"image_path\",\n    target_mode=\"categorical\",\n    target_column=\"label\",\n    batch_size=64,\n    transform=transform,\n)\n\n# Download the OpenML task for tiniest imagenet\ntask = openml.tasks.get_task(363295)\n</pre> data_module = op.OpenMLDataModule(     type_of_data=\"image\",     file_dir=\"datasets\",     filename_col=\"image_path\",     target_mode=\"categorical\",     target_column=\"label\",     batch_size=64,     transform=transform, )  # Download the OpenML task for tiniest imagenet task = openml.tasks.get_task(363295)  In\u00a0[4]: Copied! <pre>model = torchvision.models.resnet18(num_classes=200)\n</pre> model = torchvision.models.resnet18(num_classes=200) In\u00a0[\u00a0]: Copied! <pre>import torch\n\ntrainer = op.OpenMLTrainerModule(\n    experiment_name= \"Tiny ImageNet\",\n    data_module=data_module,\n    verbose=True,\n    epoch_count=1,\n    metrics= [accuracy],\n    # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.\n    callbacks=[\n        # TestCallback,\n    ],\n    opt = torch.optim.SGD, #NOTE! There is the class, do not call it like torch.optim.SGD()\n    opt_kwargs = {\"lr\": 0.01, \"momentum\": 0.9},\n)\nop.config.trainer = trainer\nrun = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\n</pre> import torch  trainer = op.OpenMLTrainerModule(     experiment_name= \"Tiny ImageNet\",     data_module=data_module,     verbose=True,     epoch_count=1,     metrics= [accuracy],     # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.     callbacks=[         # TestCallback,     ],     opt = torch.optim.SGD, #NOTE! There is the class, do not call it like torch.optim.SGD()     opt_kwargs = {\"lr\": 0.01, \"momentum\": 0.9}, ) op.config.trainer = trainer run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False) In\u00a0[\u00a0]: Copied! <pre>openml.config.apikey = ''\nrun = op.add_experiment_info_to_run(run=run, trainer=trainer) \nrun.publish()\n</pre> openml.config.apikey = '' run = op.add_experiment_info_to_run(run=run, trainer=trainer)  run.publish()"},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Optimizer/#use-a-custom-optimizer","title":"Use a Custom Optimizer\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Optimizer/#data","title":"Data\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Optimizer/#define-image-transformations","title":"Define image transformations\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Optimizer/#configure-the-data-module-and-choose-a-task","title":"Configure the Data Module and Choose a Task\u00b6","text":"<ul> <li>Make sure the data is present in the <code>file_dir</code> directory, and the <code>filename_col</code> is correctly set along with this column correctly pointing to where your data is stored.</li> </ul>"},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Optimizer/#model","title":"Model\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Custom%20Optimizer/#train-your-model-on-the-data","title":"Train your model on the data\u00b6","text":"<ul> <li>Note that by default, OpenML runs a 10 fold cross validation on the data. You cannot change this for now.</li> <li>The default Optimizer is AdamW with {\"lr\": 3e-4, \"weight_decay\": 1e-4}, but say we want to use SGD instead</li> </ul>"},{"location":"Examples/Custom%20Training%20Pipelines/Using%20Models%20from%20other%20Frameworks/HuggingFace/","title":"Using a model from HuggingFace","text":"In\u00a0[\u00a0]: Copied! <pre># openml imports\nimport openml\nimport openml_pytorch as op\nfrom openml_pytorch.callbacks import TestCallback\nfrom openml_pytorch.metrics import accuracy\nfrom openml_pytorch.trainer import convert_to_rgb\n\n# pytorch imports\nfrom torch.utils.tensorboard.writer import SummaryWriter\nfrom torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda\nimport torchvision\n\n# other imports\nimport logging\nimport warnings\n\n# set up logging\nopenml.config.logger.setLevel(logging.DEBUG)\nop.config.logger.setLevel(logging.DEBUG)\nwarnings.simplefilter(action='ignore')\n</pre> # openml imports import openml import openml_pytorch as op from openml_pytorch.callbacks import TestCallback from openml_pytorch.metrics import accuracy from openml_pytorch.trainer import convert_to_rgb  # pytorch imports from torch.utils.tensorboard.writer import SummaryWriter from torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda import torchvision  # other imports import logging import warnings  # set up logging openml.config.logger.setLevel(logging.DEBUG) op.config.logger.setLevel(logging.DEBUG) warnings.simplefilter(action='ignore') In\u00a0[\u00a0]: Copied! <pre>transform = Compose(\n    [\n        ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.\n        Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.\n        Resize((64, 64)),  # Resize the image.\n        ToTensor(),  # Convert the PIL Image back to a tensor.\n    ]\n)\n</pre> transform = Compose(     [         ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.         Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.         Resize((64, 64)),  # Resize the image.         ToTensor(),  # Convert the PIL Image back to a tensor.     ] )  In\u00a0[\u00a0]: Copied! <pre>data_module = op.OpenMLDataModule(\n    type_of_data=\"image\",\n    file_dir=\"datasets\",\n    filename_col=\"image_path\",\n    target_mode=\"categorical\",\n    target_column=\"label\",\n    batch_size=64,\n    transform=transform,\n)\n\n# Download the OpenML task for tiniest imagenet\ntask = openml.tasks.get_task(363295)\n</pre> data_module = op.OpenMLDataModule(     type_of_data=\"image\",     file_dir=\"datasets\",     filename_col=\"image_path\",     target_mode=\"categorical\",     target_column=\"label\",     batch_size=64,     transform=transform, )  # Download the OpenML task for tiniest imagenet task = openml.tasks.get_task(363295)  <ul> <li>Then you need to modify the model to work with the number of classes in your dataset</li> <li>This is something you would need to do anyway if you were transfer learning on a dataset different from the one the model was trained on.</li> <li>You can use the <code>model</code> object to access the model and modify the final layer to match the number of classes in your dataset. (note that this depends on the model you are using, and you might need to do this manually)<ul> <li>In general, try either <code>model.classifier</code> or <code>model.fc</code> or <code>model.classifier[-1]</code> to access the final layer of the model.</li> <li>If this doesnt work, try printing the model and looking at the architecture to find the correct layer to modify.</li> <li>HF provides a \"num_labels\" parameter but this does not always work as expected, so it is better to modify the final layer manually.</li> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\n\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-18\")\nmodel_o = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-18\")\n\nclass TransformerCompatibility(torch.nn.Module):\n    def __init__(self, model_from_pretrained, num_classes) -&gt; None:\n        super(TransformerCompatibility, self).__init__()\n        self.model = model_from_pretrained\n        # self.model.classifier = torch.nn.Linear(self.model.classifier.in_features, num_classes)\n        self.model.classifier._modules['1'] = torch.nn.Linear(self.model.classifier._modules['1'].in_features, num_classes)\n\n    def forward(self, input):\n        # The ViT model expects the input to be of shape (batch_size, num_channels, height, width)\n        # Ensure the input is in the correct shape\n        if len(input.shape) == 3:\n            input = input.unsqueeze(0)\n        # Forward pass through the model\n        outputs = self.model(input)\n        # The output is a tuple, where the first element is the logits\n        logits = outputs.logits\n        return logits\n\n    \nmodel = TransformerCompatibility(model_o, num_classes=200)\n</pre> import torch from transformers import AutoImageProcessor, AutoModelForImageClassification  processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-18\") model_o = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-18\")  class TransformerCompatibility(torch.nn.Module):     def __init__(self, model_from_pretrained, num_classes) -&gt; None:         super(TransformerCompatibility, self).__init__()         self.model = model_from_pretrained         # self.model.classifier = torch.nn.Linear(self.model.classifier.in_features, num_classes)         self.model.classifier._modules['1'] = torch.nn.Linear(self.model.classifier._modules['1'].in_features, num_classes)      def forward(self, input):         # The ViT model expects the input to be of shape (batch_size, num_channels, height, width)         # Ensure the input is in the correct shape         if len(input.shape) == 3:             input = input.unsqueeze(0)         # Forward pass through the model         outputs = self.model(input)         # The output is a tuple, where the first element is the logits         logits = outputs.logits         return logits       model = TransformerCompatibility(model_o, num_classes=200) In\u00a0[\u00a0]: Copied! <pre>import torch\n\ntrainer = op.OpenMLTrainerModule(\n    experiment_name= \"Tiny ImageNet\",\n    data_module=data_module,\n    verbose=True,\n    epoch_count=1,\n    metrics= [accuracy],\n    # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.\n    callbacks=[\n        # TestCallback,\n    ],\n)\nop.config.trainer = trainer\nrun = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\n</pre> import torch  trainer = op.OpenMLTrainerModule(     experiment_name= \"Tiny ImageNet\",     data_module=data_module,     verbose=True,     epoch_count=1,     metrics= [accuracy],     # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.     callbacks=[         # TestCallback,     ], ) op.config.trainer = trainer run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False) In\u00a0[\u00a0]: Copied! <pre>openml.config.apikey = ''\nrun = op.add_experiment_info_to_run(run=run, trainer=trainer) \nrun.publish()\n</pre> openml.config.apikey = '' run = op.add_experiment_info_to_run(run=run, trainer=trainer)  run.publish()"},{"location":"Examples/Custom%20Training%20Pipelines/Using%20Models%20from%20other%20Frameworks/HuggingFace/#using-a-model-from-huggingface","title":"Using a model from HuggingFace\u00b6","text":"<ul> <li>This frameworks supports training such models and storing results</li> </ul>"},{"location":"Examples/Custom%20Training%20Pipelines/Using%20Models%20from%20other%20Frameworks/HuggingFace/#data","title":"Data\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Using%20Models%20from%20other%20Frameworks/HuggingFace/#define-image-transformations","title":"Define image transformations\u00b6","text":""},{"location":"Examples/Custom%20Training%20Pipelines/Using%20Models%20from%20other%20Frameworks/HuggingFace/#configure-the-data-module-and-choose-a-task","title":"Configure the Data Module and Choose a Task\u00b6","text":"<ul> <li>Make sure the data is present in the <code>file_dir</code> directory, and the <code>filename_col</code> is correctly set along with this column correctly pointing to where your data is stored.</li> </ul>"},{"location":"Examples/Custom%20Training%20Pipelines/Using%20Models%20from%20other%20Frameworks/HuggingFace/#model","title":"Model\u00b6","text":"<ul> <li>First you need to get the model you need</li> <li></li> <li>Click use this model -&gt; transformers -&gt; get the code you need</li> </ul>"},{"location":"Examples/Custom%20Training%20Pipelines/Using%20Models%20from%20other%20Frameworks/HuggingFace/#train-your-model-on-the-data","title":"Train your model on the data\u00b6","text":"<ul> <li>Note that by default, OpenML runs a 10 fold cross validation on the data. You cannot change this for now.</li> </ul>"},{"location":"Examples/Images/Image%20Classification%20Task/","title":"Basic Image classification task","text":"In\u00a0[\u00a0]: Copied! <pre># openml imports\nimport openml\nimport openml_pytorch as op\nfrom openml_pytorch.callbacks import TestCallback\nfrom openml_pytorch.metrics import accuracy\nfrom openml_pytorch.trainer import convert_to_rgb\n\n# pytorch imports\nfrom torch.utils.tensorboard.writer import SummaryWriter\nfrom torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda\nimport torchvision\n\n# other imports\nimport logging\nimport warnings\n\n# set up logging\nopenml.config.logger.setLevel(logging.DEBUG)\nop.config.logger.setLevel(logging.DEBUG)\nwarnings.simplefilter(action='ignore')\n</pre> # openml imports import openml import openml_pytorch as op from openml_pytorch.callbacks import TestCallback from openml_pytorch.metrics import accuracy from openml_pytorch.trainer import convert_to_rgb  # pytorch imports from torch.utils.tensorboard.writer import SummaryWriter from torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda import torchvision  # other imports import logging import warnings  # set up logging openml.config.logger.setLevel(logging.DEBUG) op.config.logger.setLevel(logging.DEBUG) warnings.simplefilter(action='ignore') In\u00a0[\u00a0]: Copied! <pre>transform = Compose(\n    [\n        ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.\n        Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.\n        Resize((64, 64)),  # Resize the image.\n        ToTensor(),  # Convert the PIL Image back to a tensor.\n    ]\n)\n</pre>  transform = Compose(     [         ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.         Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.         Resize((64, 64)),  # Resize the image.         ToTensor(),  # Convert the PIL Image back to a tensor.     ] )  In\u00a0[\u00a0]: Copied! <pre>data_module = op.OpenMLDataModule(\n    type_of_data=\"image\",\n    file_dir=\"datasets\",\n    filename_col=\"image_path\",\n    target_mode=\"categorical\",\n    target_column=\"label\",\n    batch_size=64,\n    transform=transform,\n)\n\n# Download the OpenML task for tiniest imagenet\ntask = openml.tasks.get_task(363295)\n</pre> data_module = op.OpenMLDataModule(     type_of_data=\"image\",     file_dir=\"datasets\",     filename_col=\"image_path\",     target_mode=\"categorical\",     target_column=\"label\",     batch_size=64,     transform=transform, )  # Download the OpenML task for tiniest imagenet task = openml.tasks.get_task(363295)  In\u00a0[\u00a0]: Copied! <pre>model = torchvision.models.resnet18(num_classes=200)\n</pre> model = torchvision.models.resnet18(num_classes=200) In\u00a0[\u00a0]: Copied! <pre>import torch\n\ntrainer = op.OpenMLTrainerModule(\n    experiment_name= \"Tiny ImageNet\",\n    data_module=data_module,\n    verbose=True,\n    epoch_count=2,\n    metrics= [accuracy],\n    # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.\n    callbacks=[\n        # TestCallback,\n    ],\n    opt = torch.optim.Adam,\n)\nop.config.trainer = trainer\nrun = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\n</pre> import torch  trainer = op.OpenMLTrainerModule(     experiment_name= \"Tiny ImageNet\",     data_module=data_module,     verbose=True,     epoch_count=2,     metrics= [accuracy],     # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.     callbacks=[         # TestCallback,     ],     opt = torch.optim.Adam, ) op.config.trainer = trainer run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False) In\u00a0[\u00a0]: Copied! <pre>trainer.plot_loss()\n</pre> trainer.plot_loss() In\u00a0[\u00a0]: Copied! <pre>trainer.plot_lr()\n</pre> trainer.plot_lr() In\u00a0[\u00a0]: Copied! <pre>trainer.stats.metrics\n</pre> trainer.stats.metrics In\u00a0[\u00a0]: Copied! <pre>trainer.plot_all_metrics()\n</pre> trainer.plot_all_metrics() In\u00a0[\u00a0]: Copied! <pre>trainer.model_classes\n</pre> trainer.model_classes In\u00a0[\u00a0]: Copied! <pre>trainer.export_to_netron()\n</pre> trainer.export_to_netron() In\u00a0[\u00a0]: Copied! <pre>trainer.plot_all_metrics()\n</pre> trainer.plot_all_metrics() In\u00a0[\u00a0]: Copied! <pre>openml.config.apikey = ''\nrun = op.add_experiment_info_to_run(run=run, trainer=trainer) \nrun.publish()\n</pre> openml.config.apikey = '' run = op.add_experiment_info_to_run(run=run, trainer=trainer)  run.publish()"},{"location":"Examples/Images/Image%20Classification%20Task/#basic-image-classification-task","title":"Basic Image classification task\u00b6","text":""},{"location":"Examples/Images/Image%20Classification%20Task/#data","title":"Data\u00b6","text":""},{"location":"Examples/Images/Image%20Classification%20Task/#define-image-transformations","title":"Define image transformations\u00b6","text":""},{"location":"Examples/Images/Image%20Classification%20Task/#configure-the-data-module-and-choose-a-task","title":"Configure the Data Module and Choose a Task\u00b6","text":"<ul> <li>Make sure the data is present in the <code>file_dir</code> directory, and the <code>filename_col</code> is correctly set along with this column correctly pointing to where your data is stored.</li> </ul>"},{"location":"Examples/Images/Image%20Classification%20Task/#model","title":"Model\u00b6","text":""},{"location":"Examples/Images/Image%20Classification%20Task/#train-your-model-on-the-data","title":"Train your model on the data\u00b6","text":"<ul> <li>Note that by default, OpenML runs a 10 fold cross validation on the data. You cannot change this for now.</li> </ul>"},{"location":"Examples/Images/Image%20Classification%20Task/#view-information-about-your-run","title":"View information about your run\u00b6","text":""},{"location":"Examples/Images/Image%20Classification%20Task/#learning-rate-and-loss-plot","title":"Learning rate and loss plot\u00b6","text":""},{"location":"Examples/Images/Image%20Classification%20Task/#class-labels","title":"Class labels\u00b6","text":""},{"location":"Examples/Images/Image%20Classification%20Task/#model-vizualization","title":"Model Vizualization\u00b6","text":"<ul> <li>Sometimes you may want to visualize the model. You can either use netron or tensorboard for this purpose.</li> </ul>"},{"location":"Examples/Images/Image%20Classification%20Task/#netron","title":"Netron\u00b6","text":""},{"location":"Examples/Images/Image%20Classification%20Task/#tensorboard","title":"Tensorboard\u00b6","text":"<ul> <li>By default, openml will log the tensorboard logs in the <code>tensorboard_logs</code> directory. You can view the logs by running <code>tensorboard --logdir tensorboard_logs</code> in the terminal.</li> </ul>"},{"location":"Examples/Images/Image%20Classification%20Task/#publish-your-model-to-openml","title":"Publish your model to OpenML\u00b6","text":"<ul> <li>This is Optional, but publishing your model to OpenML will allow you to track your experiments and compare them with others.</li> <li>Make sure to set your apikey first.<ul> <li>You can find your apikey on your OpenML account page.</li> </ul> </li> </ul>"},{"location":"Examples/Images/Pretrained%20Transformer%20Image%20Classification%20Task/","title":"Pretrained Transformer Image Classification Task","text":"In\u00a0[\u00a0]: Copied! <pre># openml imports\nimport openml\nimport openml_pytorch as op\nfrom openml_pytorch.callbacks import TestCallback\nfrom openml_pytorch.metrics import accuracy\nfrom openml_pytorch.trainer import convert_to_rgb\n\n# pytorch imports\nfrom torch.utils.tensorboard.writer import SummaryWriter\nfrom torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda\nimport torchvision\nimport torch\n\n# other imports\nimport logging\nimport warnings\n\n# set up logging\nopenml.config.logger.setLevel(logging.DEBUG)\nopenml_pytorch.config.logger.setLevel(logging.DEBUG)\nwarnings.simplefilter(action='ignore')\n</pre> # openml imports import openml import openml_pytorch as op from openml_pytorch.callbacks import TestCallback from openml_pytorch.metrics import accuracy from openml_pytorch.trainer import convert_to_rgb  # pytorch imports from torch.utils.tensorboard.writer import SummaryWriter from torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda import torchvision import torch  # other imports import logging import warnings  # set up logging openml.config.logger.setLevel(logging.DEBUG) openml_pytorch.config.logger.setLevel(logging.DEBUG) warnings.simplefilter(action='ignore') In\u00a0[\u00a0]: Copied! <pre>transform = Compose(\n    [\n        ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.\n        Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.\n        Resize((64, 64)),  # Resize the image.\n        ToTensor(),  # Convert the PIL Image back to a tensor.\n    ]\n)\n</pre>  transform = Compose(     [         ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.         Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.         Resize((64, 64)),  # Resize the image.         ToTensor(),  # Convert the PIL Image back to a tensor.     ] )  In\u00a0[\u00a0]: Copied! <pre>data_module = op.OpenMLDataModule(\n    type_of_data=\"image\",\n    file_dir=\"datasets\",\n    filename_col=\"image_path\",\n    target_mode=\"categorical\",\n    target_column=\"label\",\n    batch_size=64,\n    transform=transform,\n)\n\n# Download the OpenML task for tiniest imagenet\ntask = openml.tasks.get_task(363295)\n</pre> data_module = op.OpenMLDataModule(     type_of_data=\"image\",     file_dir=\"datasets\",     filename_col=\"image_path\",     target_mode=\"categorical\",     target_column=\"label\",     batch_size=64,     transform=transform, )  # Download the OpenML task for tiniest imagenet task = openml.tasks.get_task(363295)  In\u00a0[\u00a0]: Copied! <pre># Example model. You can do better :)\nimport torchvision.models as models\nimport torch.nn as nn\n\n# Load the pre-trained model\nmodel = models.efficientnet_b0(pretrained=True)\n\n# Modify the last fully connected layer to the required number of classes\nnum_classes = 200\nin_features = model.classifier[-1].in_features\nmodel.classifier = nn.Sequential(\n    nn.Dropout(p=0.2, inplace=True),\n    nn.Linear(in_features, num_classes),\n)\n\n# Optional: If you're fine-tuning, you may want to freeze the pre-trained layers\n# for param in model.parameters():\n#     param.requires_grad = False\n\n# # If you want to train the last layer only (the newly added layer)\n# for param in model.fc.parameters():\n#     param.requires_grad = True\n</pre> # Example model. You can do better :) import torchvision.models as models import torch.nn as nn  # Load the pre-trained model model = models.efficientnet_b0(pretrained=True)  # Modify the last fully connected layer to the required number of classes num_classes = 200 in_features = model.classifier[-1].in_features model.classifier = nn.Sequential(     nn.Dropout(p=0.2, inplace=True),     nn.Linear(in_features, num_classes), )  # Optional: If you're fine-tuning, you may want to freeze the pre-trained layers # for param in model.parameters(): #     param.requires_grad = False  # # If you want to train the last layer only (the newly added layer) # for param in model.fc.parameters(): #     param.requires_grad = True In\u00a0[\u00a0]: Copied! <pre>trainer = op.OpenMLTrainerModule(\n    experiment_name= \"Tiny ImageNet\",\n    data_module=data_module,\n    verbose=True,\n    epoch_count=1,\n    metrics= [accuracy],\n    # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.\n    callbacks=[\n        TestCallback,\n    ],\n    opt=torch.optim.Adam,\n)\nop.config.trainer = trainer\nrun = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\n</pre>   trainer = op.OpenMLTrainerModule(     experiment_name= \"Tiny ImageNet\",     data_module=data_module,     verbose=True,     epoch_count=1,     metrics= [accuracy],     # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.     callbacks=[         TestCallback,     ],     opt=torch.optim.Adam, ) op.config.trainer = trainer run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False) In\u00a0[\u00a0]: Copied! <pre>trainer.plot_loss()\n</pre> trainer.plot_loss() In\u00a0[\u00a0]: Copied! <pre>trainer.plot_lr()\n</pre> trainer.plot_lr() In\u00a0[\u00a0]: Copied! <pre>trainer.model_classes\n</pre> trainer.model_classes In\u00a0[\u00a0]: Copied! <pre>trainer.export_to_netron()\n</pre> trainer.export_to_netron() In\u00a0[\u00a0]: Copied! <pre>openml.config.apikey = ''\nrun = op.add_experiment_info_to_run(run=run, trainer=trainer) \nrun.publish()\n</pre> openml.config.apikey = '' run = op.add_experiment_info_to_run(run=run, trainer=trainer)  run.publish()"},{"location":"Examples/Images/Pretrained%20Transformer%20Image%20Classification%20Task/#pretrained-transformer-image-classification-task","title":"Pretrained Transformer Image Classification Task\u00b6","text":""},{"location":"Examples/Images/Pretrained%20Transformer%20Image%20Classification%20Task/#data","title":"Data\u00b6","text":""},{"location":"Examples/Images/Pretrained%20Transformer%20Image%20Classification%20Task/#define-image-transformations","title":"Define image transformations\u00b6","text":""},{"location":"Examples/Images/Pretrained%20Transformer%20Image%20Classification%20Task/#configure-the-data-module-and-choose-a-task","title":"Configure the Data Module and Choose a Task\u00b6","text":"<ul> <li>Make sure the data is present in the <code>file_dir</code> directory, and the <code>filename_col</code> is correctly set along with this column correctly pointing to where your data is stored.</li> </ul>"},{"location":"Examples/Images/Pretrained%20Transformer%20Image%20Classification%20Task/#model","title":"Model\u00b6","text":""},{"location":"Examples/Images/Pretrained%20Transformer%20Image%20Classification%20Task/#train-your-model-on-the-data","title":"Train your model on the data\u00b6","text":"<ul> <li>Note that by default, OpenML runs a 10 fold cross validation on the data. You cannot change this for now.</li> </ul>"},{"location":"Examples/Images/Pretrained%20Transformer%20Image%20Classification%20Task/#view-information-about-your-run","title":"View information about your run\u00b6","text":""},{"location":"Examples/Images/Pretrained%20Transformer%20Image%20Classification%20Task/#learning-rate-and-loss-plot","title":"Learning rate and loss plot\u00b6","text":""},{"location":"Examples/Images/Pretrained%20Transformer%20Image%20Classification%20Task/#class-labels","title":"Class labels\u00b6","text":""},{"location":"Examples/Images/Pretrained%20Transformer%20Image%20Classification%20Task/#model-vizualization","title":"Model Vizualization\u00b6","text":"<ul> <li>Sometimes you may want to visualize the model. You can either use netron or tensorboard for this purpose.</li> </ul>"},{"location":"Examples/Images/Pretrained%20Transformer%20Image%20Classification%20Task/#netron","title":"Netron\u00b6","text":""},{"location":"Examples/Images/Pretrained%20Transformer%20Image%20Classification%20Task/#tensorboard","title":"Tensorboard\u00b6","text":"<ul> <li>By default, openml will log the tensorboard logs in the <code>tensorboard_logs</code> directory. You can view the logs by running <code>tensorboard --logdir tensorboard_logs</code> in the terminal.</li> </ul>"},{"location":"Examples/Images/Pretrained%20Transformer%20Image%20Classification%20Task/#publish-your-model-to-openml","title":"Publish your model to OpenML\u00b6","text":"<ul> <li>This is Optional, but publishing your model to OpenML will allow you to track your experiments and compare them with others.</li> <li>Make sure to set your apikey first.<ul> <li>You can find your apikey on your OpenML account page.</li> </ul> </li> </ul>"},{"location":"Examples/Sequential/Sequential%20Classification%20Task/","title":"Sequential Classification Task","text":"In\u00a0[\u00a0]: Copied! <pre># openml imports\nimport openml\nimport openml_pytorch as op\nfrom openml_pytorch.callbacks import TestCallback\nfrom openml_pytorch.metrics import accuracy\n\n# pytorch imports\nimport torch\n\n# other imports\nimport logging\nimport warnings\n\n# set up logging\nopenml.config.logger.setLevel(logging.DEBUG)\nopenml_pytorch.config.logger.setLevel(logging.DEBUG)\nwarnings.simplefilter(action='ignore')\n</pre> # openml imports import openml import openml_pytorch as op from openml_pytorch.callbacks import TestCallback from openml_pytorch.metrics import accuracy  # pytorch imports import torch  # other imports import logging import warnings  # set up logging openml.config.logger.setLevel(logging.DEBUG) openml_pytorch.config.logger.setLevel(logging.DEBUG) warnings.simplefilter(action='ignore') In\u00a0[\u00a0]: Copied! <pre>data_module = op.OpenMLDataModule(\n    type_of_data=\"dataframe\",\n    filename_col=\"class\",\n    target_mode=\"categorical\",\n)\n\n# Download the OpenML task for the mnist 784 dataset.\ntask = openml.tasks.get_task(3573)\n</pre> data_module = op.OpenMLDataModule(     type_of_data=\"dataframe\",     filename_col=\"class\",     target_mode=\"categorical\", )  # Download the OpenML task for the mnist 784 dataset. task = openml.tasks.get_task(3573) In\u00a0[\u00a0]: Copied! <pre>############################################################################\n# Define a sequential network that does the initial image reshaping\n# and normalization model.\nprocessing_net = torch.nn.Sequential(\n    op.layers.Functional(function=torch.Tensor.reshape,\n                                                shape=(-1, 1, 28, 28)),\n    torch.nn.BatchNorm2d(num_features=1)\n)\n############################################################################\n\n############################################################################\n# Define a sequential network that does the extracts the features from the\n# image.\nfeatures_net = torch.nn.Sequential(\n    torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5),\n    torch.nn.LeakyReLU(),\n    torch.nn.MaxPool2d(kernel_size=2),\n    torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n    torch.nn.LeakyReLU(),\n    torch.nn.MaxPool2d(kernel_size=2),\n)\n############################################################################\n\n############################################################################\n# Define a sequential network that flattens the features and compiles the\n# results into probabilities for each digit.\nresults_net = torch.nn.Sequential(\n    op.layers.Functional(function=torch.Tensor.reshape,\n                                                shape=(-1, 4 * 4 * 64)),\n    torch.nn.Linear(in_features=4 * 4 * 64, out_features=256),\n    torch.nn.LeakyReLU(),\n    torch.nn.Dropout(),\n    torch.nn.Linear(in_features=256, out_features=10),\n)\n############################################################################\n# openml.config.apikey = 'key'\n\n############################################################################\n# The main network, composed of the above specified networks.\nmodel = torch.nn.Sequential(\n    processing_net,\n    features_net,\n    results_net\n)\n############################################################################\n</pre>  ############################################################################ # Define a sequential network that does the initial image reshaping # and normalization model. processing_net = torch.nn.Sequential(     op.layers.Functional(function=torch.Tensor.reshape,                                                 shape=(-1, 1, 28, 28)),     torch.nn.BatchNorm2d(num_features=1) ) ############################################################################  ############################################################################ # Define a sequential network that does the extracts the features from the # image. features_net = torch.nn.Sequential(     torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5),     torch.nn.LeakyReLU(),     torch.nn.MaxPool2d(kernel_size=2),     torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),     torch.nn.LeakyReLU(),     torch.nn.MaxPool2d(kernel_size=2), ) ############################################################################  ############################################################################ # Define a sequential network that flattens the features and compiles the # results into probabilities for each digit. results_net = torch.nn.Sequential(     op.layers.Functional(function=torch.Tensor.reshape,                                                 shape=(-1, 4 * 4 * 64)),     torch.nn.Linear(in_features=4 * 4 * 64, out_features=256),     torch.nn.LeakyReLU(),     torch.nn.Dropout(),     torch.nn.Linear(in_features=256, out_features=10), ) ############################################################################ # openml.config.apikey = 'key'  ############################################################################ # The main network, composed of the above specified networks. model = torch.nn.Sequential(     processing_net,     features_net,     results_net ) ############################################################################  In\u00a0[\u00a0]: Copied! <pre>trainer = op.OpenMLTrainerModule(\n    experiment_name= \"MNIST\",\n    data_module=data_module,\n    verbose=True,\n    epoch_count=1,\n    metrics= [accuracy],\n    # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.\n    callbacks=[\n        TestCallback,\n    ],\n    opt = torch.optim.Adam,\n)\nop.config.trainer = trainer\nrun = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\n</pre> trainer = op.OpenMLTrainerModule(     experiment_name= \"MNIST\",     data_module=data_module,     verbose=True,     epoch_count=1,     metrics= [accuracy],     # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.     callbacks=[         TestCallback,     ],     opt = torch.optim.Adam, ) op.config.trainer = trainer run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False) In\u00a0[\u00a0]: Copied! <pre>trainer.plot_loss()\n</pre> trainer.plot_loss() In\u00a0[\u00a0]: Copied! <pre>trainer.plot_lr()\n</pre> trainer.plot_lr() In\u00a0[\u00a0]: Copied! <pre>trainer.model_classes\n</pre> trainer.model_classes In\u00a0[\u00a0]: Copied! <pre>trainer.export_to_netron()\n</pre> trainer.export_to_netron() In\u00a0[\u00a0]: Copied! <pre>openml.config.apikey = ''\nrun = op.add_experiment_info_to_run(run=run, trainer=trainer) \nrun.publish()\n</pre> openml.config.apikey = '' run = op.add_experiment_info_to_run(run=run, trainer=trainer)  run.publish()"},{"location":"Examples/Sequential/Sequential%20Classification%20Task/#sequential-classification-task","title":"Sequential Classification Task\u00b6","text":"<ul> <li>Sequential classification of a tabular MNIST dataset (Task 3573) using a simple neural network.</li> </ul>"},{"location":"Examples/Sequential/Sequential%20Classification%20Task/#data","title":"Data\u00b6","text":""},{"location":"Examples/Sequential/Sequential%20Classification%20Task/#configure-the-data-module-and-choose-a-task","title":"Configure the Data Module and Choose a Task\u00b6","text":"<ul> <li>Make sure the data is present in the <code>file_dir</code> directory, and the <code>filename_col</code> is correctly set along with this column correctly pointing to where your data is stored.</li> </ul>"},{"location":"Examples/Sequential/Sequential%20Classification%20Task/#model","title":"Model\u00b6","text":""},{"location":"Examples/Sequential/Sequential%20Classification%20Task/#train-your-model-on-the-data","title":"Train your model on the data\u00b6","text":"<ul> <li>Note that by default, OpenML runs a 10 fold cross validation on the data. You cannot change this for now.</li> </ul>"},{"location":"Examples/Sequential/Sequential%20Classification%20Task/#view-information-about-your-run","title":"View information about your run\u00b6","text":""},{"location":"Examples/Sequential/Sequential%20Classification%20Task/#learning-rate-and-loss-plot","title":"Learning rate and loss plot\u00b6","text":""},{"location":"Examples/Sequential/Sequential%20Classification%20Task/#class-labels","title":"Class labels\u00b6","text":""},{"location":"Examples/Sequential/Sequential%20Classification%20Task/#model-vizualization","title":"Model Vizualization\u00b6","text":"<ul> <li>Sometimes you may want to visualize the model. You can either use netron or tensorboard for this purpose.</li> </ul>"},{"location":"Examples/Sequential/Sequential%20Classification%20Task/#netron","title":"Netron\u00b6","text":""},{"location":"Examples/Sequential/Sequential%20Classification%20Task/#tensorboard","title":"Tensorboard\u00b6","text":"<ul> <li>By default, openml will log the tensorboard logs in the <code>tensorboard_logs</code> directory. You can view the logs by running <code>tensorboard --logdir tensorboard_logs</code> in the terminal.</li> </ul>"},{"location":"Examples/Sequential/Sequential%20Classification%20Task/#publish-your-model-to-openml","title":"Publish your model to OpenML\u00b6","text":"<ul> <li>This is Optional, but publishing your model to OpenML will allow you to track your experiments and compare them with others.</li> <li>Make sure to set your apikey first.<ul> <li>You can find your apikey on your OpenML account page.</li> </ul> </li> </ul>"},{"location":"Examples/Tabular/Tabular%20Classification/","title":"Tabular classification","text":"In\u00a0[\u00a0]: Copied! <pre># openml imports\nimport openml\nimport openml_pytorch as op\nfrom openml_pytorch.callbacks import TestCallback\nfrom openml_pytorch.metrics import accuracy\n\n# pytorch imports\nimport torch\n\n# other imports\nimport logging\nimport warnings\n\n# set up logging\nopenml.config.logger.setLevel(logging.DEBUG)\nop.config.logger.setLevel(logging.DEBUG)\nwarnings.simplefilter(action='ignore')\n</pre> # openml imports import openml import openml_pytorch as op from openml_pytorch.callbacks import TestCallback from openml_pytorch.metrics import accuracy  # pytorch imports import torch  # other imports import logging import warnings  # set up logging openml.config.logger.setLevel(logging.DEBUG) op.config.logger.setLevel(logging.DEBUG) warnings.simplefilter(action='ignore') In\u00a0[\u00a0]: Copied! <pre>data_module = op.OpenMLDataModule(\n    type_of_data=\"dataframe\",\n    target_column=\"class\",\n    target_mode=\"categorical\",\n)\n\n# supervised credit-g classification\ntask = openml.tasks.get_task(31)\n</pre> data_module = op.OpenMLDataModule(     type_of_data=\"dataframe\",     target_column=\"class\",     target_mode=\"categorical\", )  # supervised credit-g classification task = openml.tasks.get_task(31)  In\u00a0[\u00a0]: Copied! <pre>class TabularClassificationmodel(torch.nn.Module):\n    def __init__(self, input_size, output_size):\n        super(TabularClassificationmodel, self).__init__()\n        self.fc1 = torch.nn.Linear(input_size, 128)\n        self.fc2 = torch.nn.Linear(128, 64)\n        self.fc3 = torch.nn.Linear(64, output_size)\n        self.relu = torch.nn.ReLU()\n        self.softmax = torch.nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        x = self.softmax(x)\n        return x\n    \nmodel = TabularClassificationmodel(20, 2)\n</pre> class TabularClassificationmodel(torch.nn.Module):     def __init__(self, input_size, output_size):         super(TabularClassificationmodel, self).__init__()         self.fc1 = torch.nn.Linear(input_size, 128)         self.fc2 = torch.nn.Linear(128, 64)         self.fc3 = torch.nn.Linear(64, output_size)         self.relu = torch.nn.ReLU()         self.softmax = torch.nn.Softmax(dim=1)      def forward(self, x):         x = self.fc1(x)         x = self.relu(x)         x = self.fc2(x)         x = self.relu(x)         x = self.fc3(x)         x = self.softmax(x)         return x      model = TabularClassificationmodel(20, 2) In\u00a0[\u00a0]: Copied! <pre>trainer = op.OpenMLTrainerModule(\n    experiment_name= \"Credit-G\",\n    data_module=data_module,\n    verbose=True,\n    epoch_count=2,\n    metrics= [accuracy],\n    # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.\n    callbacks=[\n        # TestCallback,\n    ],\n    opt = torch.optim.Adam,\n)\nop.config.trainer = trainer\nrun = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\n</pre>   trainer = op.OpenMLTrainerModule(     experiment_name= \"Credit-G\",     data_module=data_module,     verbose=True,     epoch_count=2,     metrics= [accuracy],     # remove the TestCallback when you are done testing your pipeline. Having it here will make the pipeline run for a very short time.     callbacks=[         # TestCallback,     ],     opt = torch.optim.Adam, ) op.config.trainer = trainer run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False) In\u00a0[\u00a0]: Copied! <pre>trainer.plot_loss()\n</pre> trainer.plot_loss() In\u00a0[\u00a0]: Copied! <pre>trainer.plot_lr()\n</pre> trainer.plot_lr() In\u00a0[\u00a0]: Copied! <pre>trainer.model_classes\n</pre> trainer.model_classes In\u00a0[\u00a0]: Copied! <pre>trainer.export_to_netron()\n</pre> trainer.export_to_netron() In\u00a0[\u00a0]: Copied! <pre>openml.config.apikey = ''\nrun = op.add_experiment_info_to_run(run=run, trainer=trainer) \nrun.publish()\n</pre> openml.config.apikey = '' run = op.add_experiment_info_to_run(run=run, trainer=trainer)  run.publish()"},{"location":"Examples/Tabular/Tabular%20Classification/#tabular-classification","title":"Tabular classification\u00b6","text":"<ul> <li>Supervised credit-g classification</li> </ul>"},{"location":"Examples/Tabular/Tabular%20Classification/#data","title":"Data\u00b6","text":""},{"location":"Examples/Tabular/Tabular%20Classification/#define-image-transformations","title":"Define image transformations\u00b6","text":""},{"location":"Examples/Tabular/Tabular%20Classification/#configure-the-data-module-and-choose-a-task","title":"Configure the Data Module and Choose a Task\u00b6","text":"<ul> <li>Make sure the data is present in the <code>file_dir</code> directory, and the <code>filename_col</code> is correctly set along with this column correctly pointing to where your data is stored.</li> </ul>"},{"location":"Examples/Tabular/Tabular%20Classification/#model","title":"Model\u00b6","text":""},{"location":"Examples/Tabular/Tabular%20Classification/#train-your-model-on-the-data","title":"Train your model on the data\u00b6","text":"<ul> <li>Note that by default, OpenML runs a 10 fold cross validation on the data. You cannot change this for now.</li> </ul>"},{"location":"Examples/Tabular/Tabular%20Classification/#view-information-about-your-run","title":"View information about your run\u00b6","text":""},{"location":"Examples/Tabular/Tabular%20Classification/#learning-rate-and-loss-plot","title":"Learning rate and loss plot\u00b6","text":""},{"location":"Examples/Tabular/Tabular%20Classification/#class-labels","title":"Class labels\u00b6","text":""},{"location":"Examples/Tabular/Tabular%20Classification/#model-vizualization","title":"Model Vizualization\u00b6","text":"<ul> <li>Sometimes you may want to visualize the model. You can either use netron or tensorboard for this purpose.</li> </ul>"},{"location":"Examples/Tabular/Tabular%20Classification/#netron","title":"Netron\u00b6","text":""},{"location":"Examples/Tabular/Tabular%20Classification/#tensorboard","title":"Tensorboard\u00b6","text":"<ul> <li>By default, openml will log the tensorboard logs in the <code>tensorboard_logs</code> directory. You can view the logs by running <code>tensorboard --logdir tensorboard_logs</code> in the terminal.</li> </ul>"},{"location":"Examples/Tabular/Tabular%20Classification/#publish-your-model-to-openml","title":"Publish your model to OpenML\u00b6","text":"<ul> <li>This is Optional, but publishing your model to OpenML will allow you to track your experiments and compare them with others.</li> <li>Make sure to set your apikey first.<ul> <li>You can find your apikey on your OpenML account page.</li> </ul> </li> </ul>"}]}